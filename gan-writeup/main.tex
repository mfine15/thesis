\documentclass[]{article}

\usepackage{amssymb,amsmath}
\usepackage{hyperref}


\usepackage{graphicx,subfigure}
\usepackage{epsfig}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{url}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{boxedminipage}
\usepackage{tikz}
\usepackage{mathabx}
\usepackage{tabularx,ragged2e,booktabs,caption}
%\usepackage[linesnumbered, ruled, vlined]{algorithm2e}

\usepackage{fullpage}
\usepackage[preprint]{neurips_2019}
\usepackage{xcolor}
 \pagecolor[RGB]{38,50,56} \color[RGB]{225,246,246}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\Oracle}{\mathcal{O}}
\newcommand{\B}{\{0,1\}}
\newcommand{\ignore}[1]{}
\newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}




\usepackage{amsthm}
\usepackage{amsfonts}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\author{%
  Michael Fine
  Harvard University\\
  Cambridge MA, 02138 \\
  \texttt{mfine@college.harvard.edu} 
}
\title{Principled Private Data Release with Deep Learning}



\begin{document}
\maketitle



\section{Background}

\subsection{Query Release Problem}

We study the problem of privately generating synthetic data to answer statistical queries over a data universe $\X$. Formally, a statistical query over $\X$ is a function $q: \X \to \B$. Given a a dataset $S \in \X^n$, we define $q(S) = \sum_{s\in S} q(s)$.

Our goal is to produce a synthetic dataset that, for every query in some family of queries, takes approximately the same value as the true dataset.

\begin{definition}[$\alpha-$approximate]
    We say a synthetic dataset $S$ $\alpha-$approximates a true dataset $\hat S$ w.r.t a family of statistical queries $\Q$ if

    \begin{equation}
        \forall q \in \Q: ~~~ |q(S) - q(\hat S)| \leq \alpha
    \end{equation}
\end{definition}

[TODO finish up]

\subsection{Game Theoretic Formulation}

One can formulate the problem of producing an $\alpha-$approximate dataset as a two-player, zero sum game \cite{HRU13} between a discriminator $D$ and a generator $G$. The generator has an action set $\X$, while the discriminator has an action set $\Q$. The generator aims to output a dataset $S \in \X$ that maximally agrees with $\hat S$, while the discriminator aims to find queries $q \in \Q$ that distinguish $\hat S$ and $S$.

Formally, given a play $S \in \X$ and $q \in Q$, the discriminator gets payoff $V(S,q)$ and the generator gets payoff $V(S,q)$, where $V(S,q)$ denotes:

\begin{definition}[Payoff]
    \begin{equation}
        V(S,q) := |q(S) - q(\hat S)|
    \end{equation}
\end{definition}

The goal of both $G$ and $D$ is to maximize their worst case payoffs, thus

\begin{equation}
    \max_{q \in \Q} \min_{S \in X} V(S,q) ~~ (\text{Goal of } D) ~~~~ and ~~~~ 
    \min_{S \in X} \max_{q \in \Q} V(S,q) ~~ (\text{Goal of } G) 
\end{equation}

If there exists a point $(S^*, q^*)$ such that neither $G$ nor $D$ can improve their payoffs by playing a different move, we call that a \emph{Pure Nash Equilibrium}. Unfortunately, a pure equilibrium is not always guaranteed to exist (and likely does not in the case of synthetic data generation). 

However, the seminal work of Nash et. al showed that there always exists a \emph{Mixed Nash Equilibrium (MNE)}, where the players play \emph{probability distributions} over their action sets, instead of fixed actions. 

Let $\Delta(\X)$ and $\Delta(\Q)$ denote the set of probability distribution over $\X$ and $\Q$. Formally, if $G$ plays a strategy $u \in \Delta(\X)$ and $D$ plays  $w \in \Delta(\Q)$, we define the payoff to be the expected value of a single draw:

\begin{equation}
    V(u,w) := \mathbb{E}_{S \sim u, q \sim w} V(S,q)
\end{equation}

Thus, a pair of strategies $u^* \in \Delta(\X)$ and $w^* \in \Delta(\Q)$ forms an $\alpha-$approximate mixed nash equilibrium if for all strategies $u \in \Delta(\X)$ and $w \in \Delta(\Q)$

\begin{equation}
    V(u^*, w) \leq V(u,w) + \alpha ~~~~ and ~~~~ V(u, w^*) \leq V(u,w) - \alpha 
\end{equation}

Moreover, Gaboardi et. al showed how to reduce the problem of finding an $\alpha-$approximate dataset to the problem of finding an an $\alpha-$equilibrium in the query release game:

\begin{theorem}
    Let $(u,w)$ be the $\alpha-$approximate MNE in a query release game for a dataset $\hat S \in \X$ and a query universe $\Q$. If $\Q$ is closed under negation, then the dataset $S$ sampled from $u$ $\alpha$-approximates $\hat S$ over $Q$. \cite{GAH+14}
\end{theorem}

Hence, our task is to provide an algorithm to private reach an $\alpha-$MNE in the query release game. In the following section, we will provide the background for how this can be done with GANs.



\subsection{Generative Adversarial Networks}

Generative Adversarial Networks (GANs) \todo{}


\subsection{Online Learning}

To efficiently find equilibrium in the zero-sum GAN game, we draw on results from online learning. In the online learning setting, in each of $T$ rounds a player is given a loss function $f_t$ , possibly adversarial chosen. The players goal is to chose an action $x_{t+1} \in \mathcal{X}$ in order to minimize the cumulative $\emph{regret}$. 

\begin{definition}[Regret]
    The regret measures the cumulative loss of the player, compared to the best fixed decision in hindsight. 

    \begin{equation}
        \text{Regret}_T(f_1,...,f_T) = \sum_{t=1}^T f_t(x_t) - min_{x^* \in \mathcal{X}} \sum_{t=1}^T f_t(x^*)
    \end{equation}
\end{definition}

When a strategy provably leads to regret is sublinear in $T$, we call that \emph{no-regret}, as regret $\to 0$ as $T \to \infty$.

One approach to regret minimization is to choose the action $x_{t+1}$ that minimizes the cumulative loss over all past loss functions
\begin{equation}
    x_{t+1} = \arg\min_{x \in \X} \sum_{t=1}^T
\end{equation}
This approach is known as \emph{Follow-The-Leader}. While natural, this approach is easily exploitable by an adversary. At a high level, this is because it \emph{overfits} to past outcomes, allowing it to optimize between suboptimal strategies. To rectify this, a powerful strategy is \emph{Follow-the-Regularized-Leader}

\begin{definition}[Follow-The-Reguralized-Leader (FTRL)]
    Given a reguralization function $R(x)$ and a regularization weight $\eta_T$, at each step choose $x_{t+1} \in \X$ to minimize the regularized cumulative loss:

    \begin{equation}
        x_{t+1} = \arg\min_{x \in \X} \sum_{t=1}^T + \eta_T R(X)
    \end{equation}

    One common regularization function is the $l_2$ norm $R(x) = ||x||_2$.  
\end{definition}

When the loss function $f_T$ is convex, FTRL can be shown to be no-regret \cite{Haz19}. Unfortunately, in the GAN setting, where the loss function $f_T$ is defined by a highly non-convex deep network, we don't have that guarantee. 

\subsubsection{Online Learning for Non-convex losses}
In general, finding the minimum of a sum of non-convex functions is hard. However, in practice gradient descent over neural networks has proven to be remarkably effective at approximately solving non-convex loss functions. This leads to the natural question: assuming we have an offline non-convex optimization oracle $\mathcal{O}$, can we use that to find a no-regret strategy in the online setting? Agarwal et al showed that we can, using a Follow-The-Leader variant known as Follow-The-Perturbed-Leader \cite{AGH18}. Formally:

\begin{definition}[Offline optimization oracle]
    Let $\mathcal{O}$ take a sequence of (possibly non-convex) loss functions $(f_1...f_T) \in \mathcal{L}^T$ and a d-dimensional vector $d$, and output $x^* \in \X$
    \begin{equation}
        x^* \in \arg\min_{x \in \X} \sum_{t=1}^Tf_t + \sigma^Tx
    \end{equation}
\end{definition}

We can use this offline oracle $\mathcal{O}$ to minimize regret in the online case:

\begin{definition}[Follow-The-Perturbed-Leader (FTPL)]
    Given an offline oracle $\mathcal{O}$ and a parameter $\eta$, at each step $t$ FTPL draws a random vector $\sigma_t \sim Exp(\eta)^d$. It then outputs 
    \begin{equation}
        x^* \in \arg\min_{x \in \X} \sum_{t=1}^Tf_t + \sigma^Tx
    \end{equation}
\end{definition}

\begin{theorem}
    FTPL has sublinear regret. \cite{AGH18}
\end{theorem}

Building on the work of Freund and Schapire, we can show that if $G$ and $D$ both play FTPL for $T$ rounds, they will converge to an $\alpha-$approximate equilibrium. Formally:

\begin{theorem}\label{ftpl-equilibriu}
    Suppose that $G$ and $D$ play according to FTPL. We can choose $T \in poly(d)/\alpha^3$ such that the expected average regret of FTPL is at most $\alpha$. Then, $G_T$ and $D_T$ produce strategies in an $\alpha-$approximate equilibrium \cite{AGH18}.
\end{theorem}

\section{Results}
\todo{summarize approach}
Theorem 3 requires the existence of an actual oracle $\mathcal{O}$ that can find the minimum of a perturbed sum of non-convex functions. In this case, we need an oracle that can minimize the sub of deep neural networks. While SGD is remarkably effective in practice, we are unable to provide guarantees (probabilistic or otherwise) about how close the convergent solution SGD outputs is to the global minima. Recent work suggests that this is not a problem in practice, as spurious local minima (local minima significantly worse than the global minima) get exponentially rarer as the network gets larger \cite{CHM+14}. However, these results still rely on too many impractical assumptions to make them relevant in practice. 

However, while we cannot guarantee (or even certify) convergence to an approximate global minima in general, we can take advantage of the specific structure of the query release problem. In the query release problem (unlike most deep learning problems) we know \emph{exactly} what the global minima of loss for the generator is: 1/2 -- the loss from outputting the true, sensitive dataset. \todo{prove this}.

We can use this to track the regret bounds \todo{Explain this better in prose}

\begin{algorithm}[H]
    \KwIn{one-layer discriminator $D_\theta$, deep generator $G_\phi$, offline optimization oracle $\mathcal{O}$, Rounds $T$, noise $\eta$, output dimension $d$, game objective $M$}
    \KwResult{Trained generator $G_T$, Accuracy $\alpha$}
    \caption{QueryGAN}
    \For{$t \in 1...T$}{
        Draw discriminator and generator perturbations \\
        $\sigma_1 \sim Exp(\eta)^d ~~ and ~~ \sigma_2 \sim Exp(\eta)^d$ \\~\\

        Update D and G according to FTPL: \\
        $\theta_{t+1} \leftarrow \theta_t - \nabla_{\theta_t}\left( \sum_{t=1}^T f_t + \sigma_1^Tx \right) ~~~ \text{and} ~~~ \phi_{t+1} \leftarrow \phi_t - \nabla_{\phi_t}\left(\sum_{t=1}^T g_t + \sigma_2^Tx \right)$ \\~\\

        Update losses:\\
        $f_{t+1}(\cdot) = M(\cdot, D_{\phi_{t+1}}) ~~~ and ~~~ g_{t+1}(\cdot) = M(D_{\phi_{t+1}}, \cdot)$ \\~\\

        Store generator loss:

        $\alpha_{t+1} \leftarrow f_{t+1}(G_{\phi_{t+1}}) - 1/2$ 
    }

    \Return{Mixed strategy: $G \sim Unif\{G_{\theta_1}, G_{\theta_T}\}$, Regret: $\sum_{t \in T} \alpha_T$}

\end{algorithm}

\begin{theorem}
    Let $G, \alpha$ be the results of running QueryGAN with inputs \todo{What inputs}. Let $x$ be the dataset sampled from $G$

    \begin{equation}
        x := \{G(z_1),...,G(z_n)\} ~~~ \text{where } z \sim P_z^n
    \end{equation}\todo{what is $P_z$}

    Then $x$ is $\alpha-$approximate with respect to all queries representable by $D$.
\end{theorem}




% Unfortunately, the value of the game $V$ is not convex-concave, and therefore \todo{Theorem on no regret games solving convex concave} does not apply directly. However, Grnarova et al showed that when the discriminator $D$ is a single-layer neural network, $V$ becomes concave with respect to $D$, or \emph{semi-concave} \cite{GLL+17}. 

\begin{itemize}
    \item Replace $D$ with multiplicative weights
    \item Replace $D$ with 
\end{itemize}


\bibliographystyle{plain}
\bibliography{../works-cited.bib}

\newpage

\appendix

\end{document}
