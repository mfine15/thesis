\documentclass[]{article}

\usepackage{amssymb,amsmath}
\usepackage{hyperref}


\usepackage{graphicx,subfigure}
\usepackage{epsfig}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{url}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{boxedminipage}
\usepackage{tikz}
\usepackage{mathabx}
\usepackage{tabularx,ragged2e,booktabs,caption}
%\usepackage[linesnumbered, ruled, vlined]{algorithm2e}

\usepackage{fullpage}
\usepackage{todonotes}

\usepackage[preprint]{neurips_2019}
\usepackage{xcolor}
%  \pagecolor[RGB]{38,50,56} \color[RGB]{225,246,246}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\Oracle}{\mathcal{O}}
\newcommand{\B}{\{0,1\}}
\newcommand{\ignore}[1]{}
% \newcommand{\todo}[1]{}
% \renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}




\usepackage{amsthm}
\usepackage{amsfonts}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\author{%
  Michael Fine
  Harvard University\\
  Cambridge MA, 02138 \\
  \texttt{mfine@college.harvard.edu} 
}
\title{Principled Private Data Release with Deep Learning}



\begin{document}
\maketitle



\section{Background}

\subsection{Query Release Problem}

We study the problem of privately generating synthetic data to answer statistical queries over a data universe $\X$. Formally, a statistical query over $\X$ is a function $q: \X \to \B$. Given a a dataset $x \in \X^n$, we define $q(x) = \sum_{x_i \in x} q(x_i)$.

Our goal is to produce a synthetic dataset that, for every query in some family of queries, takes approximately the same value as the true dataset.

\begin{definition}[$\alpha-$approximate]
    We say a synthetic dataset $x$ $\alpha-$approximates a true dataset $\hat x$ w.r.t a family of statistical queries $\Q$ if

    \begin{equation}
        \forall q \in \Q: ~~~ |q(x) - q(\hat x)| \leq \alpha
    \end{equation}
\end{definition}

[TODO finish up]

\subsection{Game Theoretic Formulation}

One can formulate the problem of producing an $\alpha-$approximate dataset as a two-player, zero sum game \cite{HRU13} between a discriminator $D$ and a generator $G$. The generator has an action set $\X$, while the discriminator has an action set $\Q$. The generator aims to output a dataset $x \in \X$ that maximally agrees with $\hat x$, while the discriminator aims to find queries $q \in \Q$ that distinguish $\hat x$ and $x$.

Formally, given a play $x \in \X$ and $q \in Q$, the discriminator gets payoff $V(x,q)$ and the generator gets payoff $-V(x,q)$, where $V(x,q)$ denotes:

\begin{definition}[Payoff]
    \begin{equation}
        V(x,q) := |q(x) - q(\hat x)|
    \end{equation}
\end{definition}

The goal of both $G$ and $D$ is to maximize their worst case payoffs, thus

\begin{equation}
    \max_{q \in \Q} \min_{x \in X} V(x,q) ~~ (\text{Goal of } D) ~~~~ and ~~~~ 
    \min_{x \in X} \max_{q \in \Q} V(x,q) ~~ (\text{Goal of } G) 
\end{equation}

If there exists a point $(x^*, q^*)$ such that neither $G$ nor $D$ can improve their payoffs by playing a different move, we call that a \emph{Pure Nash Equilibrium}. Unfortunately, a pure equilibrium is not always guaranteed to exist (and likely does not in the case of synthetic data generation). 

However, the seminal work of Nash et. al showed that there always exists a \emph{Mixed Nash Equilibrium (MNE)}, where the players play \emph{probability distributions} over their action sets, instead of fixed actions. 

Let $\Delta(\X)$ and $\Delta(\Q)$ denote the set of probability distribution over $\X$ and $\Q$. Formally, if $G$ plays a strategy $g \in \Delta(\X)$ and $D$ plays  $d \in \Delta(\Q)$, we define the payoff to be the expected value of a single draw:

\begin{equation}
    V(g,d) := \mathbb{E}_{x \sim g, q \sim d} V(x,q)
\end{equation}

Thus, a pair of strategies $g \in \Delta(\X)$ and $d \in \Delta(\Q)$ forms an $\alpha-$approximate mixed nash equilibrium if for all strategies $u \in \Delta(\X)$ and $w \in \Delta(\Q)$

\begin{equation}
    V(g, w) \leq V(u,w) + \alpha ~~~~ and ~~~~ V(u, d) \leq V(u,w) - \alpha 
\end{equation}

Moreover, Gaboardi et. al showed how to reduce the problem of finding an $\alpha-$approximate dataset to the problem of finding an an $\alpha-$equilibrium in the query release game:

\begin{theorem}
    Let $(u,w)$ be the $\alpha-$approximate MNE in a query release game for a dataset $\hat x \in \X$ and a query universe $\Q$. If $\Q$ is closed under negation, then the dataset $S$ sampled from $u$ $\alpha$-approximates $\hat x$ over $Q$. \cite{GAH+14}
\end{theorem}

Hence, our task is to provide an algorithm to private reach an $\alpha-$MNE in the query release game. In the following section, we will provide the background for how this can be done with GANs.



\subsection{Generative Adversarial Networks}

Generative Adversarial Networks (GANs) \todo{}


\subsection{Online Learning}

To efficiently find equilibrium in the zero-sum GAN game, we draw on results from online learning. In the online learning setting, in each of $T$ rounds a player is given a loss function $f_t$ , possibly adversarial chosen. The players goal is to chose an action $x_{t+1} \in \mathcal{X}$ in order to minimize the cumulative $\emph{regret}$. 

\begin{definition}[Regret]
    The regret measures the cumulative loss of the player, compared to the best fixed decision in hindsight. 

    \begin{equation}
        \text{Regret}_T(f_1,...,f_T) = \sum_{t=1}^T f_t(x_t) - min_{x \in \mathcal{X}} \sum_{t=1}^T f_t(x^*)
    \end{equation}
\end{definition}

When a strategy provably leads to regret is sublinear in $T$, we call that \emph{no-regret}, as regret $\to 0$ as $T \to \infty$.

One approach to regret minimization is to choose the action $x_{t+1}$ that minimizes the cumulative loss over all past loss functions
\begin{equation}
    x_{t+1} = \arg\min_{x \in \X} \sum_{t=1}^T f_t(x)
\end{equation}
This approach is known as \emph{Follow-The-Leader}. While natural, this approach is easily exploitable by an adversary. At a high level, this is because it \emph{overfits} to past outcomes, allowing it to optimize between suboptimal strategies. To rectify this, a powerful strategy is \emph{Follow-the-Regularized-Leader}

\begin{definition}[Follow-The-Reguralized-Leader (FTRL)]
    Given a reguralization function $R(x)$ and a regularization weight $\eta_T$, at each step choose $x_{t+1} \in \X$ to minimize the regularized cumulative loss:

    \begin{equation}
        x_{t+1} = \arg\min_{x \in \X} \sum_{t=1}^T + \eta_T R(X)
    \end{equation}

    One common regularization function is the $l_2$ norm $R(x) = ||x||_2$.  
\end{definition}

When the loss function $f_T$ is convex, FTRL can be shown to be no-regret \cite{Haz19}. Unfortunately, in the GAN setting, where the loss function $f_T$ is defined by a highly non-convex deep network, we don't have that guarantee. 

\subsubsection{Online Learning for Non-convex losses}
In general, finding the minimum of a sum of non-convex functions is hard. However, in practice gradient descent over neural networks has proven to be remarkably effective at approximately solving non-convex loss functions. This leads to the natural question: assuming we have an offline non-convex optimization oracle $\mathcal{O}$, can we use that to find a no-regret strategy in the online setting? Agarwal et al showed that we can, using a Follow-The-Leader variant known as Follow-The-Perturbed-Leader \cite{AGH18}. Formally:

\begin{definition}[Offline optimization oracle]
    Let $\mathcal{O}$ take a sequence of (possibly non-convex) loss functions $(f_1...f_T) \in \mathcal{L}^T$ and a d-dimensional vector $d$, and output $x^* \in \X$
    \begin{equation}
        x^* \in \arg\min_{x \in \X} \sum_{t=1}^Tf_t(x) + \sigma^Tx
    \end{equation}
\end{definition}

If we relax the requirement to allow $\mathcal{O}$ to output an approximate minimizer $x^*$

\begin{equation}
    x^* \leq \left(\arg\min_{x \in \X} \sum_{t=1}^Tf_t(x) + \sigma^Tx\right) + \alpha
\end{equation}

we then call $\mathcal{O}$ an $\alpha-$approximate offline oracle.

\todo{Extend \cite{AGH18} defn of $\O$ to be within $\alpha$}

We can use this offline oracle $\mathcal{O}$ to minimize regret in the online case:

\begin{definition}[Follow-The-Perturbed-Leader (FTPL)]
    Given an offline oracle $\mathcal{O}$ and a parameter $\eta$, at each step $t$ FTPL draws a random vector $\sigma_t \sim Exp(\eta)^d$. It then outputs 
    \begin{equation}
        x^* \in \arg\min_{x \in \X} \sum_{t=1}^Tf_t(x) + \sigma^Tx
    \end{equation}
\end{definition}

\begin{theorem}
    FTPL has sublinear regret. \cite{AGH18}
\end{theorem}

Building on the work of Freund and Schapire, we can show that if $G$ and $D$ both play FTPL for $T$ rounds, they will converge to an $\alpha-$approximate equilibrium. Formally:

\begin{theorem}\label{ftpl-equilibriu}
    Suppose that $G$ and $D$ play according to FTPL. We can choose $T \in poly(d)/\alpha^3$ such that the expected average regret of FTPL is at most $\alpha$. Then, $G_T$ and $D_T$ produce strategies in an $\alpha-$approximate equilibrium \cite{AGH18}.
\end{theorem}

\section{Results}
\todo{summarize approach}

\todo{Show that GAN objective is concave wrt discriminator paramters ala \cite{GLL+17}}
Theorem 3 requires the existence of an actual oracle $\mathcal{O}$ that can find the minimum of a perturbed sum of non-convex functions. In this case, we need an oracle that can minimize the sub of deep neural networks. While SGD is remarkably effective in practice, we are unable to provide guarantees (probabilistic or otherwise) about how close the convergent solution SGD outputs is to the global minima. Recent work suggests that this is not a problem in practice, as spurious local minima (local minima significantly worse than the global minima) get exponentially rarer as the network gets larger \cite{CHM+14}. However, these results still rely on too many impractical assumptions to make them relevant in practice. 

However, while we cannot guarantee (or even certify) convergence to an approximate global minima in general, we can take advantage of the specific structure of the query release problem. 

\begin{theorem}
    For any discriminator, the optimal generator $G^*$ has payoff $-V(g,d) = 0$. One such generator draws a uniform sample from the true dataset $\hat x$. 
\end{theorem}
\todo{Make clearer the distinction between queries over single rows and queries over all rows, }
\begin{proof}
    Recall that the generators payoff is  $-V(g,d)$, where the value $V$ is defined as $V(g,d) := \mathbb{E}_{x \sim g, q \sim d} |q(x) - q(\hat x)|$, where $\hat x$ is the true, sensitive dataset. Clearly, the generator's payoff is at most $0$. This is trivially attaiable if $g$ is the uniform distribution over the rows of $\hat x$ \todo{Maybe another line proving this}.
\end{proof}

\todo{Talk about how this relies on discrimiantor optimality}

Theorem 4 allows us to track how close to optimal the generator $G_i$ is at each step. Assuming $D_i$ is optimal, the regret at each step is simply the generator loss $V(G_i, D_i)$. The cumulative regret is simply the sum of the generator loss at each step. 

This gives us a way to track \emph{generator} optimality, assuming the discriminator is optimal. Unlike the generator, there is no obvious maximum payoff for the discriminator in general. However, if we restrict the class of discriminators to single layer neural networks parametrized by $\theta$:

\begin{equation}
    D_\theta(x) = D_\theta(x) = \sigma(\theta^Tx + b)
\end{equation}

then $V(G, D_\theta)$ becomes convex with respect to $\theta$, and we can use standard gradient descent methods to guarantee convergence to a global optimum. 

\begin{algorithm}[H]
    \KwIn{one-layer discriminator $D_\theta$, deep generator $G_\phi$, offline optimization oracle $\mathcal{O}$, Rounds $T$, noise $\eta$, output dimension $d$, game objective $V$}
    \KwResult{Trained generator $G_T$, Accuracy $\alpha$}
    \caption{QueryGAN}
    \For{$t \in 1...T$}{
        Draw discriminator and generator perturbations \\
        $\sigma_1 \sim Exp(\eta)^d ~~ and ~~ \sigma_2 \sim Exp(\eta)^d$ \\~\\

        Update D and G according to FTPL: \\
        $\theta_{t+1} \leftarrow \theta_t - \nabla_{\theta_t}\left( \sum_{t=1}^T f_t + \sigma_1^Tx \right) ~~~ \text{and} ~~~ \phi_{t+1} \leftarrow \phi_t - \nabla_{\phi_t}\left(\sum_{t=1}^T g_t + \sigma_2^Tx \right)$ \\~\\

        Update losses:\\
        $f_{t+1}(\cdot) = V(\cdot, D_{\phi_{t+1}}) ~~~ and ~~~ g_{t+1}(\cdot) = V(G_{\theta_{t+1}}, \cdot)$ \\~\\


    }

    Calculate cumulative regret: 
    $R \leftarrow \sum_{t \in T} f_{t}(G_{\phi_{t}})$ \\


    \Return{Mixed strategy: $G \sim Unif\{G_{\theta_1}, G_{\theta_T}\}$, Regret: $R$}

\end{algorithm}

\begin{theorem}
    Let $G, \alpha$ be the results of running QueryGAN with inputs \todo{What inputs}. Let $x$ be the dataset sampled from $G$

    \begin{equation}
    x := \{G(z_1),...,G(z_n)\} ~~~ \text{where } z \sim P_z^n
    \end{equation}\todo{what is $P_z$}

    Then $x$ is $\alpha-$approximate with respect to all queries representable by $D$.
\end{theorem}

\begin{proof}
    \todo{prove}
\end{proof}

\todo{Explain context, tracking performance}

\subsection{QueryGAN privacy}
Privacy is ensured by the addition of exponentially distributed noise $\sigma_1, \sigma_2$. Interestingly, the original purpose of this noise is not privacy, but rather to ensure convergence of the online algorithm. Because of the deep connections between differential privacy and online learning \cite{NRVW19} \cite{GHM19}, however, this noise also perfectly ensures $\epsilon$-differential privacy. 

\subsubsection{Tracking privacy loss with moments accountant}
\todo{}

\subsubsection{Reporting Regret Bounds}
\begin{itemize}
    \item \todo{GAN privacy}
    \item \todo{Talk about privately reporting $\alpha$ with report noisy max}
    \item \todo{Maybe PATE-GAN}
\end{itemize}

\subsection{Extensions}
\subsubsection{Marginals}

While constraining $D$ to be a one layer linear discriminator is restricting, it still is capable of representing a number of query families of interest. Specifically, $D$ contains all $k$-way marginals.

\begin{definition}[Marginal]
    A marginal $m: \X \to \B$ over a row $x \in \{0,1\}^m$ is a monotone conjunction, parametrized by some subset $S$ of the input features.  

    \begin{equation}
        m_S(x) = \prod_{i \in S} x_i
    \end{equation}
    
    We extend this to a dataset $X$ of $n$ rows by defining $m(X) = \sum_{x \in X} m(x)$. A k-way marginal is a marginal restricted to $k$ features.
\end{definition} \cite{DR13}

A k-way marginal can be thought of as counting the number of rows with the same value in the chosen $k$ features. Marginals are a useful way of providing a synopsis of a dataset that still captures complex relationships between features. Producing a differentially private synthetic dataset that agrees with all $k-$way marginals of the true dataset is an extremely well studied problem in the field \todo{Survey marginal results}. \todo{Impossibility results}.

However, we can show that if $QueryGAN$ succeeds, it is able to match all $k-way$ marginals. This follows from the fact that a linear discriminator can contain all marginals.

\todo{define sigmoid}
\begin{theorem}
    Let $D$ be single layer discriminator parametrized by $\theta$ with a sigmoid activation function s.t. $D_\theta(x) = \sigma(\theta^Tx + b)$. For any single-row marginal $m$, there exists $\theta, b$ s.t. $D_\theta(x) = m(x)$ for all $x$.  
\end{theorem}

\begin{proof}
    This follows from the definition of a marginal. Let $m_S$ be the marginal over the features $S$. Let $\theta_i = $ \todo{Doesn't work because sigmoid is weird, need to rething activation}
\end{proof}

\begin{theorem}
    Let $G, \alpha$ be the output of running $QueryGAN$ with \todo{Parameters, steps etc}. Then $G$ will generate a dataset with all marginal counts accurate to within $\alpha$.
\end{theorem}


The proof of this theorem follows directly from Theorem 4 and 5. 

\todo{Oracle runtime (also comment on non-oracle runtime)}


\todo{Summary, contextualize empirical results}

% Unfortunately, the value of the game $V$ is not convex-concave, and therefore \todo{Theorem on no regret games solving convex concave} does not apply directly. However, Grnarova et al showed that when the discriminator $D$ is a single-layer neural network, $V$ becomes concave with respect to $D$, or \emph{semi-concave} \cite{GLL+17}. 

\subsection{QueryGAN with alternate discriminators}

While QueryGAN follows the standard GAN practice of representing both the generator and discriminator with a neural network, this is not mandatory. Indeed, given that $D$ is represented by the almost trivially simple 1 layer network, $D$ is best understood in more general terms than a neural network.

\begin{definition}[Tractable Discriminator Set]
    We say a class of functions $\mathcal{F}: \X \to \B$ parametrizes a set of tractable discriminators w.r.t a class of queries $\Q$ iff

    \begin{enumerate}
        \item $\Q \subseteq \mathcal{F}$
        \item There exists a tractable offline oracle $\mathcal{O}$ \todo{What does it do (also define tractable)}
    \end{enumerate}
\end{definition}

As shown above, the set $\mathcal{F}_{single}$ of one layer neural networks is a tractable discriminator set for all marginals (as well as all sigmoided linear functions in general).

\subsubsection{Multiplicative Weights}

Consider the renowned Multiplicative Weights algorithm. 

\subsection{TODO}
\begin{itemize}
    \item Replace $D$ with multiplicative weights
    \item Local DP GAN
\end{itemize}


\bibliographystyle{plain}
\bibliography{../works-cited.bib}

\newpage

\appendix

\end{document}
