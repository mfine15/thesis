\documentclass[]{article}

\usepackage{amssymb,amsmath}
\usepackage{hyperref}


\usepackage{graphicx,subfigure}
\usepackage{epsfig}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{bbm}

\usepackage{amssymb}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{url}
\usepackage{enumerate}
\usepackage{amsfonts}
\usepackage{boxedminipage}
\usepackage{tikz}
\usepackage{mathabx}
\usepackage[margin=1.75in]{geometry}
\usepackage{tabularx,ragged2e,booktabs,caption}
\usepackage{cleveref}
%\usepackage[linesnumbered, ruled, vlined]{algorithm2e}

% \usepackage{fullpage}
\usepackage{todonotes}

% \usepackage[preprint]{neurips_2019}
\usepackage{lmodern}
\usepackage{xcolor}
%  \pagecolor[RGB]{38,50,56} \color[RGB]{225,246,246}

\newcommand{\X}{\mathcal{X}}
\newcommand{\Q}{\mathcal{Q}}
\newcommand{\F}{\mathcal{F}}

\newcommand{\Oracle}{\mathcal{O}}
\newcommand{\B}{\{0,1\}}
\newcommand{\ignore}[1]{}

\newcommand{\E}{\mathbb{E}}
\newcommand{\p}{\mathbb{P}}
% \newcommand{\todo}[1]{}
% \renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}




\usepackage{amsthm}
\usepackage{amsfonts}

\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]

\theoremstyle{definition}
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]

\author{%
  Michael Fine
  Harvard University\\
  Cambridge MA, 02138 \\
  \texttt{mfine@college.harvard.edu} 
}
\title{Principled Private Data Release with Deep Learning}



\begin{document}
\maketitle
\setcounter{tocdepth}{2}
\tableofcontents

\section{Introduction}

\section{Related Work}
\begin{itemize}
    \item \cite{JY19}
    \item \cite{GLL+17}
    \item \cite{NRVW19}
    \item \cite{AGH18}
    \item \cite{NRW18}
    \item \cite{GAH+14}
    \item \cite{HLM12}
\end{itemize}
\section{Background}
\subsection{Differential Privacy}
\subsection{Query Release Problem}

We study the problem of privately generating synthetic data to answer statistical queries over a data universe $\X$. Formally, a statistical query over $\X$ is a function $q: \X \to \B$. Given a a dataset $x \in \X^n$, we define $q(x) := \sum_{i=0}^n q(x_i)$. For convenience, we will often normalize queries to take values $\in [0,1]$

\begin{equation}
    q(x) := \frac{1}{n}\sum_{i=0}^n q(x_i) = \E_{x_i \in \X} q(x_i)
\end{equation}

Our goal is to produce a synthetic dataset that, for every query in some family of queries, takes approximately the same value as the true dataset.

\begin{definition}[$\alpha-$approximate]
    We say a synthetic dataset $x$ $\alpha-$approximates a true dataset $\hat x$ w.r.t a family of statistical queries $\Q$ if

    \begin{equation}
        \forall q \in \Q: ~~~ |q(x) - q(\hat x)| \leq \alpha
    \end{equation}
\end{definition}

\subsection{Game Theoretic Formulation}

One can formulate the problem of producing an $\alpha-$approximate dataset as a two-player, zero sum game \cite{HRU13} between a discriminator $D$ and a generator $G$. The generator has an action set $\X$, while the discriminator has an action set $\Q$. The generator aims to output a dataset $x \in \X$ that maximally agrees with $\hat x$, while the discriminator aims to find queries $q \in \Q$ that distinguish $\hat x$ and $x$.

Formally, given a play $x \in \X$ and $q \in Q$, the discriminator gets payoff $V(x,q)$ and the generator gets payoff $-V(x,q)$, where $V(x,q)$ denotes:

\begin{definition}[Payoff]
    \begin{equation}
        V(x,q) := q(x) - q(\hat x)
    \end{equation}
\end{definition}

The goal of both $G$ and $D$ is to maximize their worst case payoffs, thus

\begin{equation}
    \max_{q \in \Q} \min_{x \in X} V(x,q) ~~ (\text{Goal of } D) ~~~~ and ~~~~ 
    \min_{x \in X} \max_{q \in \Q} V(x,q) ~~ (\text{Goal of } G) 
\end{equation}

If there exists a point $(x^*, q^*)$ such that neither $G$ nor $D$ can improve their payoffs by playing a different move, we call that a \emph{Pure Nash Equilibrium}. Unfortunately, a pure equilibrium is not always guaranteed to exist (and likely does not in the case of synthetic data generation). 

However, the seminal work of Nash et. al showed that there always exists a \emph{Mixed Nash Equilibrium (MNE)}, where the players play \emph{probability distributions} over their action sets, instead of fixed actions. 

Let $\Delta(\X)$ and $\Delta(\Q)$ denote the set of probability distribution over $\X$ and $\Q$. Formally, if $G$ plays a strategy $g \in \Delta(\X)$ and $D$ plays  $d \in \Delta(\Q)$, we define the payoff to be the expected value of a single draw:

\begin{equation}
    V(g,d) := \mathbb{E}_{x \sim g, q \sim d} V(x,q)
\end{equation}

Thus, a pair of strategies $g \in \Delta(\X)$ and $d \in \Delta(\Q)$ forms an $\alpha-$approximate mixed nash equilibrium if for all strategies $u \in \Delta(\X)$ and $w \in \Delta(\Q)$

\begin{equation}
    V(g, w) \leq V(u,w) + \alpha ~~~~ and ~~~~ V(u, d) \leq V(u,w) - \alpha 
\end{equation}

Moreover, Gaboardi et. al showed how to reduce the problem of finding an $\alpha-$approximate dataset to the problem of finding an an $\alpha-$equilibrium in the query release game:

\begin{theorem}
    Let $(u,w)$ be the $\alpha-$approximate MNE in a query release game for a dataset $\hat x \in \X$ and a query universe $\Q$. If $\Q$ is closed under negation, then the dataset $S$ sampled from $u$ $\alpha$-approximates $\hat x$ over $Q$. \cite{GAH+14}
\end{theorem}

\todo{Heuristics paper summarized this better and more concisely}

Hence, our task is to provide an algorithm to private reach an $\alpha-$MNE in the query release game. In the following section, we will provide the background for how this can be done with GANs.

\subsection{Online Learning}

To efficiently find equilibrium in the zero-sum GAN game, we draw on results from online learning. In the online learning setting, in each of $T$ rounds a player is given a loss function $f_t$ , possibly adversarial chosen. The players goal is to chose an action $x_{t+1} \in \mathcal{X}$ in order to minimize the cumulative $\emph{regret}$. 

\begin{definition}[Regret]
    The regret measures the cumulative loss of the player, compared to the best fixed decision in hindsight. 

    \begin{equation}
        \text{Regret}_T(f_1,...,f_T) = \sum_{t=1}^T f_t(x_t) - min_{x \in \mathcal{X}} \sum_{t=1}^T f_t(x^*)
    \end{equation}
\end{definition}

When a strategy provably leads to regret is sublinear in $T$, we call that \emph{no-regret}, as regret $\to 0$ as $T \to \infty$. One approach to regret minimization is to choose the action $x_{t+1}$ that minimizes the cumulative loss over all past loss functions l
\begin{equation}
    x_{t+1} = \arg\min_{x \in \X} \sum_{t=1}^T f_t(x)
\end{equation}
This approach is known as \emph{Follow-The-Leader}. While natural, this approach is easily exploitable by an adversary. At a high level, this is because it \emph{overfits} to past outcomes, allowing it to optimize between suboptimal strategies. To rectify this, a powerful strategy is \emph{Follow-the-Regularized-Leader}

\begin{definition}[Follow-The-Reguralized-Leader (FTRL)]
    Given a reguralization function $R(x)$ and a regularization weight $\eta_T$, at each step choose $x_{t+1} \in \X$ to minimize the regularized cumulative loss:

    \begin{equation}
        x_{t+1} = \arg\min_{x \in \X} \sum_{t=1}^T + \eta_T R(X)
    \end{equation}

    One common regularization function is the $l_2$ norm $R(x) = ||x||_2$.  
\end{definition}

When the loss function $f_T$ is convex, FTRL can be shown to be no-regret \cite{Haz19}. Unfortunately, in the GAN setting, where the loss function $f_T$ is defined by a highly non-convex deep network, we don't have that guarantee. 

\subsubsection{Online Learning for Non-convex losses}
In general, finding the minimum of a sum of non-convex functions is hard. However, in practice gradient descent over neural networks has proven to be remarkably effective at approximately solving non-convex loss functions. This leads to the natural question: assuming we have an offline non-convex optimization oracle $\mathcal{O}$, can we use that to find a no-regret strategy in the online setting? Agarwal et al showed that we can, using a Follow-The-Leader variant known as Follow-The-Perturbed-Leader \cite{AGH18}. Formally:

\begin{definition}[Offline optimization oracle]
    Let $\mathcal{O}$ take a sequence of (possibly non-convex) loss functions $(f_1...f_T) \in \mathcal{L}^T$ and a d-dimensional vector $d$, and output $x^* \in \X$
    \begin{equation}
        x^* \in \arg\min_{x \in \X} \sum_{t=1}^Tf_t(x) + \sigma^Tx
    \end{equation}
\end{definition}

If we relax the requirement to allow $\mathcal{O}$ to output an approximate minimizer $x^*$

\begin{equation}
    x^* \leq \arg\min_{x \in \X} \left(\sum_{t=1}^Tf_t(x) + \sigma^Tx\right) + \alpha
\end{equation}
\todo{It's not that $x^* \leq \alpha$, it's that $f(x^*)$ should be within $\alpha$}
we then call $\mathcal{O}$ an $\alpha-$approximate offline oracle.

We can use this offline oracle $\mathcal{O}$ to minimize regret in the online case:

\begin{definition}[Follow-The-Perturbed-Leader (FTPL)]
    Given an offline oracle $\mathcal{O}$ and a parameter $\eta$, at each step $t$ FTPL draws a random vector $\sigma_t \sim Exp(\eta)^d$. It then outputs 
    \begin{equation}
        x^* \in \arg\min_{x \in \X} \sum_{t=1}^Tf_t(x) + \sigma^Tx
    \end{equation}
\end{definition}

\begin{theorem}
    FTPL has sublinear regret. \cite{AGH18}
\end{theorem}

Building on the work of Freund and Schapire, we can show that if $G$ and $D$ both play FTPL for $T$ rounds, they will converge to an $\alpha-$approximate equilibrium. Formally:

\begin{theorem}\label{ftpl-equilibriu}
    Suppose that $G$ and $D$ play according to FTPL. We can choose $T \in poly(d)/\alpha^3$ such that the expected average regret of FTPL is at most $\alpha$. Then, $G_T$ and $D_T$ produce strategies in an $\alpha-$approximate equilibrium \cite{AGH18}.
\end{theorem}


\subsection{Generative Adversarial Networks}

Generative Adversarial Networks (GANs), introduced by Goodfellow et. al, are an approach to generative deep learning that has shown remarkable promise in generating high fidelity samples \cite{GPM+14}. In the GAN setup, a generator $G$ is paired with a discriminator $D$. At each round, $D$ is trained to distinguish real samples drawn from $P_{data}$ from generated samples drawn from $P_g$, while $G$ is trained to generate realistic samples that fool the discriminator. 

This yields a two player, zero sum game with minimax objective

\begin{equation}
    \min_G \max_D V_{gan}(G,D):=\frac{1}{2} \mathbb{E}_{\mathbf{x} \sim p_{data}} \log D(\mathbf{x})+\frac{1}{2} \mathbb{E}_{z \sim p_{z}} \log (1-D(G(z)))
\end{equation}


However, \cite{ACB17} showed that this cost function $V_{gan}$ is not sensible cost function in practice, when the distributions are supported by low-dimensional manifolds. Instead, they proposed to use the Earth-Mover, or Wasserstein-1 objective.

    \begin{definition}[Earth Mover Distance]
        The $EM$ distance between two distribution $\mathbb{P}_r$ and $\mathbb{P}_g$ is 

        \begin{equation}
            W(\mathbb{P}_r, \mathbb{P}_g):=\inf _{\gamma \in \Pi\left(\mathbb{P}_r, \mathbb{P}_g\right)} \mathbb{E}_{(x, y) \sim \gamma}|x-y|
        \end{equation}

        where $\Pi\left(\mathbb{P}_r, \mathbb{P}_g\right)$ denotes the set of all joint distributions $\gamma(x,y)$ whose marginals are respectively  $\mathbb{P}_r$ and $\mathbb{P}_g$.
    \end{definition}

    While this infinimum is highly intractable to compute, \cite{ACB17} used the Kantorovich-Rubinstein duality \cite{Vil08} to show that

    \begin{equation}
        W(\mathbb{P}_r, \mathbb{P}_g) = \sup _{\|f\|_L \leq 1} \mathbb{E}_{x\sim \mathbb{P}_r}[f(x)] - \mathbb{E}_{x\sim \mathbb{P}_g}[f(x)]
    \end{equation}

    where the supremum is over all $1$-Lipschitz functions.

    \begin{definition}[Lipschitz function]
        A function $f$ is said to be $L-Lipschitz$ if 

        \begin{equation}
            |f(x) - f(y)| \leq C|x-y|
        \end{equation}

        for all x,y in the domain
    \end{definition}
    
    
    Thus, if our discriminators are parametrized by a family of $1$-Lipschitz functions $\mathcal{D}$, the Wasserstein GAN objective is
    
    \begin{equation}
        \min_{G \in \mathcal{G}} \max_{D \in \mathcal{D}} \mathbb{E}_{\mathbf{x} \sim p_{data}} [D(\mathbf{x})] - \mathbb{E}_{z \sim p_{z}} [D(G(z))]
    \end{equation}

\begin{remark}
    In the Wasserstein GAN, the discriminator is no longer guaranteed to output values in $[0,1]$, and therefore cannot be interpreted as a probability. For this reason, the Wasserstein discriminator is typically called a \emph{critic}. \todo{Elaborate, maybe cite}. 
\end{remark}

Note the remarkable similarity of the GAN objective $V_{gan}$ to the earlier query release objective $V$. Recall that the mixed strategy $V$ is defined as 

\begin{align*}
    \min_{\p_g \in \nabla \X} \max_{\p_q \in \nabla\Q }V(g,d) &= \E_{x \sim \p_g}[q(x)] - \E_{x \sim \p_{data}}[q(x)]
\end{align*} \todo{Query player still doesn't work there}

Thus, achieving equilibrium in the Wasserstein GAN is equivalent to solving the query release problem, for all queries representable by the discriminator. Importantly, for the Wasserstein GAN this limits us to all queries that are $1-Lipschitz$ over each row of the output. In this context, that may prove to be infeasibly limiting. \todo{Explain why this works for images but not for queries}.

\todo{Segue}

\subsubsection{Integral Probability Metrics}

Many GAN variants can be understood as minimizing the distance between the two distributions $\p_{synth}$ and $\p_{real}$, measured by some Integral Probability Metric.

\begin{definition}[Integral Probability Metric (IPM) \cite{Mul97}]
    An IPM $\rho_\F$between two distributions $\mathbb{P}$ and $\mathbb{Q}$ is 

    \begin{equation}
        \rho_\F(\mathbb{P}, \mathbb{Q})  := \sup_{f \in \mathcal{F}} \big| \E_{x \sim \p}[f(X)] - \E_{x \sim \mathbb{Q}}[f(X)] \big|
    \end{equation}

    where $\F$ is some class of real-valued bounded measurable functions.
\end{definition}

Depending on how we constrain $\mathcal{F}$, we can recover a number of GAN architectures \cite{zotero-1283}: \todo{Explain why we need to constrain at all}

\begin{itemize}
    \item $\F = \{f: \|f\|_L \leq 1\}$ gives us Wasserstein GAN \cite{ACB17}
    \item $\F = \{f: \|f\|_\infty \leq 1 \}$ gives us Total Variation distance, as seen is Energy Based GAN \cite{ZML17}
    \item $\F = \{f: \|f\|_\mathcal{H} \leq 1 \}$ for some RKHS $\mathcal{H}$  gives us Maximum Mean Discrepancy, as seen in GMMN \cite{LSZ15a}
\end{itemize}

Our aim is to choose an $\F$ (possibly specific to each set of queries) that contains most queries of interest, while still allowing for easy GAN training.  
    
\section{QueryGAN}
\todo{summarize approach}

\todo{Show that GAN objective is concave wrt discriminator paramters ala \cite{GLL+17}}
Theorem 3 requires the existence of an actual oracle $\mathcal{O}$ that can find the minimum of a perturbed sum of non-convex functions. In this case, we need an oracle that can minimize the sub of deep neural networks. While SGD is remarkably effective in practice, we are unable to provide guarantees (probabilistic or otherwise) about how close the convergent solution SGD outputs is to the global minima. Recent work suggests that this is not a problem in practice, as spurious local minima (local minima significantly worse than the global minima) get exponentially rarer as the network gets larger \cite{CHM+14}. However, these results still rely on too many impractical assumptions to make them relevant in practice. 

However, while we cannot guarantee (or even certify) convergence to an approximate global minima in general, we can take advantage of the specific structure of the query release problem. 

\begin{theorem}
    For any discriminator, the optimal generator $G^*$ has payoff $-V(g,d) = 0$. One such generator draws a uniform sample from the true dataset $\hat x$. 
\end{theorem}
\todo{Make clearer the distinction between queries over single rows and queries over all rows, }
\begin{proof}
    Recall that the generators payoff is  $-V(g,d)$, where the value $V$ is defined as $V(g,d) := \mathbb{E}_{x \sim g, q \sim d} |q(x) - q(\hat x)|$, where $\hat x$ is the true, sensitive dataset. Clearly, the generator's payoff is at most $0$. This is trivially attainable if $g$ is the uniform distribution over the rows of $\hat x$ \todo{Maybe another line proving this}.
\end{proof}



Theorem 4 allows us to track how close to optimal the generator $G_i$ is at each step. Assuming $D_i$ is optimal, the regret at each step is simply the generator loss $V(G_i, D_i)$. The cumulative regret is simply the sum of the generator loss at each step. 

This gives us a way to track \emph{generator} optimality, assuming the discriminator is optimal. Unlike the generator, there is no obvious maximum payoff for the discriminator in general. 
However, if we restrict the class of discriminators to single layer neural networks parametrized by $\theta$:

\begin{equation}
    D_\theta(x) = \sigma(\theta^Tx + b)
\end{equation}

then $V(G, D_\theta)$ becomes convex with respect to $\theta$, and we can use standard gradient descent methods to guarantee convergence to a global optimum. \todo{Prove gradient descent bounds}

At each round, we update $D$ and $G$ accoring to gradient descent over FTPL. For convenience, let the notation $f_{0:t}$ denote the set of all functions $(f_0,...,f_t)$.

\begin{align}
     \mathcal{O}_d(\theta_t, f_{0:t}) &:= \theta_t - \nabla_{\theta_t} \sum_{t=1}^T f_t + \sigma_1^Tx \\
 \mathcal{O}_g(\phi_t, g_{0:t}) &:= \phi_t - \nabla_{\theta_t} \sum_{t=1}^T g_t + \sigma_2^Tx 
\end{align}

\begin{algorithm}[H]
    \KwIn{one-layer discriminator $D_\theta$, deep generator $G_\phi$, discriminator and generator oracle $\mathcal{O}_d$, $\mathcal{O}_g$, Rounds $T$, noise $\eta$, output dimension $d$, game objective $V$, output rows $N$}
    \KwResult{Dataset $x \in \X^d$, Accuracy $\alpha$}
    \caption{QueryGAN}
    \For{$t \in 1...T$}{
        Draw discriminator and generator perturbations \\
        $\sigma_1\sim Exp(\eta)^d ~~ and ~~ \sigma_2 \sim Exp(\eta)^d$ \\~\\

        Update D and G with their respective oracles: \\
        $\theta_{t+1} \leftarrow \mathcal{O}_d(\theta_t, f_{0:t}) ~~~ \text{and} ~~~ \phi_{t+1} \leftarrow \mathcal{O}_d(\phi_t, g_{0:t})$ \\~\\

        Update losses:\\
        $f_{t+1}(\cdot) = V(\cdot, D_{\phi_{t+1}}) ~~~ and ~~~ g_{t+1}(\cdot) = V(G_{\theta_{t+1}}, \cdot)$ \\~\\


    }~\\

    Calculate cumulative regret: 
    $R \leftarrow \sum_{t \in T} f_{t}(G_{\phi_{t}})$ \\~\\

    \For{$i \in 1...N$}{
        Draw $t \sim Unif([T])$ and $z \sim \mathcal{N}(0,1)$ \\
        Set $x_i \leftarrow G_{\theta_t}(z)$
    }~\\


    \Return{Dataset ${x_1,...,x_N}$, Regret: $R$}
    

\end{algorithm}


\begin{theorem}\label{thm: QueryGAN-Approx}
    Let $x, \alpha$ be the results of running QueryGAN with inputs \todo{What inputs}. Then $x$ is $\alpha-$approximate with respect to all queries representable by $D$.
\end{theorem}

\begin{proof}
    \todo{prove}
\end{proof}

\todo{Explain context, tracking performance}

\subsection{QueryGAN privacy}
Privacy is ensured by the addition of exponentially distributed noise $\sigma_1, \sigma_2$. Interestingly, the original purpose of this noise is not privacy, but rather to ensure convergence of the online algorithm. Because of the deep connections between differential privacy and online learning \cite{NRVW19} \cite{GHM19}, however, this noise also ensures $\epsilon$-differential privacy. 

\subsubsection{Exponential noise}

The standard mechanism for ensuring differential privacy is the Laplace mechanism:

\begin{definition}[Laplace Mechanism \cite{DR13}]
    Given any function $f: \mathbb{N}^{|\mathcal{X}|} \to \mathbb{R}^k$, the laplace mechanism is defined as

    \begin{equation}
        \mathcal{M}_L(x, f(\cdot), \epsilon)=f(x)+\left(Y_1, \ldots, Y_k\right)
    \end{equation}

    where each $Y_i$ are i.i.d drawn from $Lap(\nabla f / \epsilon)$
\end{definition}

It's easy to show that the Laplace mechanism preserves $(\epsilon, 0)$-differential privacy. However, in $QueryGAN$ we add noise drawn from the exponential distribution:

\begin{definition}[Exponential Distribution]
    The exponential distribution with parameter $\lambda$ is the distribution with density function

    \begin{equation}
        Exp(x; \lambda) := \left\{\begin{array}{ll}{\lambda e^{-\lambda x}} & {x \geq 0} \\ {0} & {x<0}\end{array}\right.
    \end{equation}
\end{definition}

Note that the Laplace mechanism can be seen as the symmetric form of the exponential distribution. Specifically, if $X \sim Lap(\lambda)$, then $|X| \sim Exp(1/\lambda)$. 

\todo{use \cite{NRVW19} to show privacy}

\subsubsection{Tracking privacy loss with moments accountant}
\todo{Also watch out -- if we rely on a fixed $T$, what if privacy budget is exceeded before we reach it?}

\subsubsection{Reporting Regret Bounds}
\begin{itemize}
    \item \todo{GAN privacy}
    \item \todo{Talk about privately reporting $\alpha$ with report noisy max}
    \item \todo{Maybe PATE-GAN}
\end{itemize}

\subsection{Query Classes}
\subsubsection{Marginals}

While constraining $D$ to be a one layer linear discriminator is restricting, it still is capable of representing a number of query families of interest. Specifically, $D$ contains all $k$-way marginals.

\begin{definition}[Marginal]
    A marginal $m: \X \to \B$ over a row $x \in \{0,1\}^m$ is a monotone conjunction, parametrized by some subset $S$ of the input features.  

    \begin{equation}
        m_S(x) = \prod_{i \in S} x_i
    \end{equation}
    
    We extend this to a dataset $X$ of $n$ rows by defining $m(X) = \sum_{x \in X} m(x)$. A k-way marginal is a marginal restricted to $k$ features.
\end{definition} \cite{DR13}

A k-way marginal can be thought of as counting the number of rows with the same value in the chosen $k$ features. Marginals are a useful way of providing a synopsis of a dataset that still captures complex relationships between features. Producing a differentially private synthetic dataset that agrees with all $k-$way marginals of the true dataset is an extremely well studied problem in the field \todo{Survey marginal results}. \todo{Impossibility results}.

However, we can show that if $QueryGAN$ succeeds, it is able to match all $k-way$ marginals. This follows from the fact that a linear discriminator can contain all marginals.

This theorem relies on the use of a Rectified Linear Unit activation function

\begin{definition}[ReLU]
    The ReLU activation function $R(x) : \mathbb{R} \to \mathbb{R}^+ := max(0,x)$
\end{definition}

This non-linearity allows us to approximate the nonlinear marginal query with a linear neural network:

\begin{theorem}\label{thm:d-marginal}
    Let $D$ be single layer discriminator parametrized by $\theta$ with a ReLU activation function s.t. $D_\theta(x) = \sigma(\theta^Tx + b)$. For any single-row marginal $m$, there exists $\theta, b$ s.t. $D_\theta(x) = m(x)$ for all $x$.  
\end{theorem} \todo{Show it can be trained to this}

\begin{proof}
    This follows from the definition of a marginal. Let $m_S$ be the marginal over the features $S$. Let $\theta_i = \mathbbm{1}_{i \in S}$. Setting $b = 1-|S|$, it's clear that 
    

    \begin{equation}
        D_\theta(x) + 1 = \left.
        \begin{cases}
          1, & \prod_{i \in S} x_i = 1 \\
          0, & \text{otherwise }
        \end{cases}
        \right\} = m_S(x)
    \end{equation}
\end{proof}

\begin{theorem}
    Let $G, \alpha$ be the output of running $QueryGAN$ with \todo{Parameters, steps etc}. Then $G$ will generate a dataset with all marginal counts accurate to within $\alpha$.
\end{theorem}


The proof of this theorem follows directly from \autoref{thm: QueryGAN-Approx} and \autoref{thm:d-marginal}.

\todo{Oracle runtime (also comment on non-oracle runtime)}


\todo{Summary, contextualize empirical results}

% Unfortunately, the value of the game $V$ is not convex-concave, and therefore \todo{Theorem on no regret games solving convex concave} does not apply directly. However, Grnarova et al showed that when the discriminator $D$ is a single-layer neural network, $V$ becomes concave with respect to $D$, or \emph{semi-concave} \cite{GLL+17}. 

\subsection{Practical QueryGAN Heuristics}

\subsection{Stochastic Loss Subsampling}
    Even if we assume that the offline optimization oracle $\O$ is in general good at optimizing non-convex functions, note that at each step $t$ $QueryGAN$ requires $\O$ to optimize over \emph{sums} of multiple highly non-convex neural networks

    \begin{equation}
        \sum_{t=1}^T g_t + \sigma_2^Tx 
    \end{equation}

    Thus, as $T$ grows larger in later rounds, this optimization problem becomes dramatically larger and less feasible in practice, straining our oracle assumptions. 

    One might hope that we can obtain almost as good regret guarantees by instead running our oracle over a random sample of loss functions drawn from some distribution over past loss functions. \todo{Prove}


\subsection{QueryGAN Rejection Sampling}
While we can prove sufficient regret bounds simply by sampling uniformly from $G_{1:t}$, this naive methods throws away valuable information. Specifically, the discriminators $D_{1:t}$ are able to evaluate the quality of a generated sample. If we train \todo{citations}

\subsection{Tailored Loss Functions}

\section{QueryGAN with alternate discriminators}

While QueryGAN follows the standard GAN practice of representing both the generator and discriminator with a neural network, this is not mandatory. Indeed, given that $D$ is represented by the almost trivially simple 1 layer network, $D$ is best understood in more general terms than a neural network.

\begin{definition}[Tractable Discriminator Set]
    We say a class of functions $\mathcal{F}: \X \to \B$ parametrizes a set of tractable discriminators w.r.t a class of queries $\Q$ iff

    \begin{enumerate}
        \item $\Q \subseteq \mathcal{F}$
        \item There exists a tractable offline oracle $\mathcal{O}$ \todo{What does it do (also define tractable)}
    \end{enumerate}
\end{definition}

As shown above, the set $\mathcal{F}_{single}$ of one layer neural networks is a tractable discriminator set for all marginals (as well as all sigmoided linear functions in general). This has the benefit of being easy to optimize in practice, without the runtime depending exponentially on the dimensionality of the query space. Relaxing that restriction by allowing for less efficiently optimizable discriminators lets us generate $\alpha$-accurate synthetic data for much larger query classes.

\subsubsection{Multiplicative Weights}

Consider the application of the renowned Multiplicative Weights algorithm to the query release problem, introduced in \cite{HR10}. 

\begin{definition}[Multiplicative Weight Oracle Algorithm]
    Fix a class of queries $\mathcal{Q}$ and a true dataset $\hat x$. Define an initial uniform distribution over queries $\theta_0$. Let $\mathcal{O}_{MW}(\theta_t, G_t)$ output a reweighted distribution 

    \begin{equation}
        \theta_{t+1}^q ~ \propto ~ \exp \left(-\eta V(G_t, q)\right) \cdot \theta_{t}^{q}
    \end{equation}

    for each $q \in \mathcal{Q}$
\end{definition}

$\theta_t$ defines a distribution, with each query weighted proportionally to how well it distinguishes real data from fake data. The discriminator draws  $D_{\theta_t}(x) := |q(x) - q(\hat x)$ where $q$ is a query drawn $q \sim \theta_t$. 

\subsection{TODO}
\begin{itemize}
    \item Local DP GAN
    \item FTPL oracle comes with privacy for free -- but also talk about how to handle non-private oracle
\end{itemize}

Proofs:

\begin{itemize}
    \item GAN objective =? query release objective
    \item FTPL gradient descent (not clear this is necessarily a full on proof)
    \item Proof of accuracy of subsampled FTPL objective
\end{itemize}

\subsection{Adversarial Kernel Learning}

Limiting 

\section{Empirical Results}

\section{Conclusion}

\bibliographystyle{alpha}
\bibliography{../works-cited.bib}

\newpage

\appendix

\end{document}
