
@article{DWW+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.10277},
  title = {Detecting {{Violations}} of {{Differential Privacy}}},
  abstract = {The widespread acceptance of differential privacy has led to the publication of many sophisticated algorithms for protecting privacy. However, due to the subtle nature of this privacy definition, many such algorithms have bugs that make them violate their claimed privacy. In this paper, we consider the problem of producing counterexamples for such incorrect algorithms. The counterexamples are designed to be short and human-understandable so that the counterexample generator can be used in the development process \textendash{} a developer could quickly explore variations of an algorithm and investigate where they break down. Our approach is statistical in nature. It runs a candidate algorithm many times and uses statistical tests to try to detect violations of differential privacy. An evaluation on a variety of incorrect published algorithms validates the usefulness of our approach: it correctly rejects incorrect algorithms and provides counterexamples for them within a few seconds.},
  language = {en},
  journal = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security  - CCS '18},
  doi = {10.1145/3243734.3243818},
  author = {Ding, Zeyu and Wang, Yuxin and Wang, Guanhong and Zhang, Danfeng and Kifer, Daniel},
  year = {2018},
  keywords = {Computer Science - Cryptography and Security,K.6.5},
  pages = {475-489},
  file = {/Users/mfine/Zotero/storage/I3ITSMML/Ding et al. - 2018 - Detecting Violations of Differential Privacy.pdf}
}

@inproceedings{RAW+10,
  address = {{Paris, France}},
  title = {Differential Privacy for Collaborative Security},
  isbn = {978-1-4503-0059-9},
  abstract = {Fighting global security threats with only a local view is inherently difficult. Internet network operators need to fight global phenomena such as botnets, but they are hampered by the fact that operators can observe only the traffic in their local domains. We propose a collaborative approach to this problem, in which operators share aggregate information about the traffic in their respective domains through an automated query mechanism. We argue that existing work on differential privacy and type systems can be leveraged to build a programmable query mechanism that can express a wide range of queries while limiting what can be learned about individual customers. We report on our progress towards building such a mechanism, and we discuss opportunities and challenges of the collaborative security approach.},
  language = {en},
  booktitle = {Proceedings of the {{Third European Workshop}} on {{System Security}} - {{EUROSEC}} '10},
  publisher = {{ACM Press}},
  doi = {10.1145/1752046.1752047},
  author = {Reed, Jason and Aviv, Adam J. and Wagner, Daniel and Haeberlen, Andreas and Pierce, Benjamin C. and Smith, Jonathan M.},
  year = {2010},
  pages = {1-7},
  file = {/Users/mfine/Zotero/storage/D3XRPMT6/Reed et al. - 2010 - Differential privacy for collaborative security.pdf}
}

@article{TKD11,
  title = {Formal {{Verification}} of {{Differential Privacy}} for {{Interactive Systems}} ({{Extended Abstract}})},
  volume = {276},
  issn = {15710661},
  abstract = {Differential privacy is a promising approach to privacy preserving data analysis with a well-developed theory for functions. Despite recent work on implementing systems that aim to provide differential privacy, the problem of formally verifying that these systems have differential privacy has not been adequately addressed. We develop a formal probabilistic automaton model of differential privacy for systems by adapting prior work on differential privacy for functions. We present the first sound verification technique for proving differential privacy of interactive systems. The technique is based on a form of probabilistic bisimulation relation. The novelty lies in the way we track quantitative privacy leakage bounds using a relation family instead of a single relation. We illustrate the proof technique on a representative automaton motivated by PINQ, an implemented system that is intended to provide differential privacy. Surprisingly, our analysis yields a privacy leakage bound of (2t {${_\ast}$} ) rather than (t {${_\ast}$} ) when -differentially private functions are called t times. The extra leakage arises from accounting for bounded memory constraints of real computers.},
  language = {en},
  journal = {Electronic Notes in Theoretical Computer Science},
  doi = {10.1016/j.entcs.2011.09.015},
  author = {Tschantz, Michael Carl and Kaynar, Dilsun and Datta, Anupam},
  month = sep,
  year = {2011},
  pages = {61-79},
  file = {/Users/mfine/Zotero/storage/NHYQAX72/Tschantz et al. - 2011 - Formal Verification of Differential Privacy for In.pdf}
}

@article{HPN,
  title = {Differential {{Privacy Under Fire}}},
  abstract = {Anonymizing private data before release is not enough to reliably protect privacy, as Netflix and AOL have learned to their cost. Recent research on differential privacy opens a way to obtain robust, provable privacy guarantees, and systems like PINQ and Airavat now offer convenient frameworks for processing arbitrary userspecified queries in a differentially private way. However, these systems are vulnerable to a variety of covertchannel attacks that can be exploited by an adversarial querier.},
  language = {en},
  author = {Haeberlen, Andreas and Pierce, Benjamin C and Narayan, Arjun},
  pages = {15},
  file = {/Users/mfine/Zotero/storage/DXWAZXZN/Haeberlen et al. - Differential Privacy Under Fire.pdf}
}

@inproceedings{ZMK+18,
  address = {{Houston, TX, USA}},
  title = {{{EKTELO}}: {{A Framework}} for {{Defining Differentially}}-{{Private Computations}}},
  isbn = {978-1-4503-4703-7},
  shorttitle = {{{EKTELO}}},
  abstract = {The adoption of di\dbend{}erential privacy is growing but the complexity of designing private, e\dbend{}cient and accurate algorithms is still high. We propose a novel programming framework and system, \dbend\dbend\dbend\dbend\dbend, for implementing both existing and new privacy algorithms. For the task of answering linear counting queries, we show that nearly all existing algorithms can be composed from operators, each conforming to one of a small number of operator classes. While past programming frameworks have helped to ensure the privacy of programs, the novelty of our framework is its signi\dbend{}cant support for authoring accurate and e\dbend{}cient (as well as private) programs. After describing the design and architecture of the \dbend\dbend\dbend\dbend\dbend{} system, we show that \dbend\dbend\dbend\dbend\dbend{} is expressive, allows for safer implementations through code reuse, and that it allows both privacy novices and experts to easily design algorithms. We demonstrate the use of \dbend\dbend\dbend\dbend\dbend{} by designing several new state-of-the-art algorithms.},
  language = {en},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Management}} of {{Data}}  - {{SIGMOD}} '18},
  publisher = {{ACM Press}},
  doi = {10.1145/3183713.3196921},
  author = {Zhang, Dan and McKenna, Ryan and Kotsogiannis, Ios and Hay, Michael and Machanavajjhala, Ashwin and Miklau, Gerome},
  year = {2018},
  pages = {115-130},
  file = {/Users/mfine/Zotero/storage/H29XFGHK/Zhang et al. - 2018 - EKTELO A Framework for Defining Differentially-Pr.pdf}
}

@article{ZK17a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.08228},
  title = {{{LightDP}}: {{Towards Automating Differential Privacy Proofs}}},
  shorttitle = {{{LightDP}}},
  abstract = {The growing popularity and adoption of differential privacy in academic and industrial settings has resulted in the development of increasingly sophisticated algorithms for releasing information while preserving privacy. Accompanying this phenomenon is the natural rise in the development and publication of incorrect algorithms, thus demonstrating the necessity of formal verification tools. However, existing formal methods for differential privacy face a dilemma: methods based on customized logics can verify sophisticated algorithms but come with a steep learning curve and significant annotation burden on the programmers, while existing programming platforms lack expressive power for some sophisticated algorithms.},
  language = {en},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages - POPL 2017},
  doi = {10.1145/3009837.3009884},
  author = {Zhang, Danfeng and Kifer, Daniel},
  year = {2017},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages,D.2.4,D.3.1,F.3.1},
  pages = {888-901},
  file = {/Users/mfine/Zotero/storage/FB69JI74/Zhang and Kifer - 2017 - LightDP Towards Automating Differential Privacy P.pdf}
}

@article{McS10,
  title = {Privacy Integrated Queries: An Extensible Platform for Privacy-Preserving Data Analysis},
  volume = {53},
  issn = {00010782},
  shorttitle = {Privacy Integrated Queries},
  abstract = {Privacy Integrated Queries (PINQ) is an extensible data analysis platform designed to provide unconditional privacy guarantees for the records of the underlying data sets. PINQ provides analysts with access to records through an SQLlike declarative language (LINQ) amidst otherwise arbitrary C\# code. At the same time, the design of PINQ's analysis language and its careful implementation provide formal guarantees of differential privacy for any and all uses of the platform. PINQ's guarantees require no trust placed in the expertise or diligence of the analysts, broadening the scope for design and deployment of privacy-preserving data analyses, especially by privacy nonexperts.},
  language = {en},
  number = {9},
  journal = {Communications of the ACM},
  doi = {10.1145/1810891.1810916},
  author = {McSherry, Frank},
  month = sep,
  year = {2010},
  pages = {89},
  file = {/Users/mfine/Zotero/storage/IISGKJ8Y/McSherry - 2010 - Privacy integrated queries an extensible platform.pdf}
}

@article{BGHP16,
  title = {Programming {{Language Techniques}} for {{Differential Privacy}}},
  volume = {3},
  language = {en},
  number = {1},
  author = {Barthe, Gilles and Gaboardi, Marco and Hsu, Justin and Pierce, Benjamin},
  year = {2016},
  pages = {20},
  file = {/Users/mfine/Zotero/storage/ALGRV337/Barthe et al. - 2016 - Programming Language Techniques for Differential P.pdf}
}

@article{Kin76,
  title = {Symbolic {{Execution}} and {{Program Testing}}},
  volume = {19},
  issn = {0001-0782},
  abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
  number = {7},
  journal = {Commun. ACM},
  doi = {10.1145/360248.360252},
  author = {King, James C.},
  month = jul,
  year = {1976},
  keywords = {program debugging,program proving,program testing,program verification,symbolic execution,symbolic interpretation},
  pages = {385--394},
  file = {/Users/mfine/Zotero/storage/FHZMGYBJ/King - 1976 - Symbolic Execution and Program Testing.pdf}
}

@inproceedings{SPM+14,
  address = {{New York, NY, USA}},
  series = {{{PLDI}} '14},
  title = {Expressing and {{Verifying Probabilistic Assertions}}},
  isbn = {978-1-4503-2784-8},
  abstract = {Traditional assertions express correctness properties that must hold on every program execution. However, many applications have probabilistic outcomes and consequently their correctness properties are also probabilistic (e.g., they identify faces in images, consume sensor data, or run on unreliable hardware). Traditional assertions do not capture these correctness properties. This paper proposes that programmers express probabilistic correctness properties with probabilistic assertions and describes a new probabilistic evaluation approach to efficiently verify these assertions. Probabilistic assertions are Boolean expressions that express the probability that a property will be true in a given execution rather than asserting that the property must always be true. Given either specific inputs or distributions on the input space, probabilistic evaluation verifies probabilistic assertions by first performing distribution extraction to represent the program as a Bayesian network. Probabilistic evaluation then uses statistical properties to simplify this representation to efficiently compute assertion probabilities directly or with sampling. Our approach is a mix of both static and dynamic analysis: distribution extraction statically builds and optimizes the Bayesian network representation and sampling dynamically interprets this representation. We implement our approach in a tool called Mayhap for C and C++ programs. We evaluate expressiveness, correctness, and performance of Mayhap on programs that use sensors, perform approximate computation, and obfuscate data for privacy. Our case studies demonstrate that probabilistic assertions describe useful correctness properties and that Mayhap efficiently verifies them.},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  publisher = {{ACM}},
  doi = {10.1145/2594291.2594294},
  author = {Sampson, Adrian and Panchekha, Pavel and Mytkowicz, Todd and McKinley, Kathryn S. and Grossman, Dan and Ceze, Luis},
  year = {2014},
  keywords = {symbolic execution,approximate computing,data obfuscation,differential privacy,probabilistic programming,sensors},
  pages = {112--122},
  file = {/Users/mfine/Zotero/storage/HQIXGAYD/Sampson et al. - 2014 - Expressing and Verifying Probabilistic Assertions.pdf}
}

@article{AH17,
  title = {Synthesizing Coupling Proofs of Differential Privacy},
  volume = {2},
  issn = {24751421},
  language = {en},
  number = {POPL},
  journal = {Proceedings of the ACM on Programming Languages},
  doi = {10.1145/3158146},
  author = {Albarghouthi, Aws and Hsu, Justin},
  month = dec,
  year = {2017},
  pages = {1-30},
  file = {/Users/mfine/Zotero/storage/3VKUSST9/Albarghouthi and Hsu - 2017 - Synthesizing coupling proofs of differential priva.pdf}
}

@article{ZK17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.08228},
  title = {{{LightDP}}: {{Towards Automating Differential Privacy Proofs}}},
  shorttitle = {{{LightDP}}},
  abstract = {The growing popularity and adoption of differential privacy in academic and industrial settings has resulted in the development of increasingly sophisticated algorithms for releasing information while preserving privacy. Accompanying this phenomenon is the natural rise in the development and publication of incorrect algorithms, thus demonstrating the necessity of formal verification tools. However, existing formal methods for differential privacy face a dilemma: methods based on customized logics can verify sophisticated algorithms but come with a steep learning curve and significant annotation burden on the programmers, while existing programming platforms lack expressive power for some sophisticated algorithms. In this paper, we present LightDP, a simple imperative language that strikes a better balance between expressive power and usability. The core of LightDP is a novel relational type system that separates relational reasoning from privacy budget calculations. With dependent types, the type system is powerful enough to verify sophisticated algorithms where the composition theorem falls short. In addition, the inference engine of LightDP infers most of the proof details, and even searches for the proof with minimal privacy cost bound when multiple proofs exist. We show that LightDP verifies sophisticated algorithms with little manual effort.},
  journal = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages - POPL 2017},
  doi = {10.1145/3009837.3009884},
  author = {Zhang, Danfeng and Kifer, Daniel},
  year = {2017},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages,D.2.4,D.3.1,F.3.1},
  pages = {888-901},
  file = {/Users/mfine/Zotero/storage/JUZRP7C7/Zhang and Kifer - 2017 - LightDP Towards Automating Differential Privacy P.pdf;/Users/mfine/Zotero/storage/B57Q2WAX/1607.html}
}

@article{GHK+16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04340},
  primaryClass = {cs, stat},
  title = {{{PSI}} (\{\textbackslash\vphantom\}{{Psi}}\vphantom\{\}): A {{Private}} Data {{Sharing Interface}}},
  shorttitle = {{{PSI}} (\{\textbackslash\vphantom\}{{Psi}}\vphantom\{\})},
  abstract = {We provide an overview of PSI ("a Private data Sharing Interface"), a system we are developing to enable researchers in the social sciences and other fields to share and explore privacy-sensitive datasets with the strong privacy protections of differential privacy.},
  journal = {arXiv:1609.04340 [cs, stat]},
  author = {Gaboardi, Marco and Honaker, James and King, Gary and Murtagh, Jack and Nissim, Kobbi and Ullman, Jonathan and Vadhan, Salil},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Computers and Society,Statistics - Methodology},
  file = {/Users/mfine/Zotero/storage/AUYSRNER/Gaboardi et al. - 2016 - PSI ( Psi ) a Private data Sharing Interface.pdf;/Users/mfine/Zotero/storage/V5AGKXIS/1609.html}
}

@inproceedings{ZCX+14,
  title = {Towards {{Accurate Histogram Publication}} under {{Differential Privacy}}},
  abstract = {Histograms are the workhorse of data mining and analysis. This paper considers the problem of publishing histograms under differential privacy, one of the strongest privacy models. Existing differentially private histogram publication schemes have shown that clustering (or grouping) is a promising idea to improve the accuracy of sanitized histograms. However, none of them fully exploits the benefit of clustering. In this paper, we introduce a new clustering framework. It features a sophisticated evaluation of the trade-off between the approximation error due to clustering and the Laplace error due to Laplace noise injected, which is normally overlooked in prior work. In particular, we propose three clustering strategies with different orders of run-time complexities. We prove the superiority of our approach by theoretical utility comparisons with the competitors. Our extensive experiments over various standard real-life and synthetic datasets confirm that our technique consistently outperforms existing competitors.},
  booktitle = {{{SDM}}},
  doi = {10.1137/1.9781611973440.68},
  author = {Zhang, Xiaoming and Chen, Rui and Xu, Jianliang and Meng, Xiaofeng and Xie, Yingtao},
  year = {2014},
  keywords = {Approximation error,Cluster analysis,Data mining,Differential privacy,Experiment,Real life,Synthetic intelligence},
  file = {/Users/mfine/Zotero/storage/956D5T85/Zhang et al. - 2014 - Towards Accurate Histogram Publication under Diffe.pdf}
}

@article{HKC+17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.09549},
  title = {Context-{{Aware Generative Adversarial Privacy}}},
  volume = {19},
  issn = {1099-4300},
  abstract = {Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge. On the one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility. On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics. We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial networks (GANs) to allow the data holder to learn privatization schemes from the dataset itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals' private variables, and an adversary that tries to infer the private variables from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical) statistical dataset models: (a) the binary data model, and (b) the binary Gaussian mixture model. For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that the privacy mechanisms learned from data (in a generative adversarial fashion) match the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice, even in the absence of dataset statistics.},
  number = {12},
  journal = {Entropy},
  doi = {10.3390/e19120656},
  author = {Huang, Chong and Kairouz, Peter and Chen, Xiao and Sankar, Lalitha and Rajagopal, Ram},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Information Theory,Computer Science - Machine Learning},
  pages = {656},
  file = {/Users/mfine/Zotero/storage/B95FQBAD/Huang et al. - 2017 - Context-Aware Generative Adversarial Privacy.pdf;/Users/mfine/Zotero/storage/SNQSAMUR/1710.html}
}

@article{AAL15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.02318},
  primaryClass = {cs, math},
  title = {Notes on {{Information}}-{{Theoretic Privacy}}},
  abstract = {We investigate the tradeoff between privacy and utility in a situation where both privacy and utility are measured in terms of mutual information. For the binary case, we fully characterize this tradeoff in case of perfect privacy and also give an upper-bound for the case where some privacy leakage is allowed. We then introduce a new quantity which quantifies the amount of private information contained in the observable data and then connect it to the optimal tradeoff between privacy and utility.},
  journal = {arXiv:1510.02318 [cs, math]},
  author = {Asoodeh, Shahab and Alajaji, Fady and Linder, Tam{\'a}s},
  month = oct,
  year = {2015},
  keywords = {Computer Science - Information Theory},
  file = {/Users/mfine/Zotero/storage/87TUGZUL/Asoodeh et al. - 2015 - Notes on Information-Theoretic Privacy.pdf;/Users/mfine/Zotero/storage/QJ3PFFA5/1510.html}
}

@inproceedings{BK11,
  address = {{Cernay-la-Ville, France}},
  title = {Information-{{Theoretic Bounds}} for {{Differentially Private Mechanisms}}},
  isbn = {978-1-61284-644-6},
  abstract = {There are two active and independent lines of research that aim at quantifying the amount of information that is disclosed by computing on confidential data. Each line of research has developed its own notion of confidentiality: on the one hand, differential privacy is the emerging consensus guarantee used for privacy-preserving data analysis. On the other hand, information-theoretic notions of leakage are used for characterizing the confidentiality properties of programs in language-based settings.},
  language = {en},
  booktitle = {2011 {{IEEE}} 24th {{Computer Security Foundations Symposium}}},
  publisher = {{IEEE}},
  doi = {10.1109/CSF.2011.20},
  author = {Barthe, Gilles and Kopf, Boris},
  month = jun,
  year = {2011},
  pages = {191-204},
  file = {/Users/mfine/Zotero/storage/N8NHMN5K/Barthe and Kopf - 2011 - Information-Theoretic Bounds for Differentially Pr.pdf}
}

@article{MHH,
  title = {Differential {{Privacy}} in the {{Wild}}: {{A}} Tutorial on Current Practices \& Open Challenges},
  abstract = {Differential privacy has emerged as an important standard for privacy preserving computation over databases containing sensitive information about individuals. Research on differential privacy spanning a number of research areas, including theory, security, database, networks, machine learning, and statistics, over the last decade has resulted in a variety of privacy preserving algorithms for a number of analysis tasks. Despite maturing research efforts, the adoption of differential privacy by practitioners in industry, academia, or government agencies has so far been rare. Hence, in this tutorial, we will first describe the foundations of differentially private algorithm design that cover the state of the art in private computation on tabular data. In the second half of the tutorial we will highlight real world applications on complex data types, and identify research challenges in applying differential privacy to real world applications.},
  language = {en},
  author = {Machanavajjhala, Ashwin and He, Xi and Hay, Michael},
  pages = {4},
  file = {/Users/mfine/Zotero/storage/42A44HZ4/Machanavajjhala et al. - Differential Privacy in the Wild A tutorial on cu.pdf}
}

@article{JY19,
  title = {{{PATE}}-{{GAN}}: {{GENERATING SYNTHETIC DATA WITH DIFFERENTIAL PRIVACY GUARANTEES}}},
  abstract = {Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the stateof-the-art method with respect to this and other notions of synthetic data quality.},
  language = {en},
  author = {Jordon, James and Yoon, Jinsung},
  year = {2019},
  pages = {21},
  file = {/Users/mfine/Zotero/storage/IKX4XQ8J/Jordon and Yoon - 2019 - PATE-GAN GENERATING SYNTHETIC DATA WITH DIFFERENT.pdf}
}

@article{NBW+,
  title = {Bridging the {{Gap Between Computer Science}} and {{Legal Approaches}} to {{Privacy}}},
  volume = {31},
  language = {en},
  number = {2},
  author = {Nissim, Kobbi and Bembenek, Aaron and Wood, Alexandra and Bun, Mark and Gasser, Urs},
  pages = {94},
  file = {/Users/mfine/Zotero/storage/ZLXGH549/02.-article-wood-7.21.pdf}
}

@incollection{HLM12,
  title = {A {{Simple}} and {{Practical Algorithm}} for {{Differentially Private Data Release}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  author = {Hardt, Moritz and Ligett, Katrina and Mcsherry, Frank},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {2339--2347},
  file = {/Users/mfine/Zotero/storage/RM93NNDE/Hardt et al. - 2012 - A Simple and Practical Algorithm for Differentiall.pdf;/Users/mfine/Zotero/storage/GIALTC7T/4548-a-simple-and-practical-algorithm-for-differentially-private-data-release.html}
}

@article{KP,
  title = {E Cient {{Noise}}-{{Tolerant Learning From Statistical Queries}}},
  language = {en},
  author = {Kearns, Michael and Park, Florham},
  pages = {27},
  file = {/Users/mfine/Zotero/storage/GNXG6AHM/Kearns and Park - E cient Noise-Tolerant Learning From Statistical Q.pdf}
}

@article{GAH+14,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1402.1526},
  primaryClass = {cs},
  title = {Dual {{Query}}: {{Practical Private Query Release}} for {{High Dimensional Data}}},
  shorttitle = {Dual {{Query}}},
  abstract = {We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.},
  journal = {arXiv:1402.1526 [cs]},
  author = {Gaboardi, Marco and Arias, Emilio Jes{\'u}s Gallego and Hsu, Justin and Roth, Aaron and Wu, Zhiwei Steven},
  month = feb,
  year = {2014},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms,Computer Science - Databases},
  file = {/Users/mfine/Zotero/storage/362U57ZM/Gaboardi et al. - 2014 - Dual Query Practical Private Query Release for Hi.pdf;/Users/mfine/Zotero/storage/YYNXAGLV/1402.html}
}

@inproceedings{FS10,
  address = {{Washington, DC, USA}},
  title = {Data Mining with Differential Privacy},
  isbn = {978-1-4503-0055-1},
  abstract = {We consider the problem of data mining with formal privacy guarantees, given a data access interface based on the differential privacy framework. Differential privacy requires that computations be insensitive to changes in any particular individual's record, thereby restricting data leaks through the results. The privacy preserving interface ensures unconditionally safe access to the data and does not require from the data miner any expertise in privacy. However, as we show in the paper, a naive utilization of the interface to construct privacy preserving data mining algorithms could lead to inferior data mining results. We address this problem by considering the privacy and the algorithmic requirements simultaneously, focusing on decision tree induction as a sample application. The privacy mechanism has a profound effect on the performance of the methods chosen by the data miner. We demonstrate that this choice could make the difference between an accurate classifier and a completely useless one. Moreover, an improved algorithm can achieve the same level of accuracy and privacy as the naive implementation but with an order of magnitude fewer learning samples.},
  language = {en},
  booktitle = {Proceedings of the 16th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '10},
  publisher = {{ACM Press}},
  doi = {10.1145/1835804.1835868},
  author = {Friedman, Arik and Schuster, Assaf},
  year = {2010},
  pages = {493},
  file = {/Users/mfine/Zotero/storage/DHRA2L79/Friedman and Schuster - 2010 - Data mining with differential privacy.pdf}
}

@article{DFH+15,
  title = {The Reusable Holdout: {{Preserving}} Validity in Adaptive Data Analysis},
  volume = {349},
  issn = {0036-8075, 1095-9203},
  shorttitle = {The Reusable Holdout},
  language = {en},
  number = {6248},
  journal = {Science},
  doi = {10.1126/science.aaa9375},
  author = {Dwork, C. and Feldman, V. and Hardt, M. and Pitassi, T. and Reingold, O. and Roth, A.},
  month = aug,
  year = {2015},
  pages = {636-638},
  file = {/Users/mfine/Zotero/storage/I7Q7RRDN/Dwork et al. - 2015 - The reusable holdout Preserving validity in adapt.pdf}
}

@techreport{BMS13,
  address = {{Rochester, NY}},
  type = {{{SSRN Scholarly Paper}}},
  title = {Fool's {{Gold}}: An {{Illustrated Critique}} of {{Differential Privacy}}},
  shorttitle = {Fool's {{Gold}}},
  abstract = {Differential privacy has taken the privacy community by storm. Computer scientists developed this technique to allow researchers to submit queries to databases without being able to glean sensitive information about the individuals described in the data. Legal scholars champion differential privacy as a practical solution to the competing interests in research and confidentiality, and policymakers are poised to adopt it as the gold standard for data privacy. It would be a disastrous mistake.},
  language = {en},
  number = {ID 2326746},
  institution = {{Social Science Research Network}},
  author = {Bambauer, Jane R. and Muralidhar, Krish and Sarathy, Rathindra},
  month = sep,
  year = {2013},
  keywords = {differential privacy,confidentiality,data privacy,data research,disclosure,privacy},
  file = {/Users/mfine/Zotero/storage/6RKE6WZ3/papers.html}
}

@article{SK18,
  title = {Integrating {{Technical}} and {{Legal Concepts}} of {{Privacy}}},
  volume = {6},
  issn = {2169-3536},
  abstract = {We provide an overview of legal and technical concepts of privacy protection. Data protection guarantees exist in European Union at a primary law level since 2009, when the Lisbon Treaty has become effective. The new regulation called general data protection regulation adopted on April 27, 2016 will enter into effect on May 25, 2018. Regarding data protection, the US follows a very different approach. The cornerstone of the technical framework of data protection is the concept of differential privacy for which the privacy's provable guarantees hold even against worst-case scenarios when the access to arbitrary auxiliary information is provided. Research directions for integrating legal and technical concepts of privacy are discussed. Although both fields, legal and technical, approach privacy from different perspectives, we expect that these approaches will converge to a common framework. The privacy-preserving framework, therefore, should be a balance between: 1) ambiguous legal concepts and precise mathematical notions of privacy; 2) the desired privacy level; 3) the information loss, which is measured by data utility metrics; and 4) the complexity and the practical feasibility of the proposed technique. Key notions of fairness are incompatible with each other, and hence any privacy-preserving framework should also include the trade-offs between conditions imposed in the definitions of fairness. The complex assembly of legal and technical components implementing various privacy requirements in the privacy-preserving framework, should be realized within the privacy-by-design concept.},
  journal = {IEEE Access},
  doi = {10.1109/ACCESS.2018.2836184},
  author = {Sokolovska, A. and Kocarev, L.},
  year = {2018},
  keywords = {differential privacy,privacy,Data privacy,data protection,data protection guarantees,data protection regulation,data utility metrics,general data protection regulation,law,Law,legal concepts,Loss measurement,primary law level,Privacy,privacy level,privacy protection,privacy requirements,privacy-by-design concept,privacy-preserving data mining,privacy-preserving framework,security of data,social implications of technology,technical concepts,technical framework},
  pages = {26543-26557},
  file = {/Users/mfine/Zotero/storage/2BAX66MV/Sokolovska and Kocarev - 2018 - Integrating Technical and Legal Concepts of Privac.pdf;/Users/mfine/Zotero/storage/FGFVL263/8358762.html}
}

@article{RYY+18,
  title = {\$\textbackslash{{textsfLoPub}}\$: {{High}}-{{Dimensional Crowdsourced Data Publication With Local Differential Privacy}}},
  volume = {13},
  issn = {1556-6013},
  shorttitle = {\$\textbackslash{{textsfLoPub}}\$},
  abstract = {High-dimensional crowdsourced data collected from numerous users produces rich knowledge about our society; however, it also brings unprecedented privacy threats to the participants. Local differential privacy (LDP), a variant of differential privacy, is recently proposed as a state-of-the-art privacy notion. Unfortunately, achieving LDP on high-dimensional crowdsourced data publication raises great challenges in terms of both computational efficiency and data utility. To this end, based on the expectation maximization (EM) algorithm and Lasso regression, we first propose efficient multi-dimensional joint distribution estimation algorithms with LDP. Then, we develop a local differentially private high-dimensional data publication algorithm (LoPub) by taking advantage of our distribution estimation techniques. In particular, correlations among multiple attributes are identified to reduce the dimensionality of crowdsourced data, thus speeding up the distribution learning process and achieving high data utility. Extensive experiments on real-world datasets demonstrate that our multivariate distribution estimation scheme significantly outperforms existing estimation schemes in terms of both communication overhead and estimation speed. Moreover, LoPub can keep, on average, 80\% and 60\% accuracy over the released datasets in terms of support vector machine and random forest classification, respectively.},
  number = {9},
  journal = {IEEE Transactions on Information Forensics and Security},
  doi = {10.1109/TIFS.2018.2812146},
  author = {Ren, X. and Yu, C. and Yu, W. and Yang, S. and Yang, X. and McCann, J. A. and Yu, P. S.},
  month = sep,
  year = {2018},
  keywords = {data privacy,Data privacy,Privacy,Correlation,crowdsourced data,crowdsourcing,data publication,distribution learning process,Estimation,expectation maximization algorithm,expectation-maximisation algorithm,Frequency estimation,high-dimensional crowdsourced data publication,high-dimensional data,high-dimensional data publication algorithm,Lasso regression,LDP,learning (artificial intelligence),local differential privacy,Local differential privacy,LoPub,multidimensional joint distribution estimation algorithms,multivariate distribution estimation scheme,private data release,regression analysis,Sensors,Servers,state-of-the-art privacy notion,unprecedented privacy threats,synth_ldp},
  pages = {2151-2166},
  file = {/Users/mfine/Zotero/storage/P5AAAAJM/Ren et al. - 2018 - $textsfLoPub$ High-Dimensional Crowdsourced Data.pdf;/Users/mfine/Zotero/storage/UGSJBEGM/8306916.html}
}

@book{Zhu17,
  address = {{New York, NY}},
  title = {Differential Privacy and Applications},
  isbn = {978-3-319-62002-2},
  language = {en},
  publisher = {{Springer Science+Business Media}},
  author = {Zhu, Tianqing},
  year = {2017},
  file = {/Users/mfine/Zotero/storage/RNP29R7Z/Zhu - 2017 - Differential privacy and applications.pdf}
}

@article{JNS,
  title = {Towards {{Practical Differential Privacy}} for {{SQL Queries}}},
  abstract = {Differential privacy promises to enable general data analytics while protecting individual privacy, but existing differential privacy mechanisms do not support the wide variety of features and databases used in real-world SQL-based analytics systems. This paper presents the first practical approach for differential privacy of SQL queries. Using 8.1 million real-world queries, we conduct an empirical study to determine the requirements for practical differential privacy, and discuss limitations of previous approaches in light of these requirements. To meet these requirements we propose elastic sensitivity, a novel method for approximating the local sensitivity of queries with general equijoins. We prove that elastic sensitivity is an upper bound on local sensitivity and can therefore be used to enforce differential privacy using any local sensitivity-based mechanism.},
  language = {en},
  author = {Johnson, Noah and Near, Joseph P and Song, Dawn},
  pages = {14},
  file = {/Users/mfine/Zotero/storage/37J3QCLR/Johnson et al. - Towards Practical Differential Privacy for SQL Que.pdf}
}

@article{zotero-143,
  type = {Article}
}

@article{LHMW14,
  title = {A Data- and Workload-Aware Algorithm for Range Queries under Differential Privacy},
  volume = {7},
  issn = {21508097},
  abstract = {We describe a new algorithm for answering a given set of range queries under -differential privacy which often achieves substantially lower error than competing methods. Our algorithm satisfies differential privacy by adding noise that is adapted to the input data and to the given query set. We first privately learn a partitioning of the domain into buckets that suit the input data well. Then we privately estimate counts for each bucket, doing so in a manner well-suited for the given query set. Since the performance of the algorithm depends on the input database, we evaluate it on a wide range of real datasets, showing that we can achieve the benefits of data-dependence on both ``easy'' and ``hard'' databases.},
  language = {en},
  number = {5},
  journal = {Proceedings of the VLDB Endowment},
  doi = {10.14778/2732269.2732271},
  author = {Li, Chao and Hay, Michael and Miklau, Gerome and Wang, Yue},
  month = jan,
  year = {2014},
  pages = {341-352},
  file = {/Users/mfine/Zotero/storage/UFMGFHHT/Li et al. - 2014 - A data- and workload-aware algorithm for range que.pdf}
}

@article{TZSD18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.02292},
  primaryClass = {cs},
  title = {When {{Homomorphic Cryptosystem Meets Differential Privacy}}: {{Training Machine Learning Classifier}} with {{Privacy Protection}}},
  shorttitle = {When {{Homomorphic Cryptosystem Meets Differential Privacy}}},
  abstract = {Machine learning (ML) classifiers are invaluable building blocks that have been used in many fields. High quality training dataset collected from multiple data providers is essential to train accurate classifiers. However, it raises concern about data privacy due to potential leakage of sensitive information in training dataset. Existing studies have proposed many solutions to privacy-preserving training of ML classifiers, but it remains a challenging task to strike a balance among accuracy, computational efficiency, and security. In this paper, we propose Heda, an efficient privacypreserving scheme for training ML classifiers. By combining homomorphic cryptosystem (HC) with differential privacy (DP), Heda obtains the tradeoffs between efficiency and accuracy, and enables flexible switch among different tradeoffs by parameter tuning. In order to make such combination efficient and feasible, we present novel designs based on both HC and DP: A library of building blocks based on partially HC are proposed to construct complex training algorithms without introducing a trusted thirdparty or computational relaxation; A set of theoretical methods are proposed to determine appropriate privacy budget and to reduce sensitivity. Security analysis demonstrates that our solution can construct complex ML training algorithm securely. Extensive experimental results show the effectiveness and efficiency of the proposed scheme.},
  journal = {arXiv:1812.02292 [cs]},
  author = {Tang, Xiangyun and Zhu, Liehuang and Shen, Meng and Du, Xiaojiang},
  month = dec,
  year = {2018},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/mfine/Zotero/storage/Z7JYW5T9/Tang et al. - 2018 - When Homomorphic Cryptosystem Meets Differential P.pdf;/Users/mfine/Zotero/storage/8ZD6ZU83/1812.html}
}

@article{AWO+15,
  title = {Towards a {{Modern Approach}} to {{Privacy}}-{{Aware Government Data Releases}}},
  volume = {30},
  issn = {1086-3818},
  abstract = {[Governments are under increasing pressure to publicly release collected data in order to promote transparency, accountability, and innovation. Because much of the data they release pertains to individuals, agencies rely on various standards and interventions to protect privacy interests while supporting a range of beneficial uses of the data. However, there are growing concerns among privacy scholars, policymakers, and the public that these approaches are incomplete, inconsistent, and difficult to navigate. To identify gaps in current practice, this Article reviews data released in response to freedom of information and Privacy Act requests, traditional public and vital records, official statistics, and e-government and open government initiatives. It finds that agencies lack formal guidance for implementing privacy interventions in specific cases. Most agencies address privacy by withholding or redacting records that contain directly or indirectly identifying information based on an ad hoc balancing of interests, and different government actors sometimes treat similar privacy risks vastly differently. These observations demonstrate the need for a more systematic approach to privacy analysis and also suggest a new way forward. In response to these concerns, this Article proposes a framework for a modern privacy analysis informed by recent advances in data privacy from disciplines such as computer science, statistics, and law. Modeled on an information security approach, this framework characterizes and distinguishes between privacy controls, threats, vulnerabilities, and utility. When developing a data release mechanism, policymakers should specify the desired data uses and expected benefits, examine each stage of the data lifecycle to identify privacy threats and vulnerabilities, and select controls for each lifecycle stage that are consistent with the uses, threats, and vulnerabilities at that stage. This Article sketches the contours of this analytical framework, populates selected portions of its contents, and illustrates how it can inform the selection of privacy controls by discussing its application to two real-world examples of government data releases.]},
  number = {3},
  journal = {Berkeley Technology Law Journal},
  author = {Altman, Micah and Wood, Alexandra and O'Brien, David R. and Vadhan, Salil and Gasser, Urs},
  year = {2015},
  pages = {1967-2072},
  file = {/Users/mfine/Zotero/storage/UXDNEFVV/Altman et al. - 2015 - Towards a Modern Approach to Privacy-Aware Governm.pdf}
}

@incollection{GKY11,
  address = {{Berlin, Heidelberg}},
  title = {Limits of {{Computational Differential Privacy}} in the {{Client}}/{{Server Setting}}},
  volume = {6597},
  isbn = {978-3-642-19570-9 978-3-642-19571-6},
  abstract = {Differential privacy is a well established definition guaranteeing that queries to a database do not reveal ``too much'' information about specific individuals who have contributed to the database. The standard definition of differential privacy is information theoretic in nature, but it is natural to consider computational relaxations and to explore what can be achieved with respect to such notions. Mironov et al. (Crypto 2009) and McGregor et al. (FOCS 2010) recently introduced and studied several variants of computational differential privacy, and show that in the two-party setting (where data is split between two parties) these relaxations can offer significant advantages.},
  language = {en},
  booktitle = {Theory of {{Cryptography}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Groce, Adam and Katz, Jonathan and Yerukhimovich, Arkady},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ishai, Yuval},
  year = {2011},
  pages = {417-431},
  file = {/Users/mfine/Zotero/storage/RS989U7E/Groce et al. - 2011 - Limits of Computational Differential Privacy in th.pdf},
  doi = {10.1007/978-3-642-19571-6_25}
}

@article{BLR11b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1109.2229},
  primaryClass = {cs},
  title = {A {{Learning Theory Approach}} to {{Non}}-{{Interactive Database Privacy}}},
  abstract = {In this paper we demonstrate that, ignoring computational constraints, it is possible to privately release synthetic databases that are useful for large classes of queries -- much larger in size than the database itself. Specifically, we give a mechanism that privately releases synthetic data for a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries. We show that this in particular implies a mechanism for counting queries that gives error guarantees that grow only with the VC-dimension of the class of queries, which itself grows only logarithmically with the size of the query class. We also show that it is not possible to privately release even simple classes of queries (such as intervals and their generalizations) over continuous domains. Despite this, we give a privacy-preserving polynomial time algorithm that releases information useful for all halfspace queries, given a slight relaxation of the utility guarantee. This algorithm does not release synthetic data, but instead another data structure capable of representing an answer for each query. We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis-aligned rectangles of constant dimension. Finally, inspired by learning theory, we introduce a new notion of data privacy, which we call distributional privacy, and show that it is strictly stronger than the prevailing privacy notion, differential privacy.},
  journal = {arXiv:1109.2229 [cs]},
  author = {Blum, Avrim and Ligett, Katrina and Roth, Aaron},
  month = sep,
  year = {2011},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/XUDXVTBL/Blum et al. - 2011 - A Learning Theory Approach to Non-Interactive Data.pdf;/Users/mfine/Zotero/storage/FWKY7TL9/1109.html}
}

@inproceedings{KM12,
  address = {{Scottsdale, Arizona, USA}},
  title = {A Rigorous and Customizable Framework for Privacy},
  isbn = {978-1-4503-1248-6},
  abstract = {In this paper we introduce a new and general privacy framework called Pufferfish. The Pufferfish framework can be used to create new privacy definitions that are customized to the needs of a given application. The goal of Pufferfish is to allow experts in an application domain, who frequently do not have expertise in privacy, to develop rigorous privacy definitions for their data sharing needs. In addition to this, the Pufferfish framework can also be used to study existing privacy definitions.},
  language = {en},
  booktitle = {Proceedings of the 31st Symposium on {{Principles}} of {{Database Systems}} - {{PODS}} '12},
  publisher = {{ACM Press}},
  doi = {10.1145/2213556.2213571},
  author = {Kifer, Daniel and Machanavajjhala, Ashwin},
  year = {2012},
  pages = {77},
  file = {/Users/mfine/Zotero/storage/XAVMVT99/Kifer and Machanavajjhala - 2012 - A rigorous and customizable framework for privacy.pdf}
}

@inproceedings{DS09,
  title = {Differential {{Privacy}} for {{Statistics}} : {{What}} We {{Know}} and {{What}} We {{Want}} to {{Learn}}},
  shorttitle = {Differential {{Privacy}} for {{Statistics}}},
  abstract = {We motivate and review the definition of differential privacy, survey some results on differentially private statistical estimators, and outline a research agenda. This survey is based on two presentations given by the authors at an NCHS/CDC sponsored workshop on data privacy in May 2008.},
  author = {Dwork, Cynthia and Smith, Adam},
  year = {2009},
  keywords = {Differential privacy,Information privacy},
  file = {/Users/mfine/Zotero/storage/S2W45CW8/Dwork and Smith - 2009 - Differential Privacy for Statistics  What we Know.pdf}
}

@incollection{AZK+19,
  address = {{Singapore}},
  title = {Privacy {{Preserving Synthetic Data Release Using Deep Learning}}},
  isbn = {9789811360480 9789811360497},
  abstract = {For many critical applications ranging from health care to social sciences, releasing personal data while protecting individual privacy is paramount. Over the years, data anonymization and synthetic data generation techniques have been proposed to address this challenge. Unfortunately, data anonymization approaches do not provide rigorous privacy guarantees. Although, there are existing synthetic data generation techniques that use rigorous definitions of differential privacy, to our knowledge, these techniques have not been compared extensively using different utility metrics.},
  language = {en},
  booktitle = {Energy {{Transfer Processes}} in {{Polynuclear Lanthanide Complexes}}},
  publisher = {{Springer Singapore}},
  author = {Abay, Nazmiye Ceren and Zhou, Yan and Kantarcioglu, Murat and Thuraisingham, Bhavani and Sweeney, Latanya},
  collaborator = {Omagari, Shun},
  year = {2019},
  pages = {510-526},
  file = {/Users/mfine/Zotero/storage/3EIQ6GTB/Abay et al. - 2019 - Privacy Preserving Synthetic Data Release Using De.pdf},
  doi = {10.1007/978-3-030-10925-7_31}
}

@article{SRN+16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.06651},
  primaryClass = {stat},
  title = {General and Specific Utility Measures for Synthetic Data},
  abstract = {Data holders can produce synthetic versions of datasets when concerns about potential disclosure restrict the availability of the original records. This paper is concerned with methods to judge whether such synthetic data have a distribution that is comparable to that of the original data, what we will term general utility. We consider how general utility compares with specific utility, the similarity of results of analyses from the synthetic data and the original data. We adapt a previous general measure of data utility, the propensity score mean-squared-error (pMSE), to the specific case of synthetic data and derive its distribution for the case when the correct synthesis model is used to create the synthetic data. Our asymptotic results are confirmed by a simulation study. We also consider two specific utility measures, confidence interval overlap and standardized difference in summary statistics, which we compare with the general utility results. We present two examples examining this comparison of general and specific utility to real data syntheses and make recommendations for their use for evaluating synthetic data.},
  journal = {arXiv:1604.06651 [stat]},
  author = {Snoke, Joshua and Raab, Gillian and Nowok, Beata and Dibben, Chris and Slavkovic, Aleksandra},
  month = apr,
  year = {2016},
  keywords = {Statistics - Applications},
  file = {/Users/mfine/Zotero/storage/4JV4ZTHI/Snoke et al. - 2016 - General and specific utility measures for syntheti.pdf;/Users/mfine/Zotero/storage/BILQW5ND/1604.html}
}

@article{BDR19,
  title = {Privacy and {{Synthetic Datasets}}},
  volume = {22},
  abstract = {Sharing is a virtue, instilled in us from childhood. Unfortunately, when it comes to big data\textemdash{}i.e., databases possessing the potential to usher in a whole new world of scientific progress\textemdash{}the legal landscape is either too greedy or too Laissez-Faire. Either all identifiers must be stripped from the data, rendering it useless, or one-step removed personally identifiable information may be shared freely, freely sharing secrets. In part, this is a result of the historic solution to database privacy, anonymization, a subtractive technique incurring not only poor privacy results, but also lackluster utility. In anonymization's stead, differential privacy arose; it provides better, nearperfect privacy, but is nonetheless subtractive in terms of utility.},
  language = {en},
  author = {Bellovin, Steven M and Dutta, Preetam K and Reitinger, Nathan},
  year = {2019},
  pages = {52},
  file = {/Users/mfine/Zotero/storage/SCFPRZ5B/Bellovin et al. - 2019 - Privacy and Synthetic Datasets.pdf}
}

@inproceedings{PWV16,
  address = {{Montreal, QC, Canada}},
  title = {The {{Synthetic Data Vault}}},
  isbn = {978-1-5090-5206-6},
  abstract = {The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a stateof-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database.},
  language = {en},
  booktitle = {2016 {{IEEE International Conference}} on {{Data Science}} and {{Advanced Analytics}} ({{DSAA}})},
  publisher = {{IEEE}},
  doi = {10.1109/DSAA.2016.49},
  author = {Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
  month = oct,
  year = {2016},
  keywords = {impt},
  pages = {399-410},
  file = {/Users/mfine/Zotero/storage/KXXFFR3A/Patki et al. - 2016 - The Synthetic Data Vault.pdf}
}

@misc{19a,
  title = {Automated Hierarchical Generative Modeling for a Relational Time Series Databases and Sampling: {{HDI}}-{{Project}}/{{SDV}}},
  copyright = {MIT},
  shorttitle = {Automated Hierarchical Generative Modeling for a Relational Time Series Databases and Sampling},
  howpublished = {MIT - The Human Data Interaction Project},
  month = apr,
  year = {2019}
}

@article{CZK+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.05141},
  primaryClass = {cs},
  title = {Ekiden: {{A Platform}} for {{Confidentiality}}-{{Preserving}}, {{Trustworthy}}, and {{Performant Smart Contract Execution}}},
  shorttitle = {Ekiden},
  abstract = {Smart contracts are applications that execute on blockchains. Today they manage billions of dollars in value and motivate visionary plans for pervasive blockchain deployment. While smart contracts inherit the availability and other security assurances of blockchains, however, they are impeded by blockchains' lack of confidentiality and poor performance. We present Ekiden, a system that addresses these critical gaps by combining blockchains with Trusted Execution Environments (TEEs). Ekiden leverages a novel architecture that separates consensus from execution, enabling efficient TEE-backed confidentiality-preserving smart-contracts and high scalability. Our prototype (with Tendermint as the consensus layer) achieves example performance of 600x more throughput and 400x less latency at 1000x less cost than the Ethereum mainnet. Another contribution of this paper is that we systematically identify and treat the pitfalls arising from harmonizing TEEs and blockchains. Treated separately, both TEEs and blockchains provide powerful guarantees, but hybridized, though, they engender new attacks. For example, in naive designs, privacy in TEE-backed contracts can be jeopardized by forgery of blocks, a seemingly unrelated attack vector. We believe the insights learned from Ekiden will prove to be of broad importance in hybridized TEE-blockchain systems.},
  journal = {arXiv:1804.05141 [cs]},
  author = {Cheng, Raymond and Zhang, Fan and Kos, Jernej and He, Warren and Hynes, Nicholas and Johnson, Noah and Juels, Ari and Miller, Andrew and Song, Dawn},
  month = apr,
  year = {2018},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/mfine/Zotero/storage/HBEP879Y/Cheng et al. - 2018 - Ekiden A Platform for Confidentiality-Preserving,.pdf;/Users/mfine/Zotero/storage/ZBSRX9PN/1804.html}
}

@inproceedings{KWF+16,
  address = {{San Francisco, California, USA}},
  title = {{{PrivateClean}}: {{Data Cleaning}} and {{Differential Privacy}}},
  isbn = {978-1-4503-3531-7},
  shorttitle = {{{PrivateClean}}},
  abstract = {Recent advances in differential privacy make it possible to guarantee user privacy while preserving the main characteristics of the data. However, most differential privacy mechanisms assume that the underlying dataset is clean. This paper explores the link between data cleaning and differential privacy in a framework we call PrivateClean. PrivateClean includes a technique for creating private datasets of numerical and discrete-valued attributes, a formalism for privacy-preserving data cleaning, and techniques for answering sum, count, and avg queries after cleaning. We show: (1) how the degree of privacy affects subsequent aggregate query accuracy, (2) how privacy potentially amplifies certain types of errors in a dataset, and (3) how this analysis can be used to tune the degree of privacy. The key insight is to maintain a bipartite graph relating dirty values to clean values and use this graph to estimate biases due to the interaction between cleaning and privacy. We validate these results on four datasets with a variety of well-studied cleaning techniques including using functional dependencies, outlier filtering, and resolving inconsistent attributes.},
  language = {en},
  booktitle = {Proceedings of the 2016 {{International Conference}} on {{Management}} of {{Data}} - {{SIGMOD}} '16},
  publisher = {{ACM Press}},
  doi = {10.1145/2882903.2915248},
  author = {Krishnan, Sanjay and Wang, Jiannan and Franklin, Michael J. and Goldberg, Ken and Kraska, Tim},
  year = {2016},
  pages = {937-951},
  file = {/Users/mfine/Zotero/storage/24M4AQZY/Krishnan et al. - 2016 - PrivateClean Data Cleaning and Differential Priva.pdf}
}

@article{GIHM17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.10266},
  primaryClass = {cs},
  title = {Private {{Exploration Primitives}} for {{Data Cleaning}}},
  abstract = {Data cleaning, or the process of detecting and repairing inaccurate or corrupt records in the data, is inherently human-driven. State of the art systems assume cleaning experts can access the data (or a sample of it) to tune the cleaning process. However, in many cases, privacy constraints disallow unfettered access to the data. To address this challenge, we observe and provide empirical evidence that data cleaning can be achieved without access to the sensitive data, but with access to a (noisy) query interface that supports a small set of linear counting query primitives. Motivated by this, we present DPClean, a first of a kind system that allows engineers tune data cleaning workflows while ensuring differential privacy. In DPClean, a cleaning engineer can pose sequences of aggregate counting queries with error tolerances. A privacy engine translates each query into a differentially private mechanism that returns an answer with error matching the specified tolerance, and allows the data owner track the overall privacy loss. With extensive experiments using human and simulated cleaning engineers on blocking and matching tasks, we demonstrate that our approach is able to achieve high cleaning quality while ensuring a reasonable privacy loss.},
  journal = {arXiv:1712.10266 [cs]},
  author = {Ge, Chang and Ilyas, Ihab F. and He, Xi and Machanavajjhala, Ashwin},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Databases},
  file = {/Users/mfine/Zotero/storage/QB8L9ASN/Ge et al. - 2017 - Private Exploration Primitives for Data Cleaning.pdf;/Users/mfine/Zotero/storage/93MGQA6Y/1712.html}
}

@incollection{BCV16,
  address = {{Berlin, Heidelberg}},
  title = {Separating {{Computational}} and {{Statistical Differential Privacy}} in the {{Client}}-{{Server Model}}},
  volume = {9985},
  isbn = {978-3-662-53640-7 978-3-662-53641-4},
  abstract = {Differential privacy is a mathematical definition of privacy for statistical data analysis. It guarantees that any (possibly adversarial) data analyst is unable to learn too much information that is specific to an individual. Mironov et al. (CRYPTO 2009) proposed several computational relaxations of differential privacy (CDP), which relax this guarantee to hold only against computationally bounded adversaries. Their work and subsequent work showed that CDP can yield substantial accuracy improvements in various multiparty privacy problems. However, these works left open whether such improvements are possible in the traditional client-server model of data analysis. In fact, Groce, Katz and Yerukhimovich (TCC 2011) showed that, in this setting, it is impossible to take advantage of CDP for many natural statistical tasks.},
  language = {en},
  booktitle = {Theory of {{Cryptography}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Bun, Mark and Chen, Yi-Hsiu and Vadhan, Salil},
  editor = {Hirt, Martin and Smith, Adam},
  year = {2016},
  pages = {607-634},
  file = {/Users/mfine/Zotero/storage/M2HRNPKQ/Bun et al. - 2016 - Separating Computational and Statistical Different.pdf},
  doi = {10.1007/978-3-662-53641-4_23}
}

@incollection{MPRV09,
  address = {{Berlin, Heidelberg}},
  title = {Computational {{Differential Privacy}}},
  volume = {5677},
  isbn = {978-3-642-03355-1 978-3-642-03356-8},
  abstract = {The definition of differential privacy has recently emerged as a leading standard of privacy guarantees for algorithms on statistical databases. We offer several relaxations of the definition which require privacy guarantees to hold only against efficient\textemdash{}i.e., computationallybounded\textemdash{}adversaries. We establish various relationships among these notions, and in doing so, we observe their close connection with the theory of pseudodense sets by Reingold et al. [1]. We extend the dense model theorem of Reingold et al. to demonstrate equivalence between two definitions (indistinguishability- and simulatability-based) of computational differential privacy.},
  language = {en},
  booktitle = {Advances in {{Cryptology}} - {{CRYPTO}} 2009},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Mironov, Ilya and Pandey, Omkant and Reingold, Omer and Vadhan, Salil},
  editor = {Halevi, Shai},
  year = {2009},
  pages = {126-142},
  file = {/Users/mfine/Zotero/storage/7J7TA7ZI/Mironov et al. - 2009 - Computational Differential Privacy.pdf},
  doi = {10.1007/978-3-642-03356-8_8}
}

@article{BLR11a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1109.2229},
  primaryClass = {cs},
  title = {A {{Learning Theory Approach}} to {{Non}}-{{Interactive Database Privacy}}},
  abstract = {In this paper we demonstrate that, ignoring computational constraints, it is possible to privately release synthetic databases that are useful for large classes of queries -- much larger in size than the database itself. Specifically, we give a mechanism that privately releases synthetic data for a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries. We show that this in particular implies a mechanism for counting queries that gives error guarantees that grow only with the VC-dimension of the class of queries, which itself grows only logarithmically with the size of the query class. We also show that it is not possible to privately release even simple classes of queries (such as intervals and their generalizations) over continuous domains. Despite this, we give a privacy-preserving polynomial time algorithm that releases information useful for all halfspace queries, given a slight relaxation of the utility guarantee. This algorithm does not release synthetic data, but instead another data structure capable of representing an answer for each query. We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis-aligned rectangles of constant dimension. Finally, inspired by learning theory, we introduce a new notion of data privacy, which we call distributional privacy, and show that it is strictly stronger than the prevailing privacy notion, differential privacy.},
  journal = {arXiv:1109.2229 [cs]},
  author = {Blum, Avrim and Ligett, Katrina and Roth, Aaron},
  month = sep,
  year = {2011},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/3Y9I3IS8/Blum et al. - 2011 - A Learning Theory Approach to Non-Interactive Data.pdf;/Users/mfine/Zotero/storage/YJLL5UHT/1109.html}
}

@article{ABD+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.02453},
  primaryClass = {cs},
  title = {A {{Reductions Approach}} to {{Fair Classification}}},
  abstract = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.},
  journal = {arXiv:1803.02453 [cs]},
  author = {Agarwal, Alekh and Beygelzimer, Alina and Dud{\'i}k, Miroslav and Langford, John and Wallach, Hanna},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/MGICU29F/Agarwal et al. - 2018 - A Reductions Approach to Fair Classification.pdf;/Users/mfine/Zotero/storage/U92N5RKU/1803.html}
}

@article{AR,
  title = {Gentle {{Measurement}} of {{Quantum States}} and {{Differential Privacy}}},
  abstract = {In differential privacy (DP), we want to query a database about n users, in a way that ``leaks at most {$\epsilon$} about any individual user,'' even conditioned on any outcome of the query. Meanwhile, in gentle measurement, we want to measure n quantum states, in a way that ``damages the states by at most {$\alpha$},'' even conditioned on any outcome of the measurement. In both cases, we can achieve the goal by techniques like deliberately adding noise to the outcome before returning it. This paper proves a new and general connection between the two subjects. Specifically, we show that on products of n quantum states, any measurement that i{$\surd$}s {$\alpha$}-gentle for small {$\alpha$} is also O ({$\alpha$})-DP, and any product measurement that is {$\epsilon$}-DP is also O ({$\epsilon$} n)-gentle.},
  language = {en},
  author = {Aaronson, Scott and Rothblum, Guy N},
  pages = {85},
  file = {/Users/mfine/Zotero/storage/ZXIYKIYD/Aaronson and Rothblum - Gentle Measurement of Quantum States and Dierenti.pdf}
}

@article{TUV12,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1205.1758},
  primaryClass = {cs},
  title = {Faster {{Algorithms}} for {{Privately Releasing Marginals}}},
  abstract = {We study the problem of releasing \$k\$-way marginals of a database \$D \textbackslash{}in (\textbackslash\{0,1\textbackslash\}\^d)\^n\$, while preserving differential privacy. The answer to a \$k\$-way marginal query is the fraction of \$D\$'s records \$x \textbackslash{}in \textbackslash\{0,1\textbackslash\}\^d\$ with a given value in each of a given set of up to \$k\$ columns. Marginal queries enable a rich class of statistical analyses of a dataset, and designing efficient algorithms for privately releasing marginal queries has been identified as an important open problem in private data analysis (cf. Barak et. al., PODS '07). We give an algorithm that runs in time \$d\^\{O(\textbackslash{}sqrt\{k\})\}\$ and releases a private summary capable of answering any \$k\$-way marginal query with at most \$\textbackslash{}pm .01\$ error on every query as long as \$n \textbackslash{}geq d\^\{O(\textbackslash{}sqrt\{k\})\}\$. To our knowledge, ours is the first algorithm capable of privately releasing marginal queries with non-trivial worst-case accuracy guarantees in time substantially smaller than the number of \$k\$-way marginal queries, which is \$d\^\{\textbackslash{}Theta(k)\}\$ (for \$k \textbackslash{}ll d\$).},
  journal = {arXiv:1205.1758 [cs]},
  author = {Thaler, Justin and Ullman, Jonathan and Vadhan, Salil},
  month = may,
  year = {2012},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/28ZMHU54/Thaler et al. - 2012 - Faster Algorithms for Privately Releasing Marginal.pdf;/Users/mfine/Zotero/storage/UT4GR3VU/1205.html}
}

@article{BL16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.01063},
  primaryClass = {stat},
  title = {Comparative {{Study}} of {{Differentially Private Data Synthesis Methods}}},
  abstract = {When sharing data among researchers or releasing data for public use, there is a risk of exposing sensitive information of individuals in the data set. Data synthesis (DS) is a statistical disclosure limitation technique for releasing synthetic data sets with pseudo individual records. Traditional DS techniques often rely on strong assumptions of a data intruder's behaviors and background knowledge to assess disclosure risk. Differential privacy (DP) formulates a theoretical approach for a strong and robust privacy guarantee in data release without having to model intruders' behaviors. Efforts have been made aiming to incorporate the DP concept in the DS process. In this paper, we examine current DIfferentially Private Data Synthesis (DIPS) techniques for releasing individual-level surrogate data for the original data, compare the techniques conceptually, and evaluate the statistical utility and inferential properties of the synthetic data via each DIPS technique through extensive simulation studies. Our work sheds light on the practical feasibility and utility of the various DIPS approaches, and suggests future research directions for DIPS.},
  journal = {arXiv:1602.01063 [stat]},
  author = {Bowen, Claire McKay and Liu, Fang},
  month = feb,
  year = {2016},
  keywords = {Statistics - Methodology},
  file = {/Users/mfine/Zotero/storage/JUX9YC2Y/Bowen and Liu - 2016 - Comparative Study of Differentially Private Data S.pdf;/Users/mfine/Zotero/storage/ZUC8DITZ/1602.html}
}

@misc{Wan19,
  title = {A Light-Weight Imperative Language for Developing Provably Privacy-Preserving Algorithms: Yxwangcs/Lightdp},
  copyright = {MIT},
  shorttitle = {A Light-Weight Imperative Language for Developing Provably Privacy-Preserving Algorithms},
  author = {Wang, Yuxin (Ryan)},
  month = may,
  year = {2019}
}

@article{BFG+16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.00283},
  title = {Differentially {{Private Bayesian Programming}}},
  abstract = {We present PrivInfer, an expressive framework for writing and verifying differentially private Bayesian machine learning algorithms. Programs in PrivInfer are written in a rich functional probabilistic programming language with constructs for performing Bayesian inference. Then, differential privacy of programs is established using a relational refinement type system, in which refinements on probability types are indexed by a metric on distributions. Our framework leverages recent developments in Bayesian inference, probabilistic programming languages, and in relational refinement types. We demonstrate the expressiveness of PrivInfer by verifying privacy for several examples of private Bayesian inference.},
  journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security - CCS'16},
  doi = {10.1145/2976749.2978371},
  author = {Barthe, Gilles and Farina, Gian Pietro and Gaboardi, Marco and Arias, Emilio Jes{\`u}s Gallego and Gordon, Andy and Hsu, Justin and Strub, Pierre-Yves},
  year = {2016},
  keywords = {Computer Science - Programming Languages},
  pages = {68-79},
  file = {/Users/mfine/Zotero/storage/C5BX8WTH/Barthe et al. - 2016 - Differentially Private Bayesian Programming.pdf;/Users/mfine/Zotero/storage/8KW4SF2X/1605.html}
}

@article{GHH+,
  title = {Linear {{Dependent Types}} for {{Differential Privacy}}},
  abstract = {Differential privacy offers a way to answer queries about sensitive information while providing strong, provable privacy guarantees, ensuring that the presence or absence of a single individual in the database has a negligible statistical effect on the query's result. Proving that a given query has this property involves establishing a bound on the query's sensitivity\textemdash{}how much its result can change when a single record is added or removed.},
  language = {en},
  author = {Gaboardi, Marco and Haeberlen, Andreas and Hsu, Justin and Narayan, Arjun and Pierce, Benjamin C},
  pages = {14},
  file = {/Users/mfine/Zotero/storage/RHZZA9JJ/Gaboardi et al. - Linear Dependent Types for Differential Privacy.pdf}
}

@article{LP79,
  title = {Social {{Processes}} and {{Proofs}} of {{Theorems}} and {{Programs}}},
  volume = {22},
  abstract = {It is argued that formal verifications of programs, no matter how obtained, will not play the same key role in the development of computer science and software engineering as proofs do in mathematics. Furthermore the absence of continuity, the inevitability of change, and the complexity of specification of significantly many real programs make the formal verification process difficult to justify and manage. It is felt that ease of formal verification should not dominate program language design.},
  language = {en},
  number = {5},
  author = {Lipton, Richard J and Perlis, Alan J},
  year = {1979},
  pages = {10},
  file = {/Users/mfine/Zotero/storage/R85TXAWV/Lipton and Perlis - 1979 - Social Processes and Proofs of Theorems and Progra.pdf}
}

@inproceedings{Abo18,
  address = {{London, United Kingdom}},
  title = {The {{U}}.{{S}}. {{Census Bureau Adopts Differential Privacy}}},
  isbn = {978-1-4503-5552-0},
  abstract = {The U.S. Census Bureau announced, via its Scientific Advisory Committee, that it would protect the publications of the 2018 End-to-End Census Test (E2E) using differential privacy. The E2E test is a dress rehearsal for the 2020 Census, the constitutionally mandated enumeration of the population used to reapportion the House of Representatives and redraw every legislative district in the country. Systems that perform successfully in the E2E test are then used in the production of the 2020 Census.},
  language = {en},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '18},
  publisher = {{ACM Press}},
  doi = {10.1145/3219819.3226070},
  author = {Abowd, John M.},
  year = {2018},
  pages = {2867-2867},
  file = {/Users/mfine/Zotero/storage/7LSPUBW6/Abowd - 2018 - The U.S. Census Bureau Adopts Differential Privacy.pdf}
}

@article{DR13,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  volume = {9},
  issn = {1551-305X, 1551-3068},
  language = {en},
  number = {3-4},
  journal = {Foundations and Trends\textregistered{} in Theoretical Computer Science},
  doi = {10.1561/0400000042},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2013},
  pages = {211-407},
  file = {/Users/mfine/Zotero/storage/FNJ5X6YA/Dwork and Roth - 2013 - The Algorithmic Foundations of Differential Privac.pdf}
}

@article{HLM10a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1012.4763},
  primaryClass = {cs},
  title = {A Simple and Practical Algorithm for Differentially Private Data Release},
  abstract = {We present new theoretical results on differentially private data release useful with respect to any target class of counting queries, coupled with experimental results on a variety of real world data sets. Specifically, we study a simple combination of the multiplicative weights approach of [Hardt and Rothblum, 2010] with the exponential mechanism of [McSherry and Talwar, 2007]. The multiplicative weights framework allows us to maintain and improve a distribution approximating a given data set with respect to a set of counting queries. We use the exponential mechanism to select those queries most incorrectly tracked by the current distribution. Combing the two, we quickly approach a distribution that agrees with the data set on the given set of queries up to small error. The resulting algorithm and its analysis is simple, but nevertheless improves upon previous work in terms of both error and running time. We also empirically demonstrate the practicality of our approach on several data sets commonly used in the statistical community for contingency table release.},
  journal = {arXiv:1012.4763 [cs]},
  author = {Hardt, Moritz and Ligett, Katrina and McSherry, Frank},
  month = dec,
  year = {2010},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/AABW33HE/Hardt et al. - 2010 - A simple and practical algorithm for differentiall.pdf;/Users/mfine/Zotero/storage/RE5G9IZB/1012.html}
}

@article{APS14,
  title = {Formal Verification of Security Protocol Implementations: A Survey},
  volume = {26},
  issn = {0934-5043, 1433-299X},
  shorttitle = {Formal Verification of Security Protocol Implementations},
  abstract = {Automated formal verification of security protocols has been mostly focused on analyzing highlevel abstract models which, however, are significantly different from real protocol implementations written in programming languages. Recently, some researchers have started investigating techniques that bring automated formal proofs closer to real implementations. This paper surveys these attempts, focusing on approaches that target the application code that implements protocol logic, rather than the libraries that implement cryptography. According to these approaches, libraries are assumed to correctly implement some models. The aim is to derive formal proofs that, under this assumption, give assurance about the application code that implements the protocol logic. The two main approaches of model extraction and code generation are presented, along with the main techniques adopted for each approach.},
  language = {en},
  number = {1},
  journal = {Formal Aspects of Computing},
  doi = {10.1007/s00165-012-0269-9},
  author = {Avalle, Matteo and Pironti, Alfredo and Sisto, Riccardo},
  month = jan,
  year = {2014},
  pages = {99-123},
  file = {/Users/mfine/Zotero/storage/8UAQQPCN/Avalle et al. - 2014 - Formal verification of security protocol implement.pdf}
}

@inproceedings{zotero-229,
  title = {{{UNDERSTAND THE DYNAMICS OF GANS VIA PRIMAL}}-{{DUAL OPTIMIZATION}}},
  file = {/Users/mfine/Zotero/storage/R5VNSFLC/pdf.pdf}
}

@article{HHYY19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.05914},
  title = {How {{Generative Adversarial Networks}} and {{Their Variants Work}}: {{An Overview}}},
  volume = {52},
  issn = {03600300},
  shorttitle = {How {{Generative Adversarial Networks}} and {{Their Variants Work}}},
  abstract = {Generative Adversarial Networks (GAN) have received wide attention in the machine learning field for their potential to learn high-dimensional, complex real data distribution. Specifically, they do not rely on any assumptions about the distribution and can generate real-like samples from latent space in a simple manner. This powerful property leads GAN to be applied to various applications such as image synthesis, image attribute editing, image translation, domain adaptation and other academic fields. In this paper, we aim to discuss the details of GAN for those readers who are familiar with, but do not comprehend GAN deeply or who wish to view GAN from various perspectives. In addition, we explain how GAN operates and the fundamental meaning of various objective functions that have been suggested recently. We then focus on how the GAN can be combined with an autoencoder framework. Finally, we enumerate the GAN variants that are applied to various tasks and other fields for those who are interested in exploiting GAN for their research.},
  number = {1},
  journal = {ACM Computing Surveys},
  doi = {10.1145/3301282},
  author = {Hong, Yongjun and Hwang, Uiwon and Yoo, Jaeyoon and Yoon, Sungroh},
  month = feb,
  year = {2019},
  keywords = {Computer Science - Machine Learning,explanation},
  pages = {1-43},
  file = {/Users/mfine/Zotero/storage/WX96AQA9/Hong et al. - 2019 - How Generative Adversarial Networks and Their Vari.pdf;/Users/mfine/Zotero/storage/T99PK9YM/1711.html}
}

@article{WDW+19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.12254},
  primaryClass = {cs},
  title = {Proving {{Differential Privacy}} with {{Shadow Execution}}},
  abstract = {Recent work on formal verification of differential privacy shows a trend toward usability and expressiveness -- generating a correctness proof of sophisticated algorithm while minimizing the annotation burden on programmers. Sometimes, combining those two requires substantial changes to program logics: one recent paper is able to verify Report Noisy Max automatically, but it involves a complex verification system using customized program logics and verifiers. In this paper, we propose a new proof technique, called shadow execution, and embed it into a language called ShadowDP. ShadowDP uses shadow execution to generate proofs of differential privacy with very few programmer annotations and without relying on customized logics and verifiers. In addition to verifying Report Noisy Max, we show that it can verify a new variant of Sparse Vector that reports the gap between some noisy query answers and the noisy threshold. Moreover, ShadowDP reduces the complexity of verification: for all of the algorithms we have evaluated, type checking and verification in total takes at most 3 seconds, while prior work takes minutes on the same algorithms.},
  journal = {arXiv:1903.12254 [cs]},
  doi = {10.1145/3314221.3314619},
  author = {Wang, Yuxin and Ding, Zeyu and Wang, Guanhong and Kifer, Daniel and Zhang, Danfeng},
  month = mar,
  year = {2019},
  keywords = {Computer Science - Programming Languages,D.2.4},
  file = {/Users/mfine/Zotero/storage/8XAGEP6F/Wang et al. - 2019 - Proving Differential Privacy with Shadow Execution.pdf;/Users/mfine/Zotero/storage/DWU52GNS/1903.html}
}

@article{Bor18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03446},
  primaryClass = {cs},
  title = {Pros and {{Cons}} of {{GAN Evaluation Measures}}},
  abstract = {Generative models, in particular generative adversarial networks (GANs), have received significant attention recently. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
  journal = {arXiv:1802.03446 [cs]},
  author = {Borji, Ali},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/mfine/Zotero/storage/VN4RFX38/Borji - 2018 - Pros and Cons of GAN Evaluation Measures.pdf;/Users/mfine/Zotero/storage/M5AY7BEG/1802.html}
}

@article{HL14,
  title = {Privacy and {{Data}}-{{Based Research}}},
  volume = {28},
  issn = {0895-3309},
  language = {en},
  number = {2},
  journal = {Journal of Economic Perspectives},
  doi = {10.1257/jep.28.2.75},
  author = {Heffetz, Ori and Ligett, Katrina},
  month = may,
  year = {2014},
  pages = {75-98},
  file = {/Users/mfine/Zotero/storage/6TDVIIYT/Heffetz and Ligett - 2014 - Privacy and Data-Based Research.pdf}
}

@inproceedings{Ull15,
  address = {{Melbourne, Victoria, Australia}},
  title = {Private {{Multiplicative Weights Beyond Linear Queries}}},
  isbn = {978-1-4503-2757-2},
  abstract = {A wide variety of fundamental data analyses in machine learning, such as linear and logistic regression, require minimizing a convex function defined by the data. Since the data may contain sensitive information about individuals, and these analyses can leak that sensitive information, it is important to be able to solve convex minimization in a privacy-preserving way.},
  language = {en},
  booktitle = {Proceedings of the 34th {{ACM Symposium}} on {{Principles}} of {{Database Systems}} - {{PODS}} '15},
  publisher = {{ACM Press}},
  doi = {10.1145/2745754.2745755},
  author = {Ullman, Jonathan},
  year = {2015},
  pages = {303-312},
  file = {/Users/mfine/Zotero/storage/KQNNDULZ/Ullman - 2015 - Private Multiplicative Weights Beyond Linear Queri.pdf}
}

@incollection{UV11,
  address = {{Berlin, Heidelberg}},
  title = {{{PCPs}} and the {{Hardness}} of {{Generating Private Synthetic Data}}},
  volume = {6597},
  isbn = {978-3-642-19570-9 978-3-642-19571-6},
  abstract = {Assuming the existence of one-way functions, we show that there is no polynomial-time, differentially private algorithm A that takes a database D {$\in$} (\{0, 1\}d)n and outputs a ``synthetic database'' D all of whose two-way marginals are approximately equal to those of D. (A two-way marginal is the fraction of database rows x {$\in$} \{0, 1\}d with a given pair of values in a given pair of columns.) This answers a question of Barak et al. (PODS `07), who gave an algorithm running in time poly(n, 2d).},
  language = {en},
  booktitle = {Theory of {{Cryptography}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Ullman, Jonathan and Vadhan, Salil},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ishai, Yuval},
  year = {2011},
  pages = {400-416},
  file = {/Users/mfine/Zotero/storage/S4W94ZJ4/Ullman and Vadhan - 2011 - PCPs and the Hardness of Generating Private Synthe.pdf},
  doi = {10.1007/978-3-642-19571-6_24}
}

@article{GHRU10b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1296},
  primaryClass = {cs},
  title = {Privately {{Releasing Conjunctions}} and the {{Statistical Query Barrier}}},
  abstract = {Suppose we would like to know all answers to a set of statistical queries C on a data set up to small error, but we can only access the data itself using statistical queries. A trivial solution is to exhaustively ask all queries in C. Can we do any better? + We show that the number of statistical queries necessary and sufficient for this task is---up to polynomial factors---equal to the agnostic learning complexity of C in Kearns' statistical query (SQ) model. This gives a complete answer to the question when running time is not a concern. + We then show that the problem can be solved efficiently (allowing arbitrary error on a small fraction of queries) whenever the answers to C can be described by a submodular function. This includes many natural concept classes, such as graph cuts and Boolean disjunctions and conjunctions. While interesting from a learning theoretic point of view, our main applications are in privacy-preserving data analysis: Here, our second result leads to the first algorithm that efficiently releases differentially private answers to of all Boolean conjunctions with 1\% average error. This presents significant progress on a key open problem in privacy-preserving data analysis. Our first result on the other hand gives unconditional lower bounds on any differentially private algorithm that admits a (potentially non-privacy-preserving) implementation using only statistical queries. Not only our algorithms, but also most known private algorithms can be implemented using only statistical queries, and hence are constrained by these lower bounds. Our result therefore isolates the complexity of agnostic learning in the SQ-model as a new barrier in the design of differentially private algorithms.},
  journal = {arXiv:1011.1296 [cs]},
  author = {Gupta, Anupam and Hardt, Moritz and Roth, Aaron and Ullman, Jonathan},
  month = nov,
  year = {2010},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/23MDBN98/Gupta et al. - 2010 - Privately Releasing Conjunctions and the Statistic.pdf;/Users/mfine/Zotero/storage/3Q4IK3B4/1011.html}
}

@article{LO,
  title = {A {{Practical Application}} of {{Differential Privacy}} to {{Personalized Online Advertising}}},
  abstract = {Online advertising plays an important role in supporting many Internet services. Personalized online advertising offers marketers a way to direct ads at very specific audiences. The vast body of Internet users combined with the ease of creating and monitoring personalized advertising campaigns make online advertising an extremely strong tool for marketers. However, many concerns arise regarding the implications of online advertising for the privacy of web users. Specifically, recent works show how the privacy of Internet users may be breached by attacks utilizing personalized advertising campaigns such as those provided by Facebook. Such attacks succeed even without the user ever noticing the attack or being able to avoid it (unless refraining from going on the Internet).},
  language = {en},
  author = {Lindell, Yehuda and Omri, Eran},
  pages = {21},
  file = {/Users/mfine/Zotero/storage/JD8R5ZMT/Lindell and Omri - A Practical Application of Dierential Privacy to .pdf}
}

@inproceedings{KM11a,
  address = {{Athens, Greece}},
  title = {No Free Lunch in Data Privacy},
  isbn = {978-1-4503-0661-4},
  abstract = {Differential privacy is a powerful tool for providing privacypreserving noisy query answers over statistical databases. It guarantees that the distribution of noisy query answers changes very little with the addition or deletion of any tuple. It is frequently accompanied by popularized claims that it provides privacy without any assumptions about the data and that it protects against attackers who know all but one record. In this paper we critically analyze the privacy protections offered by differential privacy.},
  language = {en},
  booktitle = {Proceedings of the 2011 International Conference on {{Management}} of Data - {{SIGMOD}} '11},
  publisher = {{ACM Press}},
  doi = {10.1145/1989323.1989345},
  author = {Kifer, Daniel and Machanavajjhala, Ashwin},
  year = {2011},
  pages = {193},
  file = {/Users/mfine/Zotero/storage/UH5BA56C/Kifer and Machanavajjhala - 2011 - No free lunch in data privacy.pdf}
}

@article{BLM19a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.03468},
  primaryClass = {cs, stat},
  title = {Passing {{Tests}} without {{Memorizing}}: {{Two Models}} for {{Fooling Discriminators}}},
  shorttitle = {Passing {{Tests}} without {{Memorizing}}},
  abstract = {We introduce two mathematical frameworks for foolability in the context of generative distribution learning. In a nuthsell, fooling is an algorithmic task in which the input sample is drawn from some target distribution and the goal is to output a synthetic distribution that is indistinguishable from the target w.r.t to some fixed class of tests. This framework received considerable attention in the context of Generative Adversarial Networks (GANs), a recently proposed approach which achieves impressive empirical results. From a theoretical viewpoint this problem seems difficult to model. This is due to the fact that in its basic form, the notion of foolability is susceptible to a type of overfitting called memorizing. This raises a challenge of devising notions and definitions that separate between fooling algorithms that generate new synthetic data vs. algorithms that merely memorize or copy the training set. The first model we consider is called GAM--Foolability and is inspired by GANs. Here the learner has only an indirect access to the target distribution via a discriminator. The second model, called DP--Foolability, exploits the notion of differential privacy as a candidate criterion for non-memorization. We proceed to characterize foolability within these two models and study their interrelations. We show that DP--Foolability implies GAM--Foolability and prove partial results with respect to the converse. It remains, though, an open question whether GAM--Foolability implies DP--Foolability. We also present an application in the context of differentially private PAC learning. We show that from a statistical perspective, for any class H, learnability by a private proper learner is equivalent to the existence of a private sanitizer for H. This can be seen as an analogue of the equivalence between uniform convergence and learnability in classical PAC learning.},
  journal = {arXiv:1902.03468 [cs, stat]},
  author = {Bousquet, Olivier and Livni, Roi and Moran, Shay},
  month = feb,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/5D5YQKQM/Bousquet et al. - 2019 - Passing Tests without Memorizing Two Models for F.pdf;/Users/mfine/Zotero/storage/6YSHEVSL/1902.html}
}

@article{HLM10,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1012.4763},
  primaryClass = {cs},
  title = {A Simple and Practical Algorithm for Differentially Private Data Release},
  abstract = {We present new theoretical results on differentially private data release useful with respect to any target class of counting queries, coupled with experimental results on a variety of real world data sets. Specifically, we study a simple combination of the multiplicative weights approach of [Hardt and Rothblum, 2010] with the exponential mechanism of [McSherry and Talwar, 2007]. The multiplicative weights framework allows us to maintain and improve a distribution approximating a given data set with respect to a set of counting queries. We use the exponential mechanism to select those queries most incorrectly tracked by the current distribution. Combing the two, we quickly approach a distribution that agrees with the data set on the given set of queries up to small error. The resulting algorithm and its analysis is simple, but nevertheless improves upon previous work in terms of both error and running time. We also empirically demonstrate the practicality of our approach on several data sets commonly used in the statistical community for contingency table release.},
  journal = {arXiv:1012.4763 [cs]},
  author = {Hardt, Moritz and Ligett, Katrina and McSherry, Frank},
  month = dec,
  year = {2010},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/D5XEB3HS/Hardt et al. - 2010 - A simple and practical algorithm for differentiall.pdf;/Users/mfine/Zotero/storage/VY2NDD9H/1012.html}
}

@article{GHRU10a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1296},
  primaryClass = {cs},
  title = {Privately {{Releasing Conjunctions}} and the {{Statistical Query Barrier}}},
  abstract = {Suppose we would like to know all answers to a set of statistical queries C on a data set up to small error, but we can only access the data itself using statistical queries. A trivial solution is to exhaustively ask all queries in C. Can we do any better? + We show that the number of statistical queries necessary and sufficient for this task is---up to polynomial factors---equal to the agnostic learning complexity of C in Kearns' statistical query (SQ) model. This gives a complete answer to the question when running time is not a concern. + We then show that the problem can be solved efficiently (allowing arbitrary error on a small fraction of queries) whenever the answers to C can be described by a submodular function. This includes many natural concept classes, such as graph cuts and Boolean disjunctions and conjunctions. While interesting from a learning theoretic point of view, our main applications are in privacy-preserving data analysis: Here, our second result leads to the first algorithm that efficiently releases differentially private answers to of all Boolean conjunctions with 1\% average error. This presents significant progress on a key open problem in privacy-preserving data analysis. Our first result on the other hand gives unconditional lower bounds on any differentially private algorithm that admits a (potentially non-privacy-preserving) implementation using only statistical queries. Not only our algorithms, but also most known private algorithms can be implemented using only statistical queries, and hence are constrained by these lower bounds. Our result therefore isolates the complexity of agnostic learning in the SQ-model as a new barrier in the design of differentially private algorithms.},
  journal = {arXiv:1011.1296 [cs]},
  author = {Gupta, Anupam and Hardt, Moritz and Roth, Aaron and Ullman, Jonathan},
  month = nov,
  year = {2010},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/35UJEXGB/Gupta et al. - 2010 - Privately Releasing Conjunctions and the Statistic.pdf;/Users/mfine/Zotero/storage/4EHMDY7U/1011.html}
}

@inproceedings{TLH+18,
  title = {Exploring the {{Relationship Between Dimensionality Reduction}} and {{Private Data Release}}},
  abstract = {It is important to facilitate data sharing between data owners and data analysts as data owners do not always have the ability to process and analyze data. For example, governments around the world are starting to release collected data to the public to leverage data analysis competence of the crowd. However, some privacy leakage incidents have made the public to rediscover the importance of privacy protection, leading to new privacy regulations. In existing researches dimensionality reduction has played an important role in private data release mechanisms to improve utility but its influence on privacy protection has never been examined. In this study, we perform a series of experiments and found that dimensionality reduction could provide similar privacy protection effects as K-anonymity mechanisms, and it could work as a preprocessor of K-anonymity process to it to reduce the generalization and suppression needed.},
  booktitle = {2018 {{IEEE}} 23rd {{Pacific Rim International Symposium}} on {{Dependable Computing}} ({{PRDC}})},
  doi = {10.1109/PRDC.2018.00013},
  author = {Tai, B. and Li, S. and Huang, Y. and Suri, N. and Wang, P.},
  month = dec,
  year = {2018},
  keywords = {Differential privacy,data protection,Privacy,privacy protection,data analysis,data reduction,data sharing,dimensionality reduction,Dimensionality reduction,dimensionality reduction; k-anonymity; private data release,Information technology,Principal component analysis,privacy leakage incidents,privacy regulations,private data,Technological innovation},
  pages = {25-33},
  file = {/Users/mfine/Zotero/storage/NQCGVFFG/Tai et al. - 2018 - Exploring the Relationship Between Dimensionality .pdf;/Users/mfine/Zotero/storage/4GEW2TDZ/8639039.html}
}

@misc{Rom17,
  title = {An {{Annotated Proof}} of {{Generative Adversarial Networks}} with {{Implementation Notes}}},
  abstract = {The preferred illustration of Generative Adversarial Networks (which seems indoctrinated at this point like Bob, Alice and Eve for cryptography) is the scenario of counterfitting money. The Generator is a counterfits money and the Discriminator is supposed to discriminate between real and fake dollars. As the Generator gets better, the Discriminator has to improve also. This implies a dual training scheme where each model tries to one up the other (i.e. through additional learning). In this post, we will explore the proof from the original paper and demonstrate a typical implementation.},
  journal = {Scott Rome},
  howpublished = {http://srome.github.io//An-Annotated-Proof-of-Generative-Adversarial-Networks-with-Implementation-Notes/},
  author = {Rome, Scott},
  month = aug,
  year = {2017},
  file = {/Users/mfine/Zotero/storage/6J436TSQ/An-Annotated-Proof-of-Generative-Adversarial-Networks-with-Implementation-Notes.html}
}

@misc{16a,
  title = {Wayback {{Machine}}},
  howpublished = {https://web.archive.org/web/20160222044201/https://tcs.epfl.ch/files/content/sites/tcs/files/Lec2-Fall14-Ver2.pdf},
  month = feb,
  year = {2016},
  file = {/Users/mfine/Zotero/storage/BMJ5A9J3/Lec2-Fall14-Ver2.html}
}

@article{Ull14,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.1571},
  primaryClass = {cs},
  title = {Private {{Multiplicative Weights Beyond Linear Queries}}},
  abstract = {A wide variety of fundamental data analyses in machine learning, such as linear and logistic regression, require minimizing a convex function defined by the data. Since the data may contain sensitive information about individuals, and these analyses can leak that sensitive information, it is important to be able to solve convex minimization in a privacy-preserving way. A series of recent results show how to accurately solve a single convex minimization problem in a differentially private manner. However, the same data is often analyzed repeatedly, and little is known about solving multiple convex minimization problems with differential privacy. For simpler data analyses, such as linear queries, there are remarkable differentially private algorithms such as the private multiplicative weights mechanism (Hardt and Rothblum, FOCS 2010) that accurately answer exponentially many distinct queries. In this work, we extend these results to the case of convex minimization and show how to give accurate and differentially private solutions to *exponentially many* convex minimization problems on a sensitive dataset.},
  journal = {arXiv:1407.1571 [cs]},
  author = {Ullman, Jonathan},
  month = jul,
  year = {2014},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/CFCV47FS/Ullman - 2014 - Private Multiplicative Weights Beyond Linear Queri.pdf;/Users/mfine/Zotero/storage/CR2SWM4G/1407.html}
}

@inproceedings{HR10,
  title = {A {{Multiplicative Weights Mechanism}} for {{Privacy}}-{{Preserving Data Analysis}}},
  abstract = {We consider statistical data analysis in the interactive setting. In this setting a trusted curator maintains a database of sensitive information about individual participants, and releases privacy-preserving answers to queries as they arrive. Our primary contribution is a new differentially private multiplicative weights mechanism for answering a large number of interactive counting (or linear) queries that arrive online and may be adaptively chosen. This is the first mechanism with worst-case accuracy guarantees that can answer large numbers of interactive queries and is efficient (in terms of the runtime's dependence on the data universe size). The error is asymptotically optimal in its dependence on the number of participants, and depends only logarithmically on the number of queries being answered. The running time is nearly linear in the size of the data universe. As a further contribution, when we relax the utility requirement and require accuracy only for databases drawn from a rich class of databases, we obtain exponential improvements in running time. Even in this relaxed setting we continue to guarantee privacy for any input database. Only the utility requirement is relaxed. Specifically, we show that when the input database is drawn from a smooth distribution - a distribution that does not place too much weight on any single data item - accuracy remains as above, and the running time becomes poly-logarithmic in the data universe size. The main technical contributions are the application of multiplicative weights techniques to the differential privacy setting, a new privacy analysis for the interactive setting, and a technique for reducing data dimensionality for databases drawn from smooth distributions.},
  booktitle = {2010 {{IEEE}} 51st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  doi = {10.1109/FOCS.2010.85},
  author = {Hardt, M. and Rothblum, G. N.},
  month = oct,
  year = {2010},
  keywords = {data privacy,Data privacy,Privacy,data analysis,Accuracy,data dimensionality reduction,data universe,Databases,differentially private multiplicative weights mechanism,Histograms,multiplicative weights mechanism,Noise,Noise measurement,privacy-preserving data analysis,query processing,question answering (information retrieval),statistical analysis,statistical data analysis},
  pages = {61-70},
  file = {/Users/mfine/Zotero/storage/WIGIRW9F/Hardt and Rothblum - 2010 - A Multiplicative Weights Mechanism for Privacy-Pre.pdf;/Users/mfine/Zotero/storage/QZXDE8Y2/5670948.html}
}

@article{ZLZY17,
  title = {Differentially {{Private Data Publishing}} and {{Analysis}}: {{A Survey}}},
  volume = {29},
  issn = {1041-4347},
  shorttitle = {Differentially {{Private Data Publishing}} and {{Analysis}}},
  abstract = {Differential privacy is an essential and prevalent privacy model that has been widely explored in recent decades. This survey provides a comprehensive and structured overview of two research directions: differentially private data publishing and differentially private data analysis. We compare the diverse release mechanisms of differentially private data publishing given a variety of input data in terms of query type, the maximum number of queries, efficiency, and accuracy. We identify two basic frameworks for differentially private data analysis and list the typical algorithms used within each framework. The results are compared and discussed based on output accuracy and efficiency. Further, we propose several possible directions for future research and possible applications.},
  number = {8},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  doi = {10.1109/TKDE.2017.2697856},
  author = {Zhu, T. and Li, G. and Zhou, W. and Yu, P. S.},
  month = aug,
  year = {2017},
  keywords = {Differential privacy,data privacy,Data privacy,Privacy,data analysis,query processing,Algorithm design and analysis,Data analysis,Data models,data structure,data structures,differentially private data analysis,differentially private data publishing,privacy preserving data analysis,privacy preserving data publishing,publishing,Publishing,query type,Sensitivity},
  pages = {1619-1638},
  file = {/Users/mfine/Zotero/storage/I2DT48Y9/Zhu et al. - 2017 - Differentially Private Data Publishing and Analysi.pdf;/Users/mfine/Zotero/storage/GD2RUKIN/7911185.html}
}

@incollection{BR13,
  address = {{Berlin, Heidelberg}},
  title = {Fast {{Private Data Release Algorithms}} for {{Sparse Queries}}},
  volume = {8096},
  isbn = {978-3-642-40327-9 978-3-642-40328-6},
  abstract = {We revisit the problem of accurately answering large classes of statistical queries while preserving differential privacy. Previous approaches to this problem have either been very general but have not had run-time polynomial in the size of the database, have applied only to very limited classes of queries, or have relaxed the notion of worst-case error guarantees. In this paper we consider the large class of sparse queries, which take non-zero values on only polynomially many universe elements. We give efficient query release algorithms for this class, in both the interactive and the non-interactive setting. Our algorithms also achieve better accuracy bounds than previous general techniques do when applied to sparse queries: our bounds are independent of the universe size. In fact, even the runtime of our interactive mechanism is independent of the universe size, and so can be implemented in the ``infinite universe'' model in which no finite universe need be specified by the data curator.},
  language = {en},
  booktitle = {Approximation, {{Randomization}}, and {{Combinatorial Optimization}}. {{Algorithms}} and {{Techniques}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Blum, Avrim and Roth, Aaron},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Raghavendra, Prasad and Raskhodnikova, Sofya and Jansen, Klaus and Rolim, Jos{\'e} D. P.},
  year = {2013},
  pages = {395-410},
  file = {/Users/mfine/Zotero/storage/AVRF3NDS/Blum and Roth - 2013 - Fast Private Data Release Algorithms for Sparse Qu.pdf},
  doi = {10.1007/978-3-642-40328-6_28}
}

@article{XV18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.11264},
  primaryClass = {cs, stat},
  title = {Synthesizing {{Tabular Data}} Using {{Generative Adversarial Networks}}},
  abstract = {Generative adversarial networks (GANs) implicitly learn the probability distribution of a dataset and can draw samples from the distribution. This paper presents, Tabular GAN (TGAN), a generative adversarial network which can generate tabular data like medical or educational records. Using the power of deep neural networks, TGAN generates high-quality and fully synthetic tables while simultaneously generating discrete and continuous variables. When we evaluate our model on three datasets, we find that TGAN outperforms conventional statistical generative models in both capturing the correlation between columns and scaling up for large datasets.},
  journal = {arXiv:1811.11264 [cs, stat]},
  author = {Xu, Lei and Veeramachaneni, Kalyan},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/8TH9EEZT/Xu and Veeramachaneni - 2018 - Synthesizing Tabular Data using Generative Adversa.pdf;/Users/mfine/Zotero/storage/UYECXQW8/1811.html}
}

@article{YZWY16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.05473},
  primaryClass = {cs},
  title = {{{SeqGAN}}: {{Sequence Generative Adversarial Nets}} with {{Policy Gradient}}},
  shorttitle = {{{SeqGAN}}},
  abstract = {As a new way of training generative models, Generative Adversarial Nets (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.},
  journal = {arXiv:1609.05473 [cs]},
  author = {Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/KVCDMXRH/Yu et al. - 2016 - SeqGAN Sequence Generative Adversarial Nets with .pdf;/Users/mfine/Zotero/storage/BDLKY9VQ/1609.html}
}

@article{BLR11,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1109.2229},
  primaryClass = {cs},
  title = {A {{Learning Theory Approach}} to {{Non}}-{{Interactive Database Privacy}}},
  abstract = {In this paper we demonstrate that, ignoring computational constraints, it is possible to privately release synthetic databases that are useful for large classes of queries -- much larger in size than the database itself. Specifically, we give a mechanism that privately releases synthetic data for a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries. We show that this in particular implies a mechanism for counting queries that gives error guarantees that grow only with the VC-dimension of the class of queries, which itself grows only logarithmically with the size of the query class. We also show that it is not possible to privately release even simple classes of queries (such as intervals and their generalizations) over continuous domains. Despite this, we give a privacy-preserving polynomial time algorithm that releases information useful for all halfspace queries, given a slight relaxation of the utility guarantee. This algorithm does not release synthetic data, but instead another data structure capable of representing an answer for each query. We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis-aligned rectangles of constant dimension. Finally, inspired by learning theory, we introduce a new notion of data privacy, which we call distributional privacy, and show that it is strictly stronger than the prevailing privacy notion, differential privacy.},
  journal = {arXiv:1109.2229 [cs]},
  author = {Blum, Avrim and Ligett, Katrina and Roth, Aaron},
  month = sep,
  year = {2011},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/7P92WLZL/Blum et al. - 2011 - A Learning Theory Approach to Non-Interactive Data.pdf;/Users/mfine/Zotero/storage/K63ZD7ZT/1109.html}
}

@article{Ull12,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1207.6945},
  primaryClass = {cs},
  title = {Answering N\^\{2+o(1)\} {{Counting Queries}} with {{Differential Privacy}} Is {{Hard}}},
  abstract = {A central problem in differentially private data analysis is how to design efficient algorithms capable of answering large numbers of counting queries on a sensitive database. Counting queries of the form "What fraction of individual records in the database satisfy the property q?" We prove that if one-way functions exist, then there is no algorithm that takes as input a database D in (\{0,1\}\^d)\^n, and k = n\^\{2+o(1)\} arbitrary efficiently computable counting queries, runs in time poly(d, n), and returns an approximate answer to each query, while satisfying differential privacy. We also consider the complexity of answering "simple" counting queries, and make some progress in this direction by showing that the above result holds even when we require that the queries are computable by constant depth (AC-0) circuits. Our result is almost tight in the sense that nearly n\^2 counting queries can be answered efficiently while satisfying differential privacy. Moreover, super-polynomially many queries can be answered in exponential time. We prove our results by extending the connection between differentially private counting query release and cryptographic traitor-tracing schemes to the setting where the queries are given to the sanitizer as input, and by constructing a traitor-tracing scheme that is secure in this setting.},
  journal = {arXiv:1207.6945 [cs]},
  author = {Ullman, Jonathan},
  month = jul,
  year = {2012},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Computational Complexity},
  file = {/Users/mfine/Zotero/storage/ATHKFETK/Ullman - 2012 - Answering n^ 2+o(1) Counting Queries with Differe.pdf;/Users/mfine/Zotero/storage/775BH5NS/1207.html}
}

@inproceedings{ZWL+18a,
  address = {{Toronto, Canada}},
  title = {{{CALM}}: {{Consistent Adaptive Local Marginal}} for {{Marginal Release}} under {{Local Differential Privacy}}},
  isbn = {978-1-4503-5693-0},
  shorttitle = {{{CALM}}},
  abstract = {Marginal tables are the workhorse of capturing the correlations among a set of attributes. We consider the problem of constructing marginal tables given a set of user's multi-dimensional data while satisfying Local Differential Privacy (LDP), a privacy notion that protects individual user's privacy without relying on a trusted third party. Existing works on this problem perform poorly in the high-dimensional setting; even worse, some incur very expensive computational overhead. In this paper, we propose CALM, Consistent Adaptive Local Marginal, that takes advantage of the careful challenge analysis and performs consistently better than existing methods. More importantly, CALM can scale well with large data dimensions and marginal sizes. We conduct extensive experiments on several real world datasets. Experimental results demonstrate the effectiveness and efficiency of CALM over existing methods.},
  language = {en},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}  - {{CCS}} '18},
  publisher = {{ACM Press}},
  doi = {10.1145/3243734.3243742},
  author = {Zhang, Zhikun and Wang, Tianhao and Li, Ninghui and He, Shibo and Chen, Jiming},
  year = {2018},
  keywords = {synth_ldp},
  pages = {212-229},
  file = {/Users/mfine/Zotero/storage/UQWRLGBD/Zhang et al. - 2018 - CALM Consistent Adaptive Local Marginal for Margi.pdf}
}

@inproceedings{YWRY17,
  title = {Copula-{{Based Multi}}-{{Dimensional Crowdsourced Data Synthesis}} and {{Release}} with {{Local Privacy}}},
  abstract = {Various paradigms, based on differential privacy, have been proposed to release a privacy-preserving dataset with statistical approximation. Nonetheless, most existing schemes are limited when facing highly correlated attributes, and cannot prevent privacy threats from untrusted servers. In this paper, we propose a novel Copula- based scheme to efficiently synthesize and release multi-dimensional crowdsourced data with local differential privacy. In our scheme, each participant's (or user's) data is locally transformed into bit strings based on a randomized response technique, which guarantees a participant's privacy on the participant (user) side. Then, Copula theory is leveraged to synthesize multi-dimensional crowdsourced data based on univariate marginal distribution and attribute dependence. Univariate marginal distribution is estimated by the Lasso-based regression algorithm from the aggregated privacy- preserving bit strings. Dependencies among attributes are modeled as multivariate Gaussian Copula, of which parameter is estimated by Pearson correlation coefficients. We conduct experiments to validate the effectiveness of our scheme. Our experimental results demonstrate that our scheme is effective for the release of multi-dimensional data with local differential privacy guaranteed to distributed participants.},
  booktitle = {{{GLOBECOM}} 2017 - 2017 {{IEEE Global Communications Conference}}},
  doi = {10.1109/GLOCOM.2017.8253989},
  author = {Yang, X. and Wang, T. and Ren, X. and Yu, W.},
  month = dec,
  year = {2017},
  keywords = {data privacy,Data privacy,Privacy,Correlation,crowdsourcing,local differential privacy,regression analysis,Servers,synth_ldp,aggregated privacy,aggregated privacy- preserving bit strings,attribute dependence,Copula theory,copula-based multidimensional crowdsourced data synthesis,Covariance matrices,Crowdsourcing,Distribution functions,Gaussian processes,Lasso-based regression algorithm,multidimensional data,multivariate Gaussian Copula,Pearson correlation coefficients,privacy threats,privacy-preserving dataset,randomized response technique,univariate marginal distribution},
  pages = {1-6},
  file = {/Users/mfine/Zotero/storage/5826WQHN/Yang et al. - 2017 - Copula-Based Multi-Dimensional Crowdsourced Data S.pdf;/Users/mfine/Zotero/storage/AVB6GNSJ/8253989.html}
}

@inproceedings{RYY+16c,
  title = {High-{{Dimensional Crowdsourced Data Distribution Estimation}} with {{Local Privacy}}},
  abstract = {High-dimensional crowdsourced data collected from a large number of users may produc3 rich knowledge for our society but also bring unprecedented privacy threats to participants. Recently differential privacy has been proposed as an effective means to mitigate privacy concerns. However, existing work on differential privacy suffers from the "curse of high-dimensionality" (data with multiple attributes) and high scalability (data with large scale records). Moreover, traditional methods of differential privacy were achieved via aggregation results, which cannot guarantee local privacy for distributed users in crowdsourced systems. To deal with these issues, in this paper we propose a novel scheme that can efficiently estimate multivariate joint distribution for high-dimensional data with local privacy. On the client side, we employ randomized response techniques to locally transform data from distributed users into privacy-preserving bit strings, which can prevent potential inside privacy attacks in crowdsourced systems. On the server side, the crowdsourced bit strings are aggregated for multivariate distribution estimation. Specifically, we first propose a multivariate version of the expectation maximization (EM) based algorithm to estimate the joint distribution of high dimensional data. To speed up the performance, unlike the EM-based method that needs to scan each user's bit string, we propose to use Lasso regression to obtain the distribution estimation from the aggregation information only once, which can significantly reduce the computation time for multivariate distribution estimation. Extensive experiments on real-world datasets demonstrate the efficiency of our multivariate distribution estimation scheme over existing estimation schemes.},
  booktitle = {2016 {{IEEE International Conference}} on {{Computer}} and {{Information Technology}} ({{CIT}})},
  doi = {10.1109/CIT.2016.57},
  author = {Ren, X. and Yu, C. and Yu, W. and Yang, S. and Yang, X. and McCann, J.},
  month = dec,
  year = {2016},
  keywords = {differential privacy,data privacy,crowdsourcing,expectation-maximisation algorithm,high-dimensional data,Lasso regression,regression analysis,unprecedented privacy threats,Information technology,synth_ldp,aggregation information,aggregation results,Computers,Conferences,crowdsourced systems,Crowdsourced systems,Decision support systems,distribution estimation,EM,expectation maximization based algorithm,high-dimensional crowdsourced data distribution estimation,local privacy,multivariate distribution estimation,multivariate joint distribution,privacy attacks,privacy concerns,privacy-preserving bit strings,Silicon},
  pages = {226-233},
  file = {/Users/mfine/Zotero/storage/2V6R2HKN/Ren et al. - 2016 - High-Dimensional Crowdsourced Data Distribution Es.pdf;/Users/mfine/Zotero/storage/4C8R46PW/7876342.html}
}

@article{RYY+16b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.04350},
  primaryClass = {cs},
  title = {{{LoPub}}: {{High}}-{{Dimensional Crowdsourced Data Publication}} with {{Local Differential Privacy}}},
  shorttitle = {{{LoPub}}},
  abstract = {High-dimensional crowdsourced data collected from a large number of users produces rich knowledge for our society. However, it also brings unprecedented privacy threats to participants. Local privacy, a variant of differential privacy, is proposed as a means to eliminate the privacy concern. Unfortunately, achieving local privacy on high-dimensional crowdsourced data raises great challenges on both efficiency and effectiveness. Here, based on EM and Lasso regression, we propose efficient multi-dimensional joint distribution estimation algorithms with local privacy. Then, we develop a Locally privacy-preserving high-dimensional data Publication algorithm, LoPub, by taking advantage of our distribution estimation techniques. In particular, both correlations and joint distribution among multiple attributes can be identified to reduce the dimension of crowdsourced data, thus achieving both efficiency and effectiveness in locally private high-dimensional data publication. Extensive experiments on real-world datasets demonstrated that the efficiency of our multivariate distribution estimation scheme and confirm the effectiveness of our LoPub scheme in generating approximate datasets with local privacy.},
  journal = {arXiv:1612.04350 [cs]},
  author = {Ren, Xuebin and Yu, Chia-Mu and Yu, Weiren and Yang, Shusen and Yang, Xinyu and McCann, Julie A. and Yu, Philip S.},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Cryptography and Security,synth_ldp},
  file = {/Users/mfine/Zotero/storage/TBXCWAWG/Ren et al. - 2016 - LoPub High-Dimensional Crowdsourced Data Publicat.pdf;/Users/mfine/Zotero/storage/D6IFXJCI/1612.html}
}

@article{HMSa,
  title = {{{MD}}-{{GAN}}: {{Multi}}-{{Discriminator Generative Adversarial Networks}} for {{Distributed Datasets}}},
  abstract = {A recent technical breakthrough in the domain of machine learning is the discovery and the multiple applications of Generative Adversarial Networks (GANs). Those generative models are computationally demanding, as a GAN is composed of two deep neural networks, and because it trains on large datasets. A GAN is generally trained on a single server.},
  language = {en},
  author = {Hardy, Corentin and Merrer, Erwan Le and Sericola, Bruno},
  keywords = {federated},
  pages = {12},
  file = {/Users/mfine/Zotero/storage/C7TM4IYB/Hardy et al. - MD-GAN Multi-Discriminator Generative Adversarial.pdf}
}

@article{HMS,
  title = {{{MD}}-{{GAN}}: {{Multi}}-{{Discriminator Generative Adversarial Networks}} for {{Distributed Datasets}}},
  abstract = {A recent technical breakthrough in the domain of machine learning is the discovery and the multiple applications of Generative Adversarial Networks (GANs). Those generative models are computationally demanding, as a GAN is composed of two deep neural networks, and because it trains on large datasets. A GAN is generally trained on a single server.},
  language = {en},
  author = {Hardy, Corentin and Merrer, Erwan Le and Sericola, Bruno},
  pages = {12},
  file = {/Users/mfine/Zotero/storage/HPJIZN62/Hardy et al. - MD-GAN Multi-Discriminator Generative Adversarial.pdf}
}

@inproceedings{KM11,
  address = {{Athens, Greece}},
  title = {No Free Lunch in Data Privacy},
  isbn = {978-1-4503-0661-4},
  abstract = {Differential privacy is a powerful tool for providing privacypreserving noisy query answers over statistical databases. It guarantees that the distribution of noisy query answers changes very little with the addition or deletion of any tuple. It is frequently accompanied by popularized claims that it provides privacy without any assumptions about the data and that it protects against attackers who know all but one record. In this paper we critically analyze the privacy protections offered by differential privacy.},
  language = {en},
  booktitle = {Proceedings of the 2011 International Conference on {{Management}} of Data - {{SIGMOD}} '11},
  publisher = {{ACM Press}},
  doi = {10.1145/1989323.1989345},
  author = {Kifer, Daniel and Machanavajjhala, Ashwin},
  year = {2011},
  pages = {193},
  file = {/Users/mfine/Zotero/storage/VZHIZRIV/Kifer and Machanavajjhala - 2011 - No free lunch in data privacy.pdf}
}

@article{NXY+16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.05053},
  primaryClass = {cs},
  title = {Collecting and {{Analyzing Data}} from {{Smart Device Users}} with {{Local Differential Privacy}}},
  abstract = {Organizations with a large user base, such as Samsung and Google, can potentially benefit from collecting and mining users' data. However, doing so raises privacy concerns, and risks accidental privacy breaches with serious consequences. Local differential privacy (LDP) techniques address this problem by only collecting randomized answers from each user, with guarantees of plausible deniability; meanwhile, the aggregator can still build accurate models and predictors by analyzing large amounts of such randomized data. So far, existing LDP solutions either have severely restricted functionality, or focus mainly on theoretical aspects such as asymptotical bounds rather than practical usability and performance. Motivated by this, we propose Harmony, a practical, accurate and efficient system for collecting and analyzing data from smart device users, while satisfying LDP. Harmony applies to multi-dimensional data containing both numerical and categorical attributes, and supports both basic statistics (e.g., mean and frequency estimates), and complex machine learning tasks (e.g., linear regression, logistic regression and SVM classification). Experiments using real data confirm Harmony's effectiveness.},
  journal = {arXiv:1606.05053 [cs]},
  author = {Nguy{\^e}n, Th{\^o}ng T. and Xiao, Xiaokui and Yang, Yin and Hui, Siu Cheung and Shin, Hyejin and Shin, Junbum},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Databases,ldp},
  file = {/Users/mfine/Zotero/storage/TTGP97UD/Nguyn et al. - 2016 - Collecting and Analyzing Data from Smart Device Us.pdf;/Users/mfine/Zotero/storage/WK5R8AF9/1606.html}
}

@article{RYY+16a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.04350},
  primaryClass = {cs},
  title = {{{LoPub}}: {{High}}-{{Dimensional Crowdsourced Data Publication}} with {{Local Differential Privacy}}},
  shorttitle = {{{LoPub}}},
  abstract = {High-dimensional crowdsourced data collected from a large number of users produces rich knowledge for our society. However, it also brings unprecedented privacy threats to participants. Local privacy, a variant of differential privacy, is proposed as a means to eliminate the privacy concern. Unfortunately, achieving local privacy on high-dimensional crowdsourced data raises great challenges on both efficiency and effectiveness. Here, based on EM and Lasso regression, we propose efficient multi-dimensional joint distribution estimation algorithms with local privacy. Then, we develop a Locally privacy-preserving high-dimensional data Publication algorithm, LoPub, by taking advantage of our distribution estimation techniques. In particular, both correlations and joint distribution among multiple attributes can be identified to reduce the dimension of crowdsourced data, thus achieving both efficiency and effectiveness in locally private high-dimensional data publication. Extensive experiments on real-world datasets demonstrated that the efficiency of our multivariate distribution estimation scheme and confirm the effectiveness of our LoPub scheme in generating approximate datasets with local privacy.},
  journal = {arXiv:1612.04350 [cs]},
  author = {Ren, Xuebin and Yu, Chia-Mu and Yu, Weiren and Yang, Shusen and Yang, Xinyu and McCann, Julie A. and Yu, Philip S.},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/mfine/Zotero/storage/DITCTG2K/Ren et al. - 2016 - LoPub High-Dimensional Crowdsourced Data Publicat.pdf;/Users/mfine/Zotero/storage/YXH66VQS/1612.html}
}

@inproceedings{DRV10,
  address = {{Las Vegas, NV, USA}},
  title = {Boosting and {{Differential Privacy}}},
  isbn = {978-1-4244-8525-3},
  abstract = {Boosting is a general method for improving the accuracy of learning algorithms. We use boosting to construct improved privacy-preserving synopses of an input database. These are data structures that yield, for a given set Q of queries over an input database, reasonably accurate estimates of the responses to every query in Q, even when the number of queries is much larger than the number of rows in the database. Given a base synopsis generator that takes a distribution on Q and produces a ``weak'' synopsis that yields ``good'' answers for a majority of the weight in Q, our Boosting for Queries algorithm obtains a synopsis that is good for all of Q. We ensure privacy for the rows of the database, but the boosting is performed on the queries. We also provide the first synopsis generators for arbitrary sets of arbitrary lowsensitivity queries, i.e., queries whose answers do not vary much under the addition or deletion of a single row.},
  language = {en},
  booktitle = {2010 {{IEEE}} 51st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  publisher = {{IEEE}},
  doi = {10.1109/FOCS.2010.12},
  author = {Dwork, Cynthia and Rothblum, Guy N. and Vadhan, Salil},
  month = oct,
  year = {2010},
  keywords = {synopsis},
  pages = {51-60},
  file = {/Users/mfine/Zotero/storage/EPCLVP9J/Dwork et al. - 2010 - Boosting and Differential Privacy.pdf}
}

@article{ZMW,
  title = {Collect at {{Once}}, {{Use Effectively}}: {{Making Non}}-Interactive {{Locally Private Learning Possible}}},
  abstract = {Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this paper, we extend the frontiers of Non-interactive LDP learning and estimation from several aspects. For learning with smooth generalized linear losses, we propose an approximate stochastic gradient oracle estimated from non-interactive LDP channel using Chebyshev expansion, which is combined with inexact gradient methods to obtain an efficient algorithm with quasi-polynomial sample complexity bound. For the high-dimensional world, we discover that under 2-norm assumption on data points, high-dimensional sparse linear regression and mean estimation can be achieved with logarithmic dependence on dimension, using random projection and approximate recovery. We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation possible for a broad range of learning tasks under noninteractive LDP model.},
  language = {en},
  author = {Zheng, Kai and Mou, Wenlong and Wang, Liwei},
  pages = {10},
  file = {/Users/mfine/Zotero/storage/XWJ3RFEE/Zheng et al. - Collect at Once, Use Effectively Making Non-inter.pdf}
}

@inproceedings{DNR+09,
  address = {{Bethesda, MD, USA}},
  title = {On the Complexity of Differentially Private Data Release: Efficient Algorithms and Hardness Results},
  isbn = {978-1-60558-506-2},
  shorttitle = {On the Complexity of Differentially Private Data Release},
  abstract = {We consider private data analysis in the setting in which a trusted and trustworthy curator, having obtained a large data set containing private information, releases to the public a ``sanitization'' of the data set that simultaneously protects the privacy of the individual contributors of data and offers utility to the data analyst. The sanitization may be in the form of an arbitrary data structure, accompanied by a computational procedure for determining approximate answers to queries on the original data set, or it may be a ``synthetic data set'' consisting of data items drawn from the same universe as items in the original data set; queries are carried out as if the synthetic data set were the actual input. In either case the process is non-interactive; once the sanitization has been released the original data and the curator play no further role.},
  language = {en},
  booktitle = {Proceedings of the 41st Annual {{ACM}} Symposium on {{Symposium}} on Theory of Computing - {{STOC}} '09},
  publisher = {{ACM Press}},
  doi = {10.1145/1536414.1536467},
  author = {Dwork, Cynthia and Naor, Moni and Reingold, Omer and Rothblum, Guy N. and Vadhan, Salil},
  year = {2009},
  keywords = {synopsis},
  pages = {381},
  file = {/Users/mfine/Zotero/storage/6MHLJ3Q7/Dwork et al. - 2009 - On the complexity of differentially private data r.pdf}
}

@misc{zotero-1002,
  title = {Differential-{{Private Data Publishing Through Component Analysis}}},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3883117/},
  file = {/Users/mfine/Zotero/storage/AV6JR28C/PMC3883117.html}
}

@misc{zotero-1004,
  title = {Toward {{Theoretical Understanding}} of {{Deep Learning}}: {{Slides}} and {{Bibliography}}},
  howpublished = {http://unsupervised.cs.princeton.edu/deeplearningtutorial.html},
  file = {/Users/mfine/Zotero/storage/PI7QMVA2/deeplearningtutorial.html}
}

@misc{zotero-1006,
  title = {{{deepsurveyICML18final}}.Pptx},
  howpublished = {https://www.dropbox.com/s/qonozmne0x4x2r3/deepsurveyICML18final.pptx?dl=0},
  file = {/Users/mfine/Zotero/storage/KLLJQMIT/deepsurveyICML18final.html}
}

@misc{zotero-1008,
  title = {{{IEEE Xplore Full}}-{{Text PDF}}:},
  howpublished = {https://ieeexplore-ieee-org.ezp-prod1.hul.harvard.edu/stamp/stamp.jsp?tp=\&arnumber=8306916},
  file = {/Users/mfine/Zotero/storage/8JJXSFLQ/stamp.html}
}

@article{RYY+16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.04350},
  primaryClass = {cs},
  title = {{{LoPub}}: {{High}}-{{Dimensional Crowdsourced Data Publication}} with {{Local Differential Privacy}}},
  shorttitle = {{{LoPub}}},
  abstract = {High-dimensional crowdsourced data collected from a large number of users produces rich knowledge for our society. However, it also brings unprecedented privacy threats to participants. Local privacy, a variant of differential privacy, is proposed as a means to eliminate the privacy concern. Unfortunately, achieving local privacy on high-dimensional crowdsourced data raises great challenges on both efficiency and effectiveness. Here, based on EM and Lasso regression, we propose efficient multi-dimensional joint distribution estimation algorithms with local privacy. Then, we develop a Locally privacy-preserving high-dimensional data Publication algorithm, LoPub, by taking advantage of our distribution estimation techniques. In particular, both correlations and joint distribution among multiple attributes can be identified to reduce the dimension of crowdsourced data, thus achieving both efficiency and effectiveness in locally private high-dimensional data publication. Extensive experiments on real-world datasets demonstrated that the efficiency of our multivariate distribution estimation scheme and confirm the effectiveness of our LoPub scheme in generating approximate datasets with local privacy.},
  journal = {arXiv:1612.04350 [cs]},
  author = {Ren, Xuebin and Yu, Chia-Mu and Yu, Weiren and Yang, Shusen and Yang, Xinyu and McCann, Julie A. and Yu, Philip S.},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/mfine/Zotero/storage/7NGJHTFC/Ren et al. - 2016 - LoPub High-Dimensional Crowdsourced Data Publicat.pdf;/Users/mfine/Zotero/storage/YXXUFTSJ/1612.html}
}

@inproceedings{ZWL+18,
  address = {{Toronto, Canada}},
  title = {{{CALM}}: {{Consistent Adaptive Local Marginal}} for {{Marginal Release}} under {{Local Differential Privacy}}},
  isbn = {978-1-4503-5693-0},
  shorttitle = {{{CALM}}},
  abstract = {Marginal tables are the workhorse of capturing the correlations among a set of attributes. We consider the problem of constructing marginal tables given a set of user's multi-dimensional data while satisfying Local Differential Privacy (LDP), a privacy notion that protects individual user's privacy without relying on a trusted third party. Existing works on this problem perform poorly in the high-dimensional setting; even worse, some incur very expensive computational overhead. In this paper, we propose CALM, Consistent Adaptive Local Marginal, that takes advantage of the careful challenge analysis and performs consistently better than existing methods. More importantly, CALM can scale well with large data dimensions and marginal sizes. We conduct extensive experiments on several real world datasets. Experimental results demonstrate the effectiveness and efficiency of CALM over existing methods.},
  language = {en},
  booktitle = {Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}  - {{CCS}} '18},
  publisher = {{ACM Press}},
  doi = {10.1145/3243734.3243742},
  author = {Zhang, Zhikun and Wang, Tianhao and Li, Ninghui and He, Shibo and Chen, Jiming},
  year = {2018},
  pages = {212-229},
  file = {/Users/mfine/Zotero/storage/NRV7NGBV/Zhang et al. - 2018 - CALM Consistent Adaptive Local Marginal for Margi.pdf}
}

@inproceedings{BRB+17,
  address = {{Scottsdale, Arizona, USA}},
  title = {Achieving {{Differential Privacy}} in {{Secure Multiparty Data Aggregation Protocols}} on {{Star Networks}}},
  isbn = {978-1-4503-4523-1},
  abstract = {We consider the problem of privacy-preserving data aggregation in a star network topology, i.e., several untrusting participants connected to a single aggregator. We require that the participants do not discover each other's data, and the service provider remains oblivious to each participant's individual contribution. Furthermore, the final result is to be published in a differentially private manner, i.e., the result should not reveal the contribution of any single participant to a (possibly external) adversary who knows the contributions of all other participants. In other words, we require a secure multiparty computation protocol that also incorporates a differentially private mechanism.},
  language = {en},
  booktitle = {Proceedings of the {{Seventh ACM}} on {{Conference}} on {{Data}} and {{Application Security}} and {{Privacy}} - {{CODASPY}} '17},
  publisher = {{ACM Press}},
  doi = {10.1145/3029806.3029829},
  author = {Bindschaedler, Vincent and Rane, Shantanu and Brito, Alejandro E. and Rao, Vanishree and Uzun, Ersin},
  year = {2017},
  pages = {115-125},
  file = {/Users/mfine/Zotero/storage/NHMI3ZTM/Bindschaedler et al. - 2017 - Achieving Differential Privacy in Secure Multipart.pdf}
}

@article{HMS17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1702.04683},
  primaryClass = {cs},
  title = {Distributed Deep Learning on Edge-Devices: Feasibility via Adaptive Compression},
  shorttitle = {Distributed Deep Learning on Edge-Devices},
  abstract = {A large portion of data mining and analytic services use modern machine learning techniques, such as deep learning. The state-of-the-art results by deep learning come at the price of an intensive use of computing resources. The leading frameworks (e.g., TensorFlow) are executed on GPUs or on high-end servers in datacenters. On the other end, there is a proliferation of personal devices with possibly free CPU cycles; this can enable services to run in users' homes, embedding machine learning operations. In this paper, we ask the following question: Is distributed deep learning computation on WAN connected devices feasible, in spite of the traffic caused by learning tasks? We show that such a setup rises some important challenges, most notably the ingress traffic that the servers hosting the up-to-date model have to sustain. In order to reduce this stress, we propose adaComp, a novel algorithm for compressing worker updates to the model on the server. Applicable to stochastic gradient descent based approaches, it combines efficient gradient selection and learning rate modulation. We then experiment and measure the impact of compression, device heterogeneity and reliability on the accuracy of learned models, with an emulator platform that embeds TensorFlow into Linux containers. We report a reduction of the total amount of data sent by workers to the server by two order of magnitude (e.g., 191-fold reduction for a convolutional network on the MNIST dataset), when compared to a standard asynchronous stochastic gradient descent, while preserving model accuracy.},
  journal = {arXiv:1702.04683 [cs]},
  author = {Hardy, Corentin and Merrer, Erwan Le and Sericola, Bruno},
  month = feb,
  year = {2017},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/842ZF4UI/Hardy et al. - 2017 - Distributed deep learning on edge-devices feasibi.pdf;/Users/mfine/Zotero/storage/KYB9IBPP/1702.html}
}

@inproceedings{SS15,
  address = {{Denver, Colorado, USA}},
  title = {Privacy-{{Preserving Deep Learning}}},
  isbn = {978-1-4503-3832-5},
  abstract = {Deep learning based on artificial neural networks is a very popular approach to modeling, classifying, and recognizing complex data such as images, speech, and text. The unprecedented accuracy of deep learning methods has turned them into the foundation of new AI-based services on the Internet. Commercial companies that collect user data on a large scale have been the main beneficiaries of this trend since the success of deep learning techniques is directly proportional to the amount of data available for training.},
  language = {en},
  booktitle = {Proceedings of the 22nd {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}} - {{CCS}} '15},
  publisher = {{ACM Press}},
  doi = {10.1145/2810103.2813687},
  author = {Shokri, Reza and Shmatikov, Vitaly},
  year = {2015},
  pages = {1310-1321},
  file = {/Users/mfine/Zotero/storage/VQZZEHPG/Shokri and Shmatikov - 2015 - Privacy-Preserving Deep Learning.pdf}
}

@inproceedings{HAP17,
  address = {{Dallas, Texas, USA}},
  title = {Deep {{Models Under}} the {{GAN}}: {{Information Leakage}} from {{Collaborative Deep Learning}}},
  isbn = {978-1-4503-4946-8},
  shorttitle = {Deep {{Models Under}} the {{GAN}}},
  abstract = {Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases.},
  language = {en},
  booktitle = {Proceedings of the 2017 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}  - {{CCS}} '17},
  publisher = {{ACM Press}},
  doi = {10.1145/3133956.3134012},
  author = {Hitaj, Briland and Ateniese, Giuseppe and {Perez-Cruz}, Fernando},
  year = {2017},
  pages = {603-618},
  file = {/Users/mfine/Zotero/storage/4BGU8ZMD/Hitaj et al. - 2017 - Deep Models Under the GAN Information Leakage fro.pdf}
}

@techreport{EKM+14,
  title = {Differentially {{Private Data Aggregation}} with {{Optimal Utility}}},
  abstract = {Computing aggregate statistics about user data is of vital importance for a variety of services and systems, but this practice has been shown to seriously undermine the privacy of users. Differential privacy has proved to be an effective tool to sanitize queries over a database, and various cryptographic protocols have been recently proposed to enforce differential privacy in a distributed setting, e.g., statical queries on sensitive data stored on the user's side. The widespread deployment of differential privacy techniques in real-life settings is, however, undermined by several limitations that existing constructions suffer from: they support only a limited class of queries, they pose a trade-off between privacy and utility of the query result, they are affected by the answer pollution problem, or they are inefficient. This paper presents PrivaDA, a novel design architecture for distributed differential privacy that leverages recent advances in SMPCs on fixed and floating point arithmetics to overcome the previously mentioned limitations. In particular, PrivaDA supports a variety of perturbation mechanisms (e.g., the Laplace, discrete Laplace, and exponential mechanisms) and it constitutes the first generic technique to generate noise in a fully distributed manner while maintaining the optimal utility. Furthermore, PrivaDA does not suffer from the answer pollution problem. We demonstrate the efficiency of PrivaDA with a performance evaluation, and its expressiveness and flexibility by illustrating a variety of application scenarios such as privacy-preserving web analytics.},
  number = {482},
  author = {Eigner, Fabienne and Kate, Aniket and Maffei, Matteo and Pampaloni, Francesca and Pryvalov, Ivan},
  year = {2014},
  keywords = {applications,Data Aggregation,Distributed Differential Privacy,Secure Multiparty Computation (SMPC)},
  file = {/Users/mfine/Zotero/storage/MAGFI2DU/Eigner et al. - 2014 - Differentially Private Data Aggregation with Optim.pdf;/Users/mfine/Zotero/storage/8RN546C5/482.html}
}

@misc{zotero-1037,
  title = {My.Harvard},
  howpublished = {https://courses.my.harvard.edu/psp/courses/EMPLOYEE/EMPL/h/?tab=HU\_CLASS\_SEARCH\&SearchReqJSON=\%7B\%22PageNumber\%22\%3A1\%2C\%22PageSize\%22\%3A\%22\%22\%2C\%22SortOrder\%22\%3A\%5B\%22SCORE\%22\%5D\%2C\%22Facets\%22\%3A\%5B\%5D\%2C\%22Category\%22\%3A\%22HU\_SCL\_SCHEDULED\_BRACKETED\_COURSES\%22\%2C\%22SearchPropertiesInResults\%22\%3Atrue\%2C\%22FacetsInResults\%22\%3Atrue\%2C\%22SaveRecent\%22\%3Atrue\%2C\%22TopN\%22\%3A\%22\%22\%2C\%22SearchText\%22\%3A\%22CS\%20281\%22\%7D},
  file = {/Users/mfine/Zotero/storage/R4AKRX2V/h.html}
}

@article{JGP17,
  title = {{{CATEGORICAL REPARAMETERIZATION WITH GUMBEL}}-{{SOFTMAX}}},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  language = {en},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  pages = {13},
  file = {/Users/mfine/Zotero/storage/T7L2KMUP/Jang et al. - 2017 - CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX.pdf}
}

@article{YAE+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.02903},
  primaryClass = {cs, stat},
  title = {Applied {{Federated Learning}}: {{Improving Google Keyboard Query Suggestions}}},
  shorttitle = {Applied {{Federated Learning}}},
  abstract = {Federated learning is a distributed form of machine learning where both the training data and model training are decentralized. In this paper, we use federated learning in a commercial, global-scale setting to train, evaluate and deploy a model to improve virtual keyboard search suggestion quality without direct access to the underlying user data. We describe our observations in federated training, compare metrics to live deployments, and present resulting quality increases. In whole, we demonstrate how federated learning can be applied end-to-end to both improve user experiences and enhance user privacy.},
  journal = {arXiv:1812.02903 [cs, stat]},
  author = {Yang, Timothy and Andrew, Galen and Eichner, Hubert and Sun, Haicheng and Li, Wei and Kong, Nicholas and Ramage, Daniel and Beaufays, Fran{\c c}oise},
  month = dec,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/36L9JT63/Yang et al. - 2018 - Applied Federated Learning Improving Google Keybo.pdf;/Users/mfine/Zotero/storage/NUA9CLGF/1812.html}
}

@article{MMR+16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.05629},
  primaryClass = {cs},
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  journal = {arXiv:1602.05629 [cs]},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/VI7X9882/McMahan et al. - 2016 - Communication-Efficient Learning of Deep Networks .pdf;/Users/mfine/Zotero/storage/RWVG53H5/1602.html}
}

@article{MMRA16,
  title = {Federated {{Learning}} of {{Deep Networks}} Using {{Model Averaging}}},
  volume = {abs/1602.05629},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks that proves robust to the unbalanced and non-IID data distributions that naturally arise. This method allows high-quality models to be trained in relatively few rounds of communication, the principal constraint for federated learning. The key insight is that despite the non-convex loss functions we optimize, parameter averaging over updates from multiple clients produces surprisingly good results, for example decreasing the communication needed to train an LSTM language model by two orders of magnitude.},
  journal = {ArXiv},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and y Arcas, Blaise Ag{\"u}era},
  year = {2016},
  keywords = {Experiment,Privacy,Algorithm,Cobham's thesis,Data center,Dropout (neural networks),Language model,Long short-term memory,Loss function,Mathematical optimization,Mobile device,Speech recognition,Stochastic gradient descent,Unbalanced circuit,User experience,Word lists by frequency},
  file = {/Users/mfine/Zotero/storage/NMMR3J3D/McMahan et al. - 2016 - Federated Learning of Deep Networks using Model Av.pdf}
}

@article{CBM+17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.06490},
  primaryClass = {cs},
  title = {Generating {{Multi}}-Label {{Discrete Patient Records}} Using {{Generative Adversarial Networks}}},
  abstract = {Access to electronic health record (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic patient records. Based on input real patient records, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic patient records that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and a medical expert review. We also empirically observe a limited privacy risk in both identity and attribute disclosure using medGAN.},
  journal = {arXiv:1703.06490 [cs]},
  author = {Choi, Edward and Biswal, Siddharth and Malin, Bradley and Duke, Jon and Stewart, Walter F. and Sun, Jimeng},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/mfine/Zotero/storage/M9SAKZHJ/Choi et al. - 2017 - Generating Multi-label Discrete Patient Records us.pdf;/Users/mfine/Zotero/storage/WXIYZ82R/1703.html}
}

@article{AO14,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.1537},
  primaryClass = {cs, math, stat},
  title = {Linear {{Coupling}}: {{An Ultimate Unification}} of {{Gradient}} and {{Mirror Descent}}},
  shorttitle = {Linear {{Coupling}}},
  abstract = {First-order methods play a central role in large-scale machine learning. Even though many variations exist, each suited to a particular problem, almost all such methods fundamentally rely on two types of algorithmic steps: gradient descent, which yields primal progress, and mirror descent, which yields dual progress. We observe that the performances of gradient and mirror descent are complementary, so that faster algorithms can be designed by LINEARLY COUPLING the two. We show how to reconstruct Nesterov's accelerated gradient methods using linear coupling, which gives a cleaner interpretation than Nesterov's original proofs. We also discuss the power of linear coupling by extending it to many other settings that Nesterov's methods cannot apply to.},
  journal = {arXiv:1407.1537 [cs, math, stat]},
  author = {{Allen-Zhu}, Zeyuan and Orecchia, Lorenzo},
  month = jul,
  year = {2014},
  keywords = {Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms,Statistics - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/Users/mfine/Zotero/storage/8Z3XBH2V/Allen-Zhu and Orecchia - 2014 - Linear Coupling An Ultimate Unification of Gradie.pdf;/Users/mfine/Zotero/storage/EULH4S2D/1407.html}
}

@misc{Pok19,
  title = {Cheat {{Sheet}}: {{Subgradient Descent}}, {{Mirror Descent}}, and {{Online Learning}}},
  shorttitle = {Cheat {{Sheet}}},
  abstract = {TL;DR: Cheat Sheet for non-smooth convex optimization: subgradient descent, mirror descent, and online learning. Long and technical.},
  language = {en},
  journal = {One trivial observation at a time},
  howpublished = {http://www.pokutta.com/blog/research/2019/02/27/cheatsheet-nonsmooth.html},
  author = {Pokutta, Sebastian},
  month = feb,
  year = {2019},
  file = {/Users/mfine/Zotero/storage/6QUTQMGB/cheatsheet-nonsmooth.html}
}

@incollection{JT12a,
  address = {{Berlin, Heidelberg}},
  title = {Mirror {{Descent Based Database Privacy}}},
  volume = {7408},
  isbn = {978-3-642-32511-3 978-3-642-32512-0},
  abstract = {In this paper, we focus on the problem of private database release in the interactive setting: a trusted database curator receives queries in an online manner for which it needs to respond with accurate but privacy preserving answers. To this end, we generalize the IDC (Iterative Database Construction) framework of [15,13] that maintains a differentially private artificial dataset and answers incoming linear queries using the artificial dataset. In particular, we formulate a generic IDC framework based on the Mirror Descent algorithm, a popular convex optimization algorithm [1]. We then present two concrete applications, namely, cut queries over a bipartite graph and linear queries over lowrank matrices, and provide significantly tighter error bounds than the ones by [15,13].},
  language = {en},
  booktitle = {Approximation, {{Randomization}}, and {{Combinatorial Optimization}}. {{Algorithms}} and {{Techniques}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jain, Prateek and Thakurta, Abhradeep},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gupta, Anupam and Jansen, Klaus and Rolim, Jos{\'e} and Servedio, Rocco},
  year = {2012},
  pages = {579-590},
  file = {/Users/mfine/Zotero/storage/BRESUDES/Jain and Thakurta - 2012 - Mirror Descent Based Database Privacy.pdf},
  doi = {10.1007/978-3-642-32512-0_49}
}

@article{CTUW13,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1304.3754},
  primaryClass = {cs},
  title = {Faster {{Private Release}} of {{Marginals}} on {{Small Databases}}},
  abstract = {We study the problem of answering \textbackslash{}emph\{\$k\$-way marginal\} queries on a database \$D \textbackslash{}in (\textbackslash\{0,1\textbackslash\}\^d)\^n\$, while preserving differential privacy. The answer to a \$k\$-way marginal query is the fraction of the database's records \$x \textbackslash{}in \textbackslash\{0,1\textbackslash\}\^d\$ with a given value in each of a given set of up to \$k\$ columns. Marginal queries enable a rich class of statistical analyses on a dataset, and designing efficient algorithms for privately answering marginal queries has been identified as an important open problem in private data analysis. For any \$k\$, we give a differentially private online algorithm that runs in time \$\$ \textbackslash{}min\{\textbackslash{}exp(d\^\{1-\textbackslash{}Omega(1/\textbackslash{}sqrt\{k\})\}), \textbackslash{}exp(d / \textbackslash{}log\^\{.99\} d)\textbackslash\} \$\$ per query and answers any (possibly superpolynomially long and adaptively chosen) sequence of \$k\$-way marginal queries up to error at most \$\textbackslash{}pm .01\$ on every query, provided \$n \textbackslash{}gtrsim d\^\{.51\} \$. To the best of our knowledge, this is the first algorithm capable of privately answering marginal queries with a non-trivial worst-case accuracy guarantee on a database of size \$\textbackslash{}poly(d, k)\$ in time \$\textbackslash{}exp(o(d))\$. Our algorithms are a variant of the private multiplicative weights algorithm (Hardt and Rothblum, FOCS '10), but using a different low-weight representation of the database. We derive our low-weight representation using approximations to the OR function by low-degree polynomials with coefficients of bounded \$L\_1\$-norm. We also prove a strong limitation on our approach that is of independent approximation-theoretic interest. Specifically, we show that for any \$k = o(\textbackslash{}log d)\$, any polynomial with coefficients of \$L\_1\$-norm \$poly(d)\$ that pointwise approximates the \$d\$-variate OR function on all inputs of Hamming weight at most \$k\$ must have degree \$d\^\{1-O(1/\textbackslash{}sqrt\{k\})\}\$.},
  journal = {arXiv:1304.3754 [cs]},
  author = {Chandrasekaran, Karthekeyan and Thaler, Justin and Ullman, Jonathan and Wan, Andrew},
  month = apr,
  year = {2013},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/CRD9ZRPU/Chandrasekaran et al. - 2013 - Faster Private Release of Marginals on Small Datab.pdf;/Users/mfine/Zotero/storage/63HSSL55/1304.html}
}

@article{HYF+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.12629},
  primaryClass = {cs, stat},
  title = {{{LoAdaBoost}}:{{Loss}}-{{Based AdaBoost Federated Machine Learning}} on Medical {{Data}}},
  shorttitle = {{{LoAdaBoost}}},
  abstract = {Medical data are valuable for improvement of health care, policy making and many other purposes. Vast amount of medical data are stored in different locations, on many different devices and in different data silos. Sharing medical data among different sources is a big challenge due to regulatory, operational and security reasons. One potential solution is federated machine learning ,which is a method that sends machine learning algorithms simultaneously to all data sources, train models in each source and aggregates the learned models. This strategy allows utilization of valuable data without moving them.One challenge in applying federated machine learning is the heterogeneity of data from different sources. To tackle this problem, we proposed an adaptive boosting method that increases the efficiency of federated machine learning. Using intensive care unit data from hospital, we showed that LoAdaBoost federated learning outperformed baseline method and increased communication efficiency at negligible additional cost.},
  journal = {arXiv:1811.12629 [cs, stat]},
  author = {Huang, Li and Yin, Yifeng and Fu, Zeng and Zhang, Shifa and Deng, Hao and Liu, Dianbo},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/22T98UTJ/Huang et al. - 2018 - LoAdaBoostLoss-Based AdaBoost Federated Machine L.pdf;/Users/mfine/Zotero/storage/XS9KNA7P/1811.html}
}

@article{AS18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.08071},
  title = {Autonomous {{Agents Modelling Other Agents}}: {{A Comprehensive Survey}} and {{Open Problems}}},
  volume = {258},
  issn = {00043702},
  shorttitle = {Autonomous {{Agents Modelling Other Agents}}},
  abstract = {Much research in artificial intelligence is concerned with the development of autonomous agents that can interact effectively with other agents. An important aspect of such agents is the ability to reason about the behaviours of other agents, by constructing models which make predictions about various properties of interest (such as actions, goals, beliefs) of the modelled agents. A variety of modelling approaches now exist which vary widely in their methodology and underlying assumptions, catering to the needs of the different sub-communities within which they were developed and reflecting the different practical uses for which they are intended. The purpose of the present article is to provide a comprehensive survey of the salient modelling methods which can be found in the literature. The article concludes with a discussion of open problems which may form the basis for fruitful future research.},
  journal = {Artificial Intelligence},
  doi = {10.1016/j.artint.2018.01.002},
  author = {Albrecht, Stefano V. and Stone, Peter},
  month = may,
  year = {2018},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,I.2.11},
  pages = {66-95},
  file = {/Users/mfine/Zotero/storage/XT9S7ZS4/Albrecht and Stone - 2018 - Autonomous Agents Modelling Other Agents A Compre.pdf;/Users/mfine/Zotero/storage/ZM83TWMQ/1709.html}
}

@article{FBB+18,
  title = {Science of Science},
  volume = {359},
  issn = {0036-8075, 1095-9203},
  language = {en},
  number = {6379},
  journal = {Science},
  doi = {10.1126/science.aao0185},
  author = {Fortunato, Santo and Bergstrom, Carl T. and B{\"o}rner, Katy and Evans, James A. and Helbing, Dirk and Milojevi{\'c}, Sta{\v s}a and Petersen, Alexander M. and Radicchi, Filippo and Sinatra, Roberta and Uzzi, Brian and Vespignani, Alessandro and Waltman, Ludo and Wang, Dashun and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  month = mar,
  year = {2018},
  pages = {eaao0185},
  file = {/Users/mfine/Zotero/storage/93G4U58V/Fortunato et al. - 2018 - Science of science.pdf}
}

@inproceedings{HBBA18,
  address = {{Lake Buena Vista, FL, USA}},
  title = {Deep Learning Type Inference},
  isbn = {978-1-4503-5573-5},
  abstract = {Dynamically typed languages such as JavaScript and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like TypeScript offer a middle-ground for JavaScript: a strict superset of JavaScript, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like JavaScript as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose DeepTyper, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. DeepTyper, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95\% precision that could not be inferred without the aid of DeepTyper.},
  language = {en},
  booktitle = {Proceedings of the 2018 26th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}  - {{ESEC}}/{{FSE}} 2018},
  publisher = {{ACM Press}},
  doi = {10.1145/3236024.3236051},
  author = {Hellendoorn, Vincent J. and Bird, Christian and Barr, Earl T. and Allamanis, Miltiadis},
  year = {2018},
  pages = {152-162},
  file = {/Users/mfine/Zotero/storage/R2GBNIMI/Hellendoorn et al. - 2018 - Deep learning type inference.pdf}
}

@article{BJY19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.11313},
  primaryClass = {cs},
  title = {{{AnomiGAN}}: {{Generative}} Adversarial Networks for Anonymizing Private Medical Data},
  shorttitle = {{{AnomiGAN}}},
  abstract = {Typical personal medical data contains sensitive information about individuals. Storing or sharing the personal medical data is thus often risky. For example, a short DNA sequence can provide information that can not only identify an individual, but also his or her relatives. Nonetheless, most countries and researchers agree on the necessity of collecting personal medical data. This stems from the fact that medical data, including genomic data, are an indispensable resource for further research and development regarding disease prevention and treatment. To prevent personal medical data from being misused, techniques to reliably preserve sensitive information should be developed for real world application. In this paper, we propose a framework called anonymized generative adversarial networks (AnomiGAN), to improve the maintenance of privacy of personal medical data, while also maintaining high prediction performance. We compared our method to state-of-the-art techniques and observed that our method preserves the same level of privacy as differential privacy (DP), but had better prediction results. We also observed that there is a trade-off between privacy and performance results depending on the degree of preservation of the original data. Here, we provide a mathematical overview of our proposed model and demonstrate its validation using UCI machine learning repository datasets in order to highlight its utility in practice. Experimentally, our approach delivers a better performance compared to that of the DP approach.},
  journal = {arXiv:1901.11313 [cs]},
  author = {Bae, Ho and Jung, Dahuin and Yoon, Sungroh},
  month = jan,
  year = {2019},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/HV2TRGCI/Bae et al. - 2019 - AnomiGAN Generative adversarial networks for anon.pdf;/Users/mfine/Zotero/storage/XUTZW372/1901.html}
}

@article{XSCV19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1907.00503},
  primaryClass = {cs, stat},
  title = {Modeling {{Tabular}} Data Using {{Conditional GAN}}},
  abstract = {Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difficult. Existing statistical and deep neural network models fail to properly model this type of data. We design TGAN, which uses a conditional generative adversarial network to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. TGAN outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.},
  journal = {arXiv:1907.00503 [cs, stat]},
  author = {Xu, Lei and Skoularidou, Maria and {Cuesta-Infante}, Alfredo and Veeramachaneni, Kalyan},
  month = jun,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/XRFNKD38/Xu et al. - 2019 - Modeling Tabular data using Conditional GAN.pdf;/Users/mfine/Zotero/storage/AHL3CCIL/1907.html}
}

@article{Ode19,
  title = {Open {{Questions}} about {{Generative Adversarial Networks}}},
  volume = {4},
  issn = {2476-0757},
  abstract = {What we'd like to find out about GANs that we don't know yet.},
  language = {en},
  number = {4},
  journal = {Distill},
  doi = {10.23915/distill.00018},
  author = {Odena, Augustus},
  month = apr,
  year = {2019},
  pages = {e18},
  file = {/Users/mfine/Zotero/storage/IQRUSD6P/gan-open-problems.html}
}

@article{HNW,
  title = {Adversarial {{Networks}} and {{Autoencoders}}: {{The Primal}}-{{Dual Relationship}} and {{Generalization Bounds}}},
  abstract = {Since the introduction of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAE), the literature on generative modelling has witnessed an overwhelming resurgence. The impressive, yet elusive empirical performance of GANs has lead to the rise of many GAN-VAE hybrids, with the hopes of GAN level performance and additional benefits of VAE, such as an encoder for feature reduction, which is not offered by GANs. Recently, the Wasserstein Autoencoder (WAE) was proposed, achieving performance similar to that of GANs, yet it is still unclear whether the two are fundamentally different or can be further improved into a unified model. In this work, we study the f -GAN and WAE models and make two main discoveries. First, we find that the f -GAN and WAE objectives partake in a primal-dual relationship and are equivalent under some assumptions, which then allows us to explicate the success of WAE. Second, the equivalence result allows us to, for the first time, prove generalization bounds for Autoencoder models, which is a pertinent problem when it comes to theoretical analyses of generative models. Furthermore, we show that the WAE objective is related to other statistical quantities such as the f -divergence and in particular, upper bounded by the Wasserstein distance, which then allows us to tap into existing efficient (regularized) optimal transport solvers. Our findings thus present the first primal-dual relationship between GANs and Autoencoder models, comment on generalization abilities and make a step towards unifying these models.},
  language = {en},
  author = {Husain, Hisham and Nock, Richard and Williamson, Robert C},
  pages = {26},
  file = {/Users/mfine/Zotero/storage/7FU4KDUF/Husain et al. - Adversarial Networks and Autoencoders The Primal-.pdf}
}

@article{ML,
  title = {Learning in {{Implicit Generative Models}}},
  abstract = {Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning\textemdash{}to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models\textemdash{}models that only specify a stochastic procedure with which to generate data\textemdash{}and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density-ratio and density-difference estimation. There are four approaches for density comparison, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.},
  language = {en},
  author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
  pages = {10},
  file = {/Users/mfine/Zotero/storage/CDGPRPJB/Mohamed and Lakshminarayanan - Learning in Implicit Generative Models.pdf}
}

@article{GRU11,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1107.3731},
  primaryClass = {cs},
  title = {Iterative {{Constructions}} and {{Private Data Release}}},
  abstract = {In this paper we study the problem of approximately releasing the cut function of a graph while preserving differential privacy, and give new algorithms (and new analyses of existing algorithms) in both the interactive and non-interactive settings. Our algorithms in the interactive setting are achieved by revisiting the problem of releasing differentially private, approximate answers to a large number of queries on a database. We show that several algorithms for this problem fall into the same basic framework, and are based on the existence of objects which we call iterative database construction algorithms. We give a new generic framework in which new (efficient) IDC algorithms give rise to new (efficient) interactive private query release mechanisms. Our modular analysis simplifies and tightens the analysis of previous algorithms, leading to improved bounds. We then give a new IDC algorithm (and therefore a new private, interactive query release mechanism) based on the Frieze/Kannan low-rank matrix decomposition. This new release mechanism gives an improvement on prior work in a range of parameters where the size of the database is comparable to the size of the data universe (such as releasing all cut queries on dense graphs). We also give a non-interactive algorithm for efficiently releasing private synthetic data for graph cuts with error O(|V|\^\{1.5\}). Our algorithm is based on randomized response and a non-private implementation of the SDP-based, constant-factor approximation algorithm for cut-norm due to Alon and Naor. Finally, we give a reduction based on the IDC framework showing that an efficient, private algorithm for computing sufficiently accurate rank-1 matrix approximations would lead to an improved efficient algorithm for releasing private synthetic data for graph cuts. We leave finding such an algorithm as our main open problem.},
  journal = {arXiv:1107.3731 [cs]},
  author = {Gupta, Anupam and Roth, Aaron and Ullman, Jonathan},
  month = jul,
  year = {2011},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/T8HE7SS3/Gupta et al. - 2011 - Iterative Constructions and Private Data Release.pdf;/Users/mfine/Zotero/storage/U6HLQ33P/1107.html}
}

@article{LCC+17a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08584},
  primaryClass = {cs, stat},
  title = {{{MMD GAN}}: {{Towards Deeper Understanding}} of {{Moment Matching Network}}},
  shorttitle = {{{MMD GAN}}},
  abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
  journal = {arXiv:1705.08584 [cs, stat]},
  author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and P{\'o}czos, Barnab{\'a}s},
  month = may,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/U7957NBY/Li et al. - 2017 - MMD GAN Towards Deeper Understanding of Moment Ma.pdf;/Users/mfine/Zotero/storage/5DXV4Y3V/1705.html}
}

@article{LCC+17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08584},
  primaryClass = {cs, stat},
  title = {{{MMD GAN}}: {{Towards Deeper Understanding}} of {{Moment Matching Network}}},
  shorttitle = {{{MMD GAN}}},
  abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
  journal = {arXiv:1705.08584 [cs, stat]},
  author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and P{\'o}czos, Barnab{\'a}s},
  month = may,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/3LHA7XTK/Li et al. - 2017 - MMD GAN Towards Deeper Understanding of Moment Ma.pdf;/Users/mfine/Zotero/storage/Q7LNJXLL/1705.html}
}

@article{LSZ15a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.02761},
  primaryClass = {cs, stat},
  title = {Generative {{Moment Matching Networks}}},
  abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
  journal = {arXiv:1502.02761 [cs, stat]},
  author = {Li, Yujia and Swersky, Kevin and Zemel, Richard},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/2PP4EBQX/Li et al. - 2015 - Generative Moment Matching Networks.pdf;/Users/mfine/Zotero/storage/6H784BTS/1502.html}
}

@article{JT,
  title = {Discrete {{Reproducing Kernel Hilbert Spaces}}: {{Sampling}} and {{Distribution}} of {{Dirac}}-Masses},
  abstract = {We study reproducing kernels, and associated reproducing kernel Hilbert spaces (RKHSs) H over infinite, discrete and countable sets V . In this setting we analyze in detail the distributions of the corresponding Dirac point-masses of V . Illustrations include certain models from neural networks: An Extreme Learning Machine (ELM) is a neural networkconfiguration in which a hidden layer of weights are randomly sampled, and where the object is then to compute resulting output. For RKHSs H of functions defined on a prescribed countable infinite discrete set V , we characterize those which contain the Dirac masses {$\delta$}x for all points x in V . Further examples and applications where this question plays an important role are: (i) discrete Brownian motion-Hilbert spaces, i.e., discrete versions of the Cameron-Martin Hilbert space; (ii) energy-Hilbert spaces corresponding to graph-Laplacians where the set V of vertices is then equipped with a resistance metric; and finally (iii) the study of Gaussian free fields.},
  language = {en},
  author = {Jorgensen, Palle and Tian, Feng},
  pages = {36},
  file = {/Users/mfine/Zotero/storage/LZ6DRIVQ/Jorgensen and Tian - Discrete Reproducing Kernel Hilbert Spaces Sampli.pdf}
}

@article{DRG15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.03906},
  primaryClass = {cs, stat},
  title = {Training Generative Neural Networks via {{Maximum Mean Discrepancy}} Optimization},
  abstract = {We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD.},
  journal = {arXiv:1505.03906 [cs, stat]},
  author = {Dziugaite, Gintare Karolina and Roy, Daniel M. and Ghahramani, Zoubin},
  month = may,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/JSW5DT9K/Dziugaite et al. - 2015 - Training generative neural networks via Maximum Me.pdf;/Users/mfine/Zotero/storage/4I6HQ3S2/1505.html}
}

@article{DeD,
  title = {Information {{Theory}} for {{Intelligent People}}},
  language = {en},
  author = {DeDeo, Simon},
  pages = {15},
  file = {/Users/mfine/Zotero/storage/6KWQZNZA/DeDeo - Information Theory for Intelligent People.pdf}
}

@incollection{JT12,
  address = {{Berlin, Heidelberg}},
  title = {Mirror {{Descent Based Database Privacy}}},
  volume = {7408},
  isbn = {978-3-642-32511-3 978-3-642-32512-0},
  abstract = {In this paper, we focus on the problem of private database release in the interactive setting: a trusted database curator receives queries in an online manner for which it needs to respond with accurate but privacy preserving answers. To this end, we generalize the IDC (Iterative Database Construction) framework of [15,13] that maintains a differentially private artificial dataset and answers incoming linear queries using the artificial dataset. In particular, we formulate a generic IDC framework based on the Mirror Descent algorithm, a popular convex optimization algorithm [1]. We then present two concrete applications, namely, cut queries over a bipartite graph and linear queries over lowrank matrices, and provide significantly tighter error bounds than the ones by [15,13].},
  language = {en},
  booktitle = {Approximation, {{Randomization}}, and {{Combinatorial Optimization}}. {{Algorithms}} and {{Techniques}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jain, Prateek and Thakurta, Abhradeep},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gupta, Anupam and Jansen, Klaus and Rolim, Jos{\'e} and Servedio, Rocco},
  year = {2012},
  pages = {579-590},
  file = {/Users/mfine/Zotero/storage/XN55YR7R/Jain and Thakurta - 2012 - Mirror Descent Based Database Privacy.pdf},
  doi = {10.1007/978-3-642-32512-0_49}
}

@article{Mit98,
  title = {{{MythBusters}}: {{A Deep Learning Edition}}},
  language = {en},
  author = {Mit, Sasha Rakhlin},
  year = {1998},
  pages = {17},
  file = {/Users/mfine/Zotero/storage/G7EFSI4P/Mit - 1998 - MythBusters A Deep Learning Edition.pdf}
}

@article{Lia18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.03179},
  primaryClass = {cs, math, stat},
  title = {On {{How Well Generative Adversarial Networks Learn Densities}}: {{Nonparametric}} and {{Parametric Results}}},
  shorttitle = {On {{How Well Generative Adversarial Networks Learn Densities}}},
  abstract = {We study in this paper the rate of convergence for learning distributions with the adversarial framework and Generative Adversarial Networks (GANs), which subsumes Wasserstein, Sobolev and MMD GANs as special cases. We study a wide range of parametric and nonparametric target distributions, under a collection of objective evaluation metrics. On the nonparametric end, we investigate the minimax optimal rates and fundamental difficulty of the density estimation under the adversarial framework. On the parametric end, we establish a theory for general neural network classes (including deep leaky ReLU as a special case), that characterizes the interplay on the choice of generator and discriminator. We investigate how to obtain a good statistical guarantee for GANs through the lens of regularization. We discover and isolate a new notion of regularization, called the \textbackslash{}textit\{generator/discriminator pair regularization\}, that sheds light on the advantage of GANs compared to classical parametric and nonparametric approaches for density estimation. We develop novel oracle inequalities as the main tools for analyzing GANs, which is of independent theoretical interest.},
  journal = {arXiv:1811.03179 [cs, math, stat]},
  author = {Liang, Tengyuan},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Statistics Theory},
  file = {/Users/mfine/Zotero/storage/STELU6KZ/Liang - 2018 - On How Well Generative Adversarial Networks Learn .pdf;/Users/mfine/Zotero/storage/XBYQWDTL/1811.html}
}

@article{LSZ15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.02761},
  primaryClass = {cs, stat},
  title = {Generative {{Moment Matching Networks}}},
  abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
  language = {en},
  journal = {arXiv:1502.02761 [cs, stat]},
  author = {Li, Yujia and Swersky, Kevin and Zemel, Richard},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/4AB39H8X/Li et al. - 2015 - Generative Moment Matching Networks.pdf}
}

@article{BLM19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.03468},
  primaryClass = {cs, stat},
  title = {Passing {{Tests}} without {{Memorizing}}: {{Two Models}} for {{Fooling Discriminators}}},
  shorttitle = {Passing {{Tests}} without {{Memorizing}}},
  abstract = {We introduce two mathematical frameworks for foolability in the context of generative distribution learning. In a nuthsell, fooling is an algorithmic task in which the input sample is drawn from some target distribution and the goal is to output a synthetic distribution that is indistinguishable from the target w.r.t to some fixed class of tests. This framework received considerable attention in the context of Generative Adversarial Networks (GANs), a recently proposed approach which achieves impressive empirical results. From a theoretical viewpoint this problem seems difficult to model. This is due to the fact that in its basic form, the notion of foolability is susceptible to a type of overfitting called memorizing. This raises a challenge of devising notions and definitions that separate between fooling algorithms that generate new synthetic data vs. algorithms that merely memorize or copy the training set. The first model we consider is called GAM--Foolability and is inspired by GANs. Here the learner has only an indirect access to the target distribution via a discriminator. The second model, called DP--Foolability, exploits the notion of differential privacy as a candidate criterion for non-memorization. We proceed to characterize foolability within these two models and study their interrelations. We show that DP--Foolability implies GAM--Foolability and prove partial results with respect to the converse. It remains, though, an open question whether GAM--Foolability implies DP--Foolability. We also present an application in the context of differentially private PAC learning. We show that from a statistical perspective, for any class H, learnability by a private proper learner is equivalent to the existence of a private sanitizer for H. This can be seen as an analogue of the equivalence between uniform convergence and learnability in classical PAC learning.},
  journal = {arXiv:1902.03468 [cs, stat]},
  author = {Bousquet, Olivier and Livni, Roi and Moran, Shay},
  month = feb,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/T6AAKX67/Bousquet et al. - 2019 - Passing Tests without Memorizing Two Models for F.pdf;/Users/mfine/Zotero/storage/EBJJCNMQ/1902.html}
}

@article{Shi14,
  title = {There {{Can Be No Turing}}-{{Test}}-{{Passing Memorizing Machines}}},
  volume = {14},
  abstract = {Anti-behaviorist arguments against the validity of the Turing Test as a sufficient condition for attributing intelligence are based on a memorizing machine, which has recorded within it responses to every possible Turing Test interaction of up to a fixed length. The mere possibility of such a machine is claimed to be enough to invalidate the Turing Test.},
  language = {en},
  number = {16},
  author = {Shieber, Stuart M},
  year = {2014},
  pages = {14},
  file = {/Users/mfine/Zotero/storage/9LEMU9C9/Shieber - 2014 - There Can Be No Turing-Test-Passing Memorizing Mac.pdf}
}

@article{Aro50,
  title = {Theory of {{Reproducing Kernels}}},
  volume = {68},
  issn = {0002-9947},
  number = {3},
  journal = {Transactions of the American Mathematical Society},
  doi = {10.2307/1990404},
  author = {Aronszajn, N.},
  year = {1950},
  pages = {337-404},
  file = {/Users/mfine/Zotero/storage/IKDRIK2Q/Aronszajn - 1950 - Theory of Reproducing Kernels.pdf}
}

@article{SC,
  title = {Kernel {{Methods}} for {{Pattern Analysis}}},
  language = {en},
  author = {{Shawe-Taylor}, John and Cristianini, Nello},
  pages = {478},
  file = {/Users/mfine/Zotero/storage/EH7RF87T/Shawe-Taylor and Cristianini - Kernel Methods for Pattern Analysis.pdf}
}

@inproceedings{TH15,
  title = {{{KernelADASYN}}: {{Kernel}} Based Adaptive Synthetic Data Generation for Imbalanced Learning},
  shorttitle = {{{KernelADASYN}}},
  abstract = {In imbalanced learning, most standard classification algorithms usually fail to properly represent data distribution and provide unfavorable classification performance. More specifically, the decision rule of minority class is usually weaker than majority class, leading to many misclassification of expensive minority class data. Motivated by our previous work ADASYN [1], this paper presents a novel kernel based adaptive synthetic over-sampling approach, named KernelADASYN, for imbalanced data classification problems. The idea is to construct an adaptive over-sampling distribution to generate synthetic minority class data. The adaptive over-sampling distribution is first estimated with kernel density estimation methods and is further weighted by the difficulty level for different minority class data. The classification performance of our proposed adaptive over-sampling approach is evaluated on several real-life benchmarks, specifically on medical and healthcare applications. The experimental results show the competitive classification performance for many real-life imbalanced data classification problems.},
  booktitle = {2015 {{IEEE Congress}} on {{Evolutionary Computation}} ({{CEC}})},
  doi = {10.1109/CEC.2015.7256954},
  author = {Tang, B. and He, H.},
  month = may,
  year = {2015},
  keywords = {Estimation,learning (artificial intelligence),Accuracy,adaptive over-sampling,data distribution,expensive minority class data misclassification,imbalanced data classification problems,imbalanced learning,Imbalanced learning,Kernel,kernel based adaptive synthetic data generation,kernel based adaptive synthetic over-sampling approach,kernel density estimation,kernel density estimation methods,KernelADASYN,Measurement,medical and healthcare data learning,minority class decision rule,pattern classification,pattern recognition,sampling methods,Sampling methods,standard classification algorithms,Standards,Training data},
  pages = {664-671},
  file = {/Users/mfine/Zotero/storage/NRURLELV/Tang and He - 2015 - KernelADASYN Kernel based adaptive synthetic data.pdf;/Users/mfine/Zotero/storage/NUFD5ARF/7256954.html}
}

@article{19,
  title = {Chebyshev Polynomials},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {The Chebyshev polynomials are two sequences of polynomials, denoted Tn(x) and Un(x). They are defined as follows. By the double angle formula,

  
    
      
        cos
        
        (
        2
        \texttheta{}
        )
        =
        2
        
          cos
          
            2
          
        
        
        (
        \texttheta{}
        )
        -
        1
      
    
    \{\textbackslash{}displaystyle \textbackslash{}cos(2\textbackslash{}theta )=2\textbackslash{}cos \^\{2\}(\textbackslash{}theta )-1\}
  is a polynomial in cos(\texttheta{}), so define T2(x)=2x2-1. The other Tn(x) are defined similarly, using cos(n\texttheta{})=Tn(cos(\texttheta{})). Similarly, define the other sequence by sin(n\texttheta{})=Un-1(cos(\texttheta{}))sin(\texttheta{}), where we have used de Moivre's formula to note that sin(n\texttheta{}) is sin(\texttheta{}) times a polynomial in cos(\texttheta{}). For instance, 

  
    
      
        sin
        
        (
        3
        \texttheta{}
        )
        =
        (
        4
        
          cos
          
            2
          
        
        
        (
        \texttheta{}
        )
        -
        1
        )
        sin
        
        (
        \texttheta{}
        )
      
    
    \{\textbackslash{}displaystyle \textbackslash{}sin(3\textbackslash{}theta )=(4\textbackslash{}cos \^\{2\}(\textbackslash{}theta )-1)\textbackslash{}sin(\textbackslash{}theta )\}
  gives U2(\texttheta{})=4x2-1. The Tn(x) and Un(x) are called Chebyshev polynomials of the first and second kind, respectively.

The Tn(x) are orthogonal with respect to the inner product 

  
    
      
        (
        f
        (
        x
        )
        ,
        g
        (
        x
        )
        )
        =
        
          {$\int$}
          
            -
            1
          
          
            1
          
        
        f
        (
        x
        )
        g
        (
        x
        )
        
          
            
              d
              x
            
            
              1
              -
              
                x
                
                  2
                
              
            
          
        
      
    
    \{\textbackslash{}displaystyle (f(x),g(x))=\textbackslash{}int \_\{-1\}\^\{1\}f(x)g(x)\{\textbackslash{}frac \{dx\}\{\textbackslash{}sqrt \{1-x\^\{2\}\}\}\}\}
  and Un(x) are orthogonal with respect to a different product. This follows from the fact that the Chebyschev polynomials solve the Chebyshev differential equations

  
    
      
        
          (
          
            1
            -
            
              x
              
                2
              
            
          
          )
        
        
          y
          {${''}$}
        
        -
        x
        
          y
          {${'}$}
        
        +
        
          n
          
            2
          
        
        y
        =
        0
      
    
    \{\textbackslash{}displaystyle \textbackslash{}left(1-x\^\{2\}\textbackslash{}right)y''-xy'+n\^\{2\}y=0\}
  

  
    
      
        
          (
          
            1
            -
            
              x
              
                2
              
            
          
          )
        
        
          y
          {${''}$}
        
        -
        3
        x
        
          y
          {${'}$}
        
        +
        n
        (
        n
        +
        2
        )
        y
        =
        0
      
    
    \{\textbackslash{}displaystyle \textbackslash{}left(1-x\^\{2\}\textbackslash{}right)y''-3xy'+n(n+2)y=0\}
  which are Sturm\textendash{}Liouville differential equations. It is a general feature of such differential equations that there is a distinguished orthonormal set of solutions.
The Chebyshev polynomials Tn are polynomials with the largest possible leading coefficient whose absolute value on the interval [-1,1] is bounded by 1.  They are also the extremal polynomials for many other properties. Chebyshev polynomials are important in approximation theory because the roots of Tn(x), which are also called Chebyshev nodes, are used as nodes in polynomial interpolation. The resulting interpolation polynomial minimizes the problem of Runge's phenomenon and provides an approximation that is close to the polynomial of best approximation to a continuous function under the maximum norm. This approximation leads directly to the method of Clenshaw\textendash{}Curtis quadrature. 

These polynomials were named after Pafnuty Chebyshev,. The letter T is used because of the alternative transliterations of the name Chebyshev as Tchebycheff, Tchebyshev (French) or Tschebyschow (German).},
  language = {en},
  journal = {Wikipedia},
  month = aug,
  year = {2019},
  note = {Page Version ID: 912608859}
}

@article{ZM15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1405.2664},
  title = {{{FastMMD}}: {{Ensemble}} of {{Circular Discrepancy}} for {{Efficient Two}}-{{Sample Test}}},
  volume = {27},
  issn = {0899-7667, 1530-888X},
  shorttitle = {{{FastMMD}}},
  abstract = {The maximum mean discrepancy (MMD) is a recently proposed test statistic for two-sample test. Its quadratic time complexity, however, greatly hampers its availability to large-scale applications. To accelerate the MMD calculation, in this study we propose an efficient method called FastMMD. The core idea of FastMMD is to equivalently transform the MMD with shiftinvariant kernels into the amplitude expectation of a linear combination of sinusoid components based on Bochner's theorem and Fourier transform (Rahimi \& Recht, 2007). Taking advantage of sampling of Fourier transform, FastMMD decreases the time complexity for MMD calculation from O(N 2d) to O(N d), where N and d are the size and dimension of the sample set, respectively. For kernels that are spherically invariant, the computation can be further accelerated to O(N log d) by using the Fastfood technique (Le et al., 2013). The uniform convergence of our method has also been theoretically proved in both unbiased and biased estimates. We have further provided a geometric explanation for our method, namely ensemble of circular discrepancy, which facilitates us to understand the insight of MMD, and is hopeful to help arouse more extensive metrics for assessing two-sample test. Experimental results substantiate that FastMMD is with similar accuracy as exact MMD, while with faster computation speed and lower variance than the existing MMD approximation methods.},
  language = {en},
  number = {6},
  journal = {Neural Computation},
  doi = {10.1162/NECO_a_00732},
  author = {Zhao, Ji and Meng, Deyu},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  pages = {1345-1372},
  file = {/Users/mfine/Zotero/storage/TK7ZN5N6/Zhao and Meng - 2015 - FastMMD Ensemble of Circular Discrepancy for Effi.pdf}
}

@article{GHRU10,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1296},
  primaryClass = {cs},
  title = {Privately {{Releasing Conjunctions}} and the {{Statistical Query Barrier}}},
  abstract = {Suppose we would like to know all answers to a set of statistical queries C on a data set up to small error, but we can only access the data itself using statistical queries. A trivial solution is to exhaustively ask all queries in C. Can we do any better? + We show that the number of statistical queries necessary and sufficient for this task is---up to polynomial factors---equal to the agnostic learning complexity of C in Kearns' statistical query (SQ) model. This gives a complete answer to the question when running time is not a concern. + We then show that the problem can be solved efficiently (allowing arbitrary error on a small fraction of queries) whenever the answers to C can be described by a submodular function. This includes many natural concept classes, such as graph cuts and Boolean disjunctions and conjunctions. While interesting from a learning theoretic point of view, our main applications are in privacy-preserving data analysis: Here, our second result leads to the first algorithm that efficiently releases differentially private answers to of all Boolean conjunctions with 1\% average error. This presents significant progress on a key open problem in privacy-preserving data analysis. Our first result on the other hand gives unconditional lower bounds on any differentially private algorithm that admits a (potentially non-privacy-preserving) implementation using only statistical queries. Not only our algorithms, but also most known private algorithms can be implemented using only statistical queries, and hence are constrained by these lower bounds. Our result therefore isolates the complexity of agnostic learning in the SQ-model as a new barrier in the design of differentially private algorithms.},
  journal = {arXiv:1011.1296 [cs]},
  author = {Gupta, Anupam and Hardt, Moritz and Roth, Aaron and Ullman, Jonathan},
  month = nov,
  year = {2010},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms},
  file = {/Users/mfine/Zotero/storage/CERT2VWV/Gupta et al. - 2010 - Privately Releasing Conjunctions and the Statistic.pdf;/Users/mfine/Zotero/storage/2Z2HMLZY/1011.html}
}

@article{PLA18,
  title = {A {{Novel Boolean Kernels Family}} for {{Categorical Data}}},
  volume = {20},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  abstract = {Kernel based classifiers, such as SVM, are considered state-of-the-art algorithms and are widely used on many classification tasks. However, this kind of methods are hardly interpretable and for this reason they are often considered as black-box models. In this paper, we propose a new family of Boolean kernels for categorical data where features correspond to propositional formulas applied to the input variables. The idea is to create human-readable features to ease the extraction of interpretation rules directly from the embedding space. Experiments on artificial and benchmark datasets show the effectiveness of the proposed family of kernels with respect to established ones, such as RBF, in terms of classification accuracy.},
  language = {en},
  number = {6},
  journal = {Entropy},
  doi = {10.3390/e20060444},
  author = {Polato, Mirko and Lauriola, Ivano and Aiolli, Fabio},
  month = jun,
  year = {2018},
  keywords = {boolean kernels,categorical data,kernel methods,SVM},
  pages = {444},
  file = {/Users/mfine/Zotero/storage/LL3JBAE5/Polato et al. - 2018 - A Novel Boolean Kernels Family for Categorical Dat.pdf;/Users/mfine/Zotero/storage/QTCEX9FI/444.html}
}

@article{ZZH+18,
  title = {{{ON THE DISCRIMINATION}}-{{GENERALIZATION TRADE}}- {{OFF IN GANS}}},
  abstract = {Generative adversarial training can be generally understood as minimizing certain moment matching loss defined by a set of discriminator functions, typically neural networks. The discriminator set should be large enough to be able to uniquely identify the true distribution (discriminative), and also be small enough to go beyond memorizing samples (generalizable). In this paper, we show that a discriminator set is guaranteed to be discriminative whenever its linear span is dense in the set of bounded continuous functions. This is a very mild condition satisfied even by neural networks with a single neuron. Further, we develop generalization bounds between the learned distribution and true distribution under different evaluation metrics. When evaluated with neural distance, our bounds show that generalization is guaranteed as long as the discriminator set is small enough, regardless of the size of the generator or hypothesis set. When evaluated with KL divergence, our bound provides an explanation on the counter-intuitive behaviors of testing likelihood in GAN training. Our analysis sheds lights on understanding the practical performance of GANs.},
  language = {en},
  author = {Zhang, Pengchuan and Zhou, Dengyong and He, Xiaodong and Liu, Qiang and Xu, Tao},
  year = {2018},
  pages = {26},
  file = {/Users/mfine/Zotero/storage/RBAC7G7N/Zhang et al. - 2018 - ON THE DISCRIMINATION-GENERALIZATION TRADE- OFF IN.pdf}
}

@misc{Hara,
  title = {Do {{GANs}} Actually Do Distribution Learning?},
  abstract = {Algorithms off the convex path.},
  journal = {Off the convex path},
  howpublished = {http://offconvex.github.io/2017/07/06/GANs3/},
  author = {Hardt, Moritz},
  file = {/Users/mfine/Zotero/storage/RJPHL8CY/GANs3.html}
}

@misc{Har,
  title = {Generalization and {{Equilibrium}} in {{Generative Adversarial Networks}} ({{GANs}})},
  abstract = {Algorithms off the convex path.},
  journal = {Off the convex path},
  howpublished = {http://offconvex.github.io/2017/03/30/GANs2/},
  author = {Hardt, Moritz},
  file = {/Users/mfine/Zotero/storage/YUGR8VXD/GANs2.html}
}

@article{HRU13,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1211.0877},
  title = {Differential {{Privacy}} for the {{Analyst}} via {{Private Equilibrium Computation}}},
  abstract = {We give new mechanisms for answering exponentially many queries from multiple analysts on a private database, while protecting differential privacy both for the individuals in the database and for the analysts. That is, our mechanism's answer to each query is nearly insensitive to changes in the queries asked by other analysts. Our mechanism is the first to offer differential privacy on the joint distribution over analysts' answers, providing privacy for data analysts even if the other data analysts collude or register multiple accounts. In some settings, we are able to achieve nearly optimal error rates (even compared to mechanisms which do not offer analyst privacy), and we are able to extend our techniques to handle non-linear queries. Our analysis is based on a novel view of the private query-release problem as a two-player zero-sum game, which may be of independent interest.},
  journal = {Proceedings of the 45th annual ACM symposium on Symposium on theory of computing - STOC '13},
  doi = {10.1145/2488608.2488651},
  author = {Hsu, Justin and Roth, Aaron and Ullman, Jonathan},
  year = {2013},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Data Structures and Algorithms},
  pages = {341},
  file = {/Users/mfine/Zotero/storage/7U2Z6TS6/Hsu et al. - 2013 - Differential Privacy for the Analyst via Private E.pdf;/Users/mfine/Zotero/storage/MQR8JZTF/1211.html}
}

@article{NRW18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.07765},
  primaryClass = {cs, stat},
  title = {How to {{Use Heuristics}} for {{Differential Privacy}}},
  abstract = {We develop theory for using heuristics to solve computationally hard problems in differential privacy. Heuristic approaches have enjoyed tremendous success in machine learning, for which performance can be empirically evaluated. However, privacy guarantees cannot be evaluated empirically, and must be proven --- without making heuristic assumptions. We show that learning problems over broad classes of functions can be solved privately and efficiently, assuming the existence of a non-private oracle for solving the same problem. Our first algorithm yields a privacy guarantee that is contingent on the correctness of the oracle. We then give a reduction which applies to a class of heuristics which we call certifiable, which allows us to convert oracle-dependent privacy guarantees to worst-case privacy guarantee that hold even when the heuristic standing in for the oracle might fail in adversarial ways. Finally, we consider a broad class of functions that includes most classes of simple boolean functions studied in the PAC learning literature, including conjunctions, disjunctions, parities, and discrete halfspaces. We show that there is an efficient algorithm for privately constructing synthetic data for any such class, given a non-private learning oracle. This in particular gives the first oracle-efficient algorithm for privately generating synthetic data for contingency tables. The most intriguing question left open by our work is whether or not every problem that can be solved differentially privately can be privately solved with an oracle-efficient algorithm. While we do not resolve this, we give a barrier result that suggests that any generic oracle-efficient reduction must fall outside of a natural class of algorithms (which includes the algorithms given in this paper).},
  journal = {arXiv:1811.07765 [cs, stat]},
  author = {Neel, Seth and Roth, Aaron and Wu, Zhiwei Steven},
  month = nov,
  year = {2018},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Data Structures and Algorithms,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/ZA2SUAIK/Neel et al. - 2018 - How to Use Heuristics for Differential Privacy.pdf;/Users/mfine/Zotero/storage/CXK87A3W/1811.html}
}

@incollection{LBC17a,
  title = {Approximation and {{Convergence Properties}} of {{Generative Adversarial Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Liu, Shuang and Bousquet, Olivier and Chaudhuri, Kamalika},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5545--5553},
  file = {/Users/mfine/Zotero/storage/4FDE92DM/Liu et al. - 2017 - Approximation and Convergence Properties of Genera.pdf;/Users/mfine/Zotero/storage/5XT9YTAK/7138-approximation-and-convergence-properties-of-generative-adversarial-learning.html}
}

@article{CHM+14,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.0233},
  primaryClass = {cs},
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  journal = {arXiv:1412.0233 [cs]},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  month = nov,
  year = {2014},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/CN3F9FA3/Choromanska et al. - 2014 - The Loss Surfaces of Multilayer Networks.pdf;/Users/mfine/Zotero/storage/2GS4EFXC/1412.html}
}

@article{GLL+17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03269},
  primaryClass = {cs, stat},
  title = {An {{Online Learning Approach}} to {{Generative Adversarial Networks}}},
  abstract = {We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.},
  journal = {arXiv:1706.03269 [cs, stat]},
  author = {Grnarova, Paulina and Levy, Kfir Y. and Lucchi, Aurelien and Hofmann, Thomas and Krause, Andreas},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/GINHB5IQ/Grnarova et al. - 2017 - An Online Learning Approach to Generative Adversar.pdf;/Users/mfine/Zotero/storage/R49VUE5Q/1706.html}
}

@article{NK17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.04156},
  primaryClass = {cs, math, stat},
  title = {Gradient Descent {{GAN}} Optimization Is Locally Stable},
  abstract = {Despite the growing prominence of generative adversarial networks (GANs), optimization in GANs is still a poorly understood topic. In this paper, we analyze the "gradient descent" form of GAN optimization i.e., the natural setting where we simultaneously take small gradient steps in both generator and discriminator parameters. We show that even though GAN optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still \textbackslash{}emph\{locally asymptotically stable\} for the traditional GAN formulation. On the other hand, we show that the recently proposed Wasserstein GAN can have non-convergent limit cycles near equilibrium. Motivated by this stability analysis, we propose an additional regularization term for gradient descent GAN updates, which \textbackslash{}emph\{is\} able to guarantee local stability for both the WGAN and the traditional GAN, and also shows practical promise in speeding up convergence and addressing mode collapse.},
  journal = {arXiv:1706.04156 [cs, math, stat]},
  author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/mfine/Zotero/storage/W5F5P3SU/Nagarajan and Kolter - 2017 - Gradient descent GAN optimization is locally stabl.pdf;/Users/mfine/Zotero/storage/ITFASWCG/1706.html}
}

@article{OSG+18a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.07268},
  primaryClass = {cs, stat},
  title = {Beyond {{Local Nash Equilibria}} for {{Adversarial Networks}}},
  abstract = {Save for some special cases, current training methods for Generative Adversarial Networks (GANs) are at best guaranteed to converge to a `local Nash equilibrium` (LNE). Such LNEs, however, can be arbitrarily far from an actual Nash equilibrium (NE), which implies that there are no guarantees on the quality of the found generator or classifier. This paper proposes to model GANs explicitly as finite games in mixed strategies, thereby ensuring that every LNE is an NE. With this formulation, we propose a solution method that is proven to monotonically converge to a resource-bounded Nash equilibrium (RB-NE): by increasing computational resources we can find better solutions. We empirically demonstrate that our method is less prone to typical GAN problems such as mode collapse, and produces solutions that are less exploitable than those produced by GANs and MGANs, and closely resemble theoretical predictions about NEs.},
  journal = {arXiv:1806.07268 [cs, stat]},
  author = {Oliehoek, Frans A. and Savani, Rahul and Gallego, Jose and {van der Pol}, Elise and Gro{\ss}, Roderich},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/X3MQXI3B/Oliehoek et al. - 2018 - Beyond Local Nash Equilibria for Adversarial Netwo.pdf;/Users/mfine/Zotero/storage/UCXYJPVL/1806.html}
}

@article{DLT+,
  title = {Gradient {{Descent Learns One}}-Hidden-Layer {{CNN}}: {{Don}}'t Be {{Afraid}} of {{Spurious Local Minima}}},
  language = {en},
  author = {Du, Simon S and Lee, Jason D and Tian, Yuandong and Poczos, Barnabas and Singh, Aarti},
  pages = {10},
  file = {/Users/mfine/Zotero/storage/L22DIR2S/Du et al. - Gradient Descent Learns One-hidden-layer CNN Don.pdf}
}

@article{MSF,
  title = {Imposing {{Hard Constraints}} on {{Deep Networks}}: {{Promises}} and {{Limitations}}},
  abstract = {Imposing constraints on the output of a Deep Neural Net is one way to improve the quality of its predictions while loosening the requirements for labeled training data. Such constraints are usually imposed as soft constraints by adding new terms to the loss function that is minimized during training. An alternative is to impose them as hard constraints, which has a number of theoretical benefits but has not been explored so far due to the perceived intractability of the problem.},
  language = {en},
  author = {{Marquez-Neila}, Pablo and Salzmann, Mathieu and Fua, Pascal},
  pages = {9},
  file = {/Users/mfine/Zotero/storage/GM657EN2/Marquez-Neila et al. - Imposing Hard Constraints on Deep Networks Promis.pdf}
}

@article{FT18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.11740},
  primaryClass = {cs, stat},
  title = {A {{Convex Duality Framework}} for {{GANs}}},
  abstract = {Generative adversarial network (GAN) is a minimax game between a generator mimicking the true model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function, this game reduces to finding the generative model minimizing a divergence measure, e.g. the Jensen-Shannon (JS) divergence, to the data distribution. However, in practice the discriminator is constrained to be in a smaller class \$\textbackslash{}mathcal\{F\}\$ such as neural nets. Then, a natural question is how the divergence minimization interpretation changes as we constrain \$\textbackslash{}mathcal\{F\}\$. In this work, we address this question by developing a convex duality framework for analyzing GANs. For a convex set \$\textbackslash{}mathcal\{F\}\$, this duality framework interprets the original GAN formulation as finding the generative model with minimum JS-divergence to the distributions penalized to match the moments of the data distribution, with the moments specified by the discriminators in \$\textbackslash{}mathcal\{F\}\$. We show that this interpretation more generally holds for f-GAN and Wasserstein GAN. As a byproduct, we apply the duality framework to a hybrid of f-divergence and Wasserstein distance. Unlike the f-divergence, we prove that the proposed hybrid divergence changes continuously with the generative model, which suggests regularizing the discriminator's Lipschitz constant in f-GAN and vanilla GAN. We numerically evaluate the power of the suggested regularization schemes for improving GAN's training performance.},
  journal = {arXiv:1810.11740 [cs, stat]},
  author = {Farnia, Farzan and Tse, David},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/L9QEBCDA/Farnia and Tse - 2018 - A Convex Duality Framework for GANs.pdf;/Users/mfine/Zotero/storage/NDGMCFL6/1810.html}
}

@article{BMR19,
  title = {{{APPROXIMABILITY OF DISCRIMINATORS IMPLIES DIVERSITY IN GANS}}},
  abstract = {While Generative Adversarial Networks (GANs) have empirically produced impressive results on learning complex real-world distributions, recent works have shown that they suffer from lack of diversity or mode collapse. The theoretical work of Arora et al. (2017a) suggests a dilemma about GANs' statistical properties: powerful discriminators cause overfitting, whereas weak discriminators cannot detect mode collapse.},
  language = {en},
  author = {Bai, Yu and Ma, Tengyu and Risteski, Andrej},
  year = {2019},
  pages = {38},
  file = {/Users/mfine/Zotero/storage/XBVWJ5YB/Bai et al. - 2019 - APPROXIMABILITY OF DISCRIMINATORS IMPLIES DIVERSIT.pdf}
}

@article{LBC17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08991},
  primaryClass = {cs, stat},
  title = {Approximation and {{Convergence Properties}} of {{Generative Adversarial Learning}}},
  abstract = {Generative adversarial networks (GAN) approximate a target data distribution by jointly optimizing an objective function through a "two-player game" between a generator and a discriminator. Despite their empirical success, however, two very basic questions on how well they can approximate the target distribution remain unanswered. First, it is not known how restricting the discriminator family affects the approximation quality. Second, while a number of different objective functions have been proposed, we do not understand when convergence to the global minima of the objective function leads to convergence to the target distribution under various notions of distributional convergence. In this paper, we address these questions in a broad and unified setting by defining a notion of adversarial divergences that includes a number of recently proposed objective functions. We show that if the objective function is an adversarial divergence with some additional conditions, then using a restricted discriminator family has a moment-matching effect. Additionally, we show that for objective functions that are strict adversarial divergences, convergence in the objective function implies weak convergence, thus generalizing previous results.},
  journal = {arXiv:1705.08991 [cs, stat]},
  author = {Liu, Shuang and Bousquet, Olivier and Chaudhuri, Kamalika},
  month = may,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/A7KV2V4L/Liu et al. - 2017 - Approximation and Convergence Properties of Genera.pdf;/Users/mfine/Zotero/storage/8I8HAZNM/1705.html}
}

@article{CMS,
  title = {Differentially {{Private Empirical Risk Minimization}}},
  abstract = {Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the {$\epsilon$}-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacypreserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.},
  language = {en},
  author = {Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D},
  pages = {41},
  file = {/Users/mfine/Zotero/storage/J8WDB29R/Chaudhuri et al. - Differentially Private Empirical Risk Minimization.pdf}
}

@inproceedings{FS96,
  address = {{Desenzano del Garda, Italy}},
  title = {Game Theory, on-Line Prediction and Boosting},
  isbn = {978-0-89791-811-4},
  abstract = {We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann's famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this gameplaying algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the ``dual'' of this game.},
  language = {en},
  booktitle = {Proceedings of the Ninth Annual Conference on {{Computational}} Learning Theory  - {{COLT}} '96},
  publisher = {{ACM Press}},
  doi = {10.1145/238061.238163},
  author = {Freund, Yoav and Schapire, Robert E.},
  year = {1996},
  pages = {325-332},
  file = {/Users/mfine/Zotero/storage/EW6B87TQ/Freund and Schapire - 1996 - Game theory, on-line prediction and boosting.pdf}
}

@article{OSG+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.07268},
  primaryClass = {cs, stat},
  title = {Beyond {{Local Nash Equilibria}} for {{Adversarial Networks}}},
  abstract = {Save for some special cases, current training methods for Generative Adversarial Networks (GANs) are at best guaranteed to converge to a `local Nash equilibrium` (LNE). Such LNEs, however, can be arbitrarily far from an actual Nash equilibrium (NE), which implies that there are no guarantees on the quality of the found generator or classifier. This paper proposes to model GANs explicitly as finite games in mixed strategies, thereby ensuring that every LNE is an NE. With this formulation, we propose a solution method that is proven to monotonically converge to a resource-bounded Nash equilibrium (RB-NE): by increasing computational resources we can find better solutions. We empirically demonstrate that our method is less prone to typical GAN problems such as mode collapse, and produces solutions that are less exploitable than those produced by GANs and MGANs, and closely resemble theoretical predictions about NEs.},
  journal = {arXiv:1806.07268 [cs, stat]},
  author = {Oliehoek, Frans A. and Savani, Rahul and Gallego, Jose and {van der Pol}, Elise and Gro{\ss}, Roderich},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/9PZGRGWY/Oliehoek et al. - 2018 - Beyond Local Nash Equilibria for Adversarial Netwo.pdf;/Users/mfine/Zotero/storage/A5AG6Y9I/1806.html}
}

@article{HLC18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.02002},
  primaryClass = {cs, stat},
  title = {Finding {{Mixed Nash Equilibria}} of {{Generative Adversarial Networks}}},
  abstract = {We reconsider the training objective of Generative Adversarial Networks (GANs) from the mixed Nash Equilibria (NE) perspective. Inspired by the classical prox methods, we develop a novel algorithmic framework for GANs via an infinite-dimensional two-player game and prove rigorous convergence rates to the mixed NE, resolving the longstanding problem that no provably convergent algorithm exists for general GANs. We then propose a principled procedure to reduce our novel prox methods to simple sampling routines, leading to practically efficient algorithms. Finally, we provide experimental evidence that our approach outperforms methods that seek pure strategy equilibria, such as SGD, Adam, and RMSProp, both in speed and quality.},
  journal = {arXiv:1811.02002 [cs, stat]},
  author = {Hsieh, Ya-Ping and Liu, Chen and Cevher, Volkan},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/KAYWD7T6/Hsieh et al. - 2018 - Finding Mixed Nash Equilibria of Generative Advers.pdf;/Users/mfine/Zotero/storage/E2YUEU9I/1811.html}
}

@article{GXC+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.08647},
  primaryClass = {cs, stat},
  title = {Fictitious {{GAN}}: {{Training GANs}} with {{Historical Models}}},
  shorttitle = {Fictitious {{GAN}}},
  abstract = {Generative adversarial networks (GANs) are powerful tools for learning generative models. In practice, the training may suffer from lack of convergence. GANs are commonly viewed as a two-player zero-sum game between two neural networks. Here, we leverage this game theoretic view to study the convergence behavior of the training process. Inspired by the fictitious play learning process, a novel training method, referred to as Fictitious GAN, is introduced. Fictitious GAN trains the deep neural networks using a mixture of historical models. Specifically, the discriminator (resp. generator) is updated according to the best-response to the mixture outputs from a sequence of previously trained generators (resp. discriminators). It is shown that Fictitious GAN can effectively resolve some convergence issues that cannot be resolved by the standard training approach. It is proved that asymptotically the average of the generator outputs has the same distribution as the data samples.},
  journal = {arXiv:1803.08647 [cs, stat]},
  author = {Ge, Hao and Xia, Yin and Chen, Xu and Berry, Randall and Wu, Ying},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/EPTBYR6C/Ge et al. - 2018 - Fictitious GAN Training GANs with Historical Mode.pdf;/Users/mfine/Zotero/storage/Y9CLUA5Y/1803.html}
}

@article{LAL+19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.06758},
  primaryClass = {cs, stat},
  title = {Algorithms for {{Verifying Deep Neural Networks}}},
  abstract = {Deep neural networks are widely used for nonlinear function approximation with applications ranging from computer vision to control. Although these networks involve the composition of simple arithmetic operations, it can be very challenging to verify whether a particular network satisfies certain input-output properties. This article surveys methods that have emerged recently for soundly verifying such properties. These methods borrow insights from reachability analysis, optimization, and search. We discuss fundamental differences and connections between existing algorithms. In addition, we provide pedagogical implementations of existing methods and compare them on a set of benchmark problems.},
  journal = {arXiv:1903.06758 [cs, stat]},
  author = {Liu, Changliu and Arnon, Tomer and Lazarus, Christopher and Barrett, Clark and Kochenderfer, Mykel J.},
  month = mar,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/Y2VY52KI/Liu et al. - 2019 - Algorithms for Verifying Deep Neural Networks.pdf;/Users/mfine/Zotero/storage/898HDWD4/1903.html}
}

@incollection{LPW+17,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  shorttitle = {The {{Expressive Power}} of {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {6231--6239},
  file = {/Users/mfine/Zotero/storage/ZGEK7R32/Lu et al. - 2017 - The Expressive Power of Neural Networks A View fr.pdf;/Users/mfine/Zotero/storage/2ZDT5DBM/7203-the-expressive-power-of-neural-networks-a-view-from-the-width.html}
}

@article{TGB+17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.02386},
  primaryClass = {cs, stat},
  title = {{{AdaGAN}}: {{Boosting Generative Models}}},
  shorttitle = {{{AdaGAN}}},
  abstract = {Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.},
  journal = {arXiv:1701.02386 [cs, stat]},
  author = {Tolstikhin, Ilya and Gelly, Sylvain and Bousquet, Olivier and {Simon-Gabriel}, Carl-Johann and Sch{\"o}lkopf, Bernhard},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/H6TDIC5M/Tolstikhin et al. - 2017 - AdaGAN Boosting Generative Models.pdf;/Users/mfine/Zotero/storage/AZ5KN898/1701.html}
}

@article{LAX+17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.09549},
  primaryClass = {cs},
  title = {Distributional {{Adversarial Networks}}},
  abstract = {We propose a framework for adversarial training that relies on a sample rather than a single sample point as the fundamental unit of discrimination. Inspired by discrepancy measures and two-sample tests between probability distributions, we propose two such distributional adversaries that operate and predict on samples, and show how they can be easily implemented on top of existing models. Various experimental results show that generators trained with our distributional adversaries are much more stable and are remarkably less prone to mode collapse than traditional models trained with pointwise prediction discriminators. The application of our framework to domain adaptation also results in considerable improvement over recent state-of-the-art.},
  journal = {arXiv:1706.09549 [cs]},
  author = {Li, Chengtao and {Alvarez-Melis}, David and Xu, Keyulu and Jegelka, Stefanie and Sra, Suvrit},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/BB8ARZTF/Li et al. - 2017 - Distributional Adversarial Networks.pdf;/Users/mfine/Zotero/storage/WQZYHWQ7/1706.html}
}

@article{LY17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.02894},
  primaryClass = {cond-mat, stat},
  title = {Geometric {{GAN}}},
  abstract = {Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.},
  journal = {arXiv:1705.02894 [cond-mat, stat]},
  author = {Lim, Jae Hyun and Ye, Jong Chul},
  month = may,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks},
  file = {/Users/mfine/Zotero/storage/EXE3UYX8/Lim and Ye - 2017 - Geometric GAN.pdf;/Users/mfine/Zotero/storage/PYXI3GBL/1705.html}
}

@article{XLW+18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.06739},
  primaryClass = {cs, stat},
  title = {Differentially {{Private Generative Adversarial Network}}},
  abstract = {Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.},
  journal = {arXiv:1802.06739 [cs, stat]},
  author = {Xie, Liyang and Lin, Kaixiang and Wang, Shu and Wang, Fei and Zhou, Jiayu},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/LJXKFC6Y/Xie et al. - 2018 - Differentially Private Generative Adversarial Netw.pdf;/Users/mfine/Zotero/storage/TAKGDEVS/1802.html}
}

@misc{16,
  title = {An {{Alternative Update Rule}} for {{Generative Adversarial Networks}}},
  abstract = {It is mentioned in the original GAN paper (Goodfellow et al, 2014) that the algorithm can be interpreted as minimising Jensen-Shannon divergence under some ideal conditions. This note is about a way to modify GANs slightly, so that they minimise \$\textbackslash{}operatorname\{KL\}[Q|P]\$ divergence instead of JS divergence. I...},
  language = {en},
  journal = {inFERENCe},
  howpublished = {https://www.inference.vc/an-alternative-update-rule-for-generative-adversarial-networks/},
  month = mar,
  year = {2016},
  file = {/Users/mfine/Zotero/storage/4S24KJRF/an-alternative-update-rule-for-generative-adversarial-networks.html}
}

@article{AGH18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.07362},
  primaryClass = {cs, stat},
  title = {Learning in {{Non}}-Convex {{Games}} with an {{Optimization Oracle}}},
  abstract = {We consider online learning in an adversarial, non-convex setting under the assumption that the learner has an access to an offline optimization oracle. In the general setting of prediction with expert advice, Hazan et al. (2016) established that in the optimization-oracle model, online learning requires exponentially more computation than statistical learning. In this paper we show that by slightly strengthening the oracle model, the online and the statistical learning models become computationally equivalent. Our result holds for any Lipschitz and bounded (but not necessarily convex) function. As an application we demonstrate how the offline oracle enables efficient computation of an equilibrium in non-convex games, that include GAN (generative adversarial networks) as a special case.},
  journal = {arXiv:1810.07362 [cs, stat]},
  author = {Agarwal, Naman and Gonen, Alon and Hazan, Elad},
  month = oct,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/KZ6HHSWV/Agarwal et al. - 2018 - Learning in Non-convex Games with an Optimization .pdf;/Users/mfine/Zotero/storage/38MUCKXM/1810.html}
}

@article{SW,
  title = {Minimizing the {{Maximal Loss}}: {{How}} and {{Why}}},
  abstract = {A commonly used learning rule is to approximately minimize the average loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the maximal loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.},
  language = {en},
  author = {{Shalev-Shwartz}, Shai and Wexler, Yonatan},
  pages = {13},
  file = {/Users/mfine/Zotero/storage/KNCXI5UY/Shalev-Shwartz and Wexler - Minimizing the Maximal Loss How and Why.pdf}
}

@incollection{FLYH17,
  title = {Learning with {{Average Top}}-k {{Loss}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Fan, Yanbo and Lyu, Siwei and Ying, Yiming and Hu, Baogang},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {497--505},
  file = {/Users/mfine/Zotero/storage/AMLVI3CH/Fan et al. - 2017 - Learning with Average Top-k Loss.pdf;/Users/mfine/Zotero/storage/NYYIWAJ2/6653-learning-with-average-top-k-loss.html}
}

@misc{zotero-1250,
  title = {Ju {{Sun}} | {{Provable Nonconvex Methods}}/{{Algorithms}}},
  howpublished = {https://sunju.org/research/nonconvex/},
  file = {/Users/mfine/Zotero/storage/BPBUSGAN/nonconvex.html}
}

@book{CL06,
  address = {{Cambridge; New York}},
  title = {Prediction, Learning, and Games},
  isbn = {978-0-511-19178-7 978-0-511-54692-1 978-0-511-18995-1 978-0-511-19059-9 978-0-511-19091-9 978-0-511-19131-2 978-0-521-84108-5},
  abstract = {The central theme here is a model of prediction using expert advice, a general framework within which many related problems can be cast and discussed, including repeated game playing, adaptive data compression, sequential investment in the stock market, and sequential pattern analysis.},
  language = {en},
  publisher = {{Cambridge University Press}},
  author = {{Cesa-Bianchi}, Nicol{\`o} and Lugosi, G{\'a}bor},
  year = {2006},
  file = {/Users/mfine/Zotero/storage/X9FEQW78/Cesa-Bianchi and Lugosi - 2006 - Prediction, learning, and games.pdf},
  note = {OCLC: 70056026}
}

@article{HK15,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1504.02089},
  primaryClass = {cs},
  title = {The {{Computational Power}} of {{Optimization}} in {{Online Learning}}},
  abstract = {We consider the fundamental problem of prediction with expert advice where the experts are "optimizable": there is a black-box optimization oracle that can be used to compute, in constant time, the leading expert in retrospect at any point in time. In this setting, we give a novel online algorithm that attains vanishing regret with respect to \$N\$ experts in total \$\textbackslash{}widetilde\{O\}(\textbackslash{}sqrt\{N\})\$ computation time. We also give a lower bound showing that this running time cannot be improved (up to log factors) in the oracle model, thereby exhibiting a quadratic speedup as compared to the standard, oracle-free setting where the required time for vanishing regret is \$\textbackslash{}widetilde\{\textbackslash{}Theta\}(N)\$. These results demonstrate an exponential gap between the power of optimization in online learning and its power in statistical learning: in the latter, an optimization oracle---i.e., an efficient empirical risk minimizer---allows to learn a finite hypothesis class of size \$N\$ in time \$O(\textbackslash{}log\{N\})\$. We also study the implications of our results to learning in repeated zero-sum games, in a setting where the players have access to oracles that compute, in constant time, their best-response to any mixed strategy of their opponent. We show that the runtime required for approximating the minimax value of the game in this setting is \$\textbackslash{}widetilde\{\textbackslash{}Theta\}(\textbackslash{}sqrt\{N\})\$, yielding again a quadratic improvement upon the oracle-free setting, where \$\textbackslash{}widetilde\{\textbackslash{}Theta\}(N)\$ is known to be tight.},
  journal = {arXiv:1504.02089 [cs]},
  author = {Hazan, Elad and Koren, Tomer},
  month = apr,
  year = {2015},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning},
  file = {/Users/mfine/Zotero/storage/X39A7P8I/Hazan and Koren - 2015 - The Computational Power of Optimization in Online .pdf;/Users/mfine/Zotero/storage/4W3CT56B/1504.html}
}

@article{GHM19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.11311},
  primaryClass = {cs, stat},
  title = {Private {{Learning Implies Online Learning}}: {{An Efficient Reduction}}},
  shorttitle = {Private {{Learning Implies Online Learning}}},
  abstract = {We study the relationship between the notions of differentially private learning and online learning in games. Several recent works have shown that differentially private learning implies online learning, but an open problem of Neel, Roth, and Wu \textbackslash{}cite\{NeelAaronRoth2018\} asks whether this implication is \{\textbackslash{}it efficient\}. Specifically, does an efficient differentially private learner imply an efficient online learner? In this paper we resolve this open question in the context of pure differential privacy. We derive an efficient black-box reduction from differentially private learning to online learning from expert advice.},
  journal = {arXiv:1905.11311 [cs, stat]},
  author = {Gonen, Alon and Hazan, Elad and Moran, Shay},
  month = may,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/mfine/Zotero/storage/WK29SNJX/Gonen et al. - 2019 - Private Learning Implies Online Learning An Effic.pdf;/Users/mfine/Zotero/storage/U5UWBB6S/1905.html}
}

@article{SN19,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1903.08110},
  primaryClass = {cs, math, stat},
  title = {Online {{Non}}-{{Convex Learning}}: {{Following}} the {{Perturbed Leader}} Is {{Optimal}}},
  shorttitle = {Online {{Non}}-{{Convex Learning}}},
  abstract = {We study the problem of online learning with non-convex losses, where the learner has access to an offline optimization oracle. We show that the classical Follow the Perturbed Leader (FTPL) algorithm achieves optimal regret rate of \$O(T\^\{-1/2\})\$ in this setting. This improves upon the previous best-known regret rate of \$O(T\^\{-1/3\})\$ for FTPL. We further show that an optimistic variant of FTPL achieves better regret bounds when the sequence of losses encountered by the learner is `predictable'.},
  journal = {arXiv:1903.08110 [cs, math, stat]},
  author = {Suggala, Arun Sai and Netrapalli, Praneeth},
  month = mar,
  year = {2019},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/mfine/Zotero/storage/PT2JLQ8C/Suggala and Netrapalli - 2019 - Online Non-Convex Learning Following the Perturbe.pdf;/Users/mfine/Zotero/storage/PANDZGRW/1903.html}
}

@article{JLGJ18,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09357},
  primaryClass = {cs, math, stat},
  title = {On the {{Local Minima}} of the {{Empirical Risk}}},
  abstract = {Population risk is always of primary interest in machine learning; however, learning algorithms only have access to the empirical risk. Even for applications with nonconvex nonsmooth losses (such as modern deep networks), the population risk is generally significantly more well-behaved from an optimization point of view than the empirical risk. In particular, sampling can create many spurious local minima. We consider a general framework which aims to optimize a smooth nonconvex function \$F\$ (population risk) given only access to an approximation \$f\$ (empirical risk) that is pointwise close to \$F\$ (i.e., \$\textbackslash{}|F-f\textbackslash{}|\_\{\textbackslash{}infty\} \textbackslash{}le \textbackslash{}nu\$). Our objective is to find the \$\textbackslash{}epsilon\$-approximate local minima of the underlying function \$F\$ while avoiding the shallow local minima---arising because of the tolerance \$\textbackslash{}nu\$---which exist only in \$f\$. We propose a simple algorithm based on stochastic gradient descent (SGD) on a smoothed version of \$f\$ that is guaranteed to achieve our goal as long as \$\textbackslash{}nu \textbackslash{}le O(\textbackslash{}epsilon\^\{1.5\}/d)\$. We also provide an almost matching lower bound showing that our algorithm achieves optimal error tolerance \$\textbackslash{}nu\$ among all algorithms making a polynomial number of queries of \$f\$. As a concrete example, we show that our results can be directly used to give sample complexities for learning a ReLU unit.},
  journal = {arXiv:1803.09357 [cs, math, stat]},
  author = {Jin, Chi and Liu, Lydia T. and Ge, Rong and Jordan, Michael I.},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/mfine/Zotero/storage/FAIG5ELQ/Jin et al. - 2018 - On the Local Minima of the Empirical Risk.pdf;/Users/mfine/Zotero/storage/B6KSM3QL/1803.html}
}

@article{FS99,
  title = {Adaptive {{Game Playing Using Multiplicative Weights}}},
  volume = {29},
  issn = {08998256},
  language = {en},
  number = {1-2},
  journal = {Games and Economic Behavior},
  doi = {10.1006/game.1999.0738},
  author = {Freund, Yoav and Schapire, Robert E.},
  month = oct,
  year = {1999},
  pages = {79-103},
  file = {/Users/mfine/Zotero/storage/5S8MUA92/Freund and Schapire - 1999 - Adaptive Game Playing Using Multiplicative Weights.pdf}
}


